{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51df98f4-0dcb-4a5a-a8b8-9ad05613edbc",
   "metadata": {},
   "source": [
    "## VectorSotre Chroma DB  with Langchain Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e67cfc1-2e1b-49b8-99a5-ffcecf6dbf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from IPython.display  import display as disp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b837b136-99eb-4d92-9a3a-049e18f2c270",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Directory Constant\n",
    "CACHE_DATASET_DIR = Path('cache_dir')\n",
    "CHROMA_DB_DIR = Path('Knowledge_space')\n",
    "if not CHROMA_DB_DIR.is_dir():\n",
    "    CHROMA_DB_DIR.mkdir(parents=True)\n",
    "if not CACHE_DATASET_DIR.is_dir():\n",
    "    CACHE_DATASET_DIR.mkdir(parents=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b946ba-e498-4b51-ae40-683c69bea365",
   "metadata": {},
   "source": [
    "### Sentence Transformer Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24be84ef-acc1-438e-8256-5ea7f41882c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings.sentence_transformer \\\n",
    "import SentenceTransformerEmbeddings\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cca166f",
   "metadata": {},
   "source": [
    "### Differents Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1491e07-4fc2-4967-b7dc-a72a0b8f0496",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aritr\\anaconda3\\envs\\llm\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "modules.json: 100%|██████████| 349/349 [00:00<?, ?B/s] \n",
      "c:\\Users\\aritr\\anaconda3\\envs\\llm\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\aritr\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "config_sentence_transformers.json: 100%|██████████| 116/116 [00:00<?, ?B/s] \n",
      "README.md: 100%|██████████| 10.6k/10.6k [00:00<?, ?B/s]\n",
      "sentence_bert_config.json: 100%|██████████| 53.0/53.0 [00:00<?, ?B/s]\n",
      "config.json: 100%|██████████| 612/612 [00:00<?, ?B/s] \n",
      "pytorch_model.bin: 100%|██████████| 90.9M/90.9M [00:17<00:00, 5.10MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 350/350 [00:00<?, ?B/s] \n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 3.55MB/s]\n",
      "tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 848kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 112/112 [00:00<?, ?B/s] \n",
      "1_Pooling/config.json: 100%|██████████| 190/190 [00:00<?, ?B/s] \n"
     ]
    }
   ],
   "source": [
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "736330e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "google_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\",google_api_key='AIzaSyDTwXKPH5RgnpyKh9mxZnbyh2fFd4rNRZE')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5aba59b-0550-4ae8-8028-b63e8bdc8fc5",
   "metadata": {},
   "source": [
    "### Text Content Extraction from PDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fd50958-f774-4a89-b3ee-a56b7aa0b8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "097a628b-cb11-4512-93d8-ff537dbe514b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='8.3 When the Model Is Wrong . . . . . . . . . . . . . . . . . . . . 203', metadata={'source': './tmp/tmp.pdf', 'page': 4})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = PyPDFLoader(\"./tmp/tmp.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "text_spliter = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=0)\n",
    "texts = text_spliter.split_documents(pages)\n",
    "texts[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c117827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5174de-18b3-45af-a04b-61997c3e7bcf",
   "metadata": {},
   "source": [
    "### Creating Database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0dd9a3ed-95e3-439c-a14e-056bb8c670da",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db = Chroma.from_documents(documents=texts,\n",
    "                    embedding=google_embeddings, \n",
    "                    persist_directory=CHROMA_DB_DIR.resolve().as_posix())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b237b6dc-f5ae-4234-a97e-a77c89f0d392",
   "metadata": {},
   "source": [
    "### Loading Vector Database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "51dafc2c-129c-41fa-b0d5-3d139d54f797",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db_2 = Chroma(persist_directory=CHROMA_DB_DIR.resolve().as_posix(), embedding_function=embedding_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1b679d2b-e8b0-4936-9b66-b7525a53f09e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ids', 'embeddings', 'metadatas', 'documents', 'uris', 'data'])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'page': 34, 'source': './tmp/Dsa.pdf'}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disp(vector_db_2.get().keys())\n",
    "vector_db_2.get()['metadatas'][-150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9fdc0721-8d17-4bf9-9b57-ec6a27d4c628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "picture of the algorithm’s performance.\n",
      "As can be seen from Figs. 2and3, the results of two algorithms are\n",
      "very dependent on the task and one cannot be considered superior for all\n",
      "environments. Speciﬁcally, the Q-Learning algorithm has performed better on\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is Q Learning\"\n",
    "docs = vector_db_2.similarity_search(query)\n",
    "print(docs[1].page_content)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdd155d-26cd-475d-bcf8-0cf789b6a01f",
   "metadata": {},
   "source": [
    "### Search Data using VectorStoreRetriver "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a3d7d6bc-1a09-4f1d-95e8-c0077e53277a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'similarity'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "retriver = vector_db_2.as_retriever()\n",
    "docs = retriver.get_relevant_documents(query)\n",
    "disp(retriver.search_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b5eceeab-6f9f-4aa7-a738-e76ba4304b65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741fa1dd-29de-49d9-aa8c-8f38078111ba",
   "metadata": {},
   "source": [
    "## LLM Chain With Google Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a880304d-06cb-4786-b826-88214d2e4e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAI \n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5d6685e5-1ff9-42a7-b515-8f4790dc9261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "GOOGLE_API_KEY = getpass()\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "62020708-9965-40ff-9250-1fd30663c060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(name='models/chat-bison-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='PaLM 2 Chat (Legacy)',\n",
      "      description='A legacy text-only model optimized for chat conversations',\n",
      "      input_token_limit=4096,\n",
      "      output_token_limit=1024,\n",
      "      supported_generation_methods=['generateMessage', 'countMessageTokens'],\n",
      "      temperature=0.25,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/text-bison-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='PaLM 2 (Legacy)',\n",
      "      description='A legacy model that understands text and generates text as an output',\n",
      "      input_token_limit=8196,\n",
      "      output_token_limit=1024,\n",
      "      supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n",
      "      temperature=0.7,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/embedding-gecko-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Embedding Gecko',\n",
      "      description='Obtain a distributed representation of a text.',\n",
      "      input_token_limit=1024,\n",
      "      output_token_limit=1,\n",
      "      supported_generation_methods=['embedText', 'countTextTokens'],\n",
      "      temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/gemini-pro',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini Pro',\n",
      "      description='The best model for scaling across a wide range of tasks',\n",
      "      input_token_limit=30720,\n",
      "      output_token_limit=2048,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=0.9,\n",
      "      top_p=1.0,\n",
      "      top_k=1)\n",
      "Model(name='models/gemini-pro-vision',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini Pro Vision',\n",
      "      description='The best image understanding model to handle a broad range of applications',\n",
      "      input_token_limit=12288,\n",
      "      output_token_limit=4096,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=0.4,\n",
      "      top_p=1.0,\n",
      "      top_k=32)\n",
      "Model(name='models/embedding-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Embedding 001',\n",
      "      description='Obtain a distributed representation of a text.',\n",
      "      input_token_limit=2048,\n",
      "      output_token_limit=1,\n",
      "      supported_generation_methods=['embedContent', 'countTextTokens'],\n",
      "      temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/aqa',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Model that performs Attributed Question Answering.',\n",
      "      description=('Model trained to return answers to questions that are grounded in provided '\n",
      "                   'sources, along with estimating answerable probability.'),\n",
      "      input_token_limit=7168,\n",
      "      output_token_limit=1024,\n",
      "      supported_generation_methods=['generateAnswer'],\n",
      "      temperature=0.2,\n",
      "      top_p=1.0,\n",
      "      top_k=40)\n"
     ]
    }
   ],
   "source": [
    "for model in genai.list_models():\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e5bcea84-8f56-4ec1-b898-e9d05cf22771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fine, thank you'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_name = r'models/chat-bison-001'\n",
    "model_name = r'models/text-bison-001'\n",
    "# model_name = r'models/gemini-pro'\n",
    "# model_name = r'models//gemini-pro-version'\n",
    "# model_name = r'models/aqa'\n",
    "\n",
    "\n",
    "llm = GoogleGenerativeAI(model=model_name, google_api_key=GOOGLE_API_KEY)\n",
    "llm.invoke(\"Hello how are you ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a8c6cd-152e-4f81-bc7b-7e8a52c41612",
   "metadata": {},
   "source": [
    "### Basic LLM Chain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d552042f-eb1a-4648-be73-f83f7f099725",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "125b5886-a64d-440a-bd2a-eb2ab2320da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate.from_template(    \n",
    "'''This is some information for your knoledge:\n",
    "{knowledge}\\n\\n\n",
    "Answer the given question from the previous content.\n",
    "Question : {question}''')\n",
    "\n",
    "chain = prompt_template | llm\n",
    "\n",
    "# input_variable=['knowledge','question'],\n",
    "# llm_chain= LLMChain(llm=llm, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6be9b8b3-08f0-431d-bb0f-dec972a0c164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q-learning is a model-free reinforcement learning algorithm, which means that it does not require a model of the environment. It works by iteratively updating an action-value function, which estimates the expected return of taking a particular action in a particular state. Q-learning is often used in robotics and game playing.'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'knowledge':'\\n'.join([page.page_content for page in docs])\n",
    "              , 'question':\"What is Q learning\"})df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aef56f4-4211-46a3-91cd-dec8da6621f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a115204d-f3c8-4b54-a2b4-0aab92ffc764",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ceda58a-893f-4f75-b6bc-577415377370",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d10e0b8e-1da9-42a2-a3c4-5fa2ed369c92",
   "metadata": {},
   "source": [
    "## Experiment with New Book \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "06908d15-07b6-496b-bb4e-e36cdfc84d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='8.3 Quick Sort . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65\\n8.4 Insertion Sort . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\\n8.5 Shell Sort . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68\\n8.6 Radix Sort . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68\\n8.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70\\n9 Numeric 72\\n9.1 Primality Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72\\n9.2 Base conversions . . . . . . . . . . . . . . . . . . . . . . . . . . . 72\\n9.3 Attaining the greatest common denominator of two numbers . . 73\\n9.4 Computing the maximum value for a number of a speciﬁc base\\nconsisting of N digits . . . . . . . . . . . . . . . . . . . . . . . . . 74\\n9.5 Factorial of a number . . . . . . . . . . . . . . . . . . . . . . . . 74\\n9.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75\\nII', metadata={'source': './tmp/Dsa.pdf', 'page': 4})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = PyPDFLoader(\"./tmp/Dsa.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "text_spliter = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=0)\n",
    "texts = text_spliter.split_documents(pages)\n",
    "texts[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "87912336-51d2-40e4-ad78-df93b942f29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db = Chroma.from_documents(documents=texts,\n",
    "                    embedding=embedding_function, \n",
    "                    persist_directory=CHROMA_DB_DIR.resolve().as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d1baa876-c7ae-41b5-a859-8d970f8b266a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'What is Dynamic Programminng ?'\n",
    "retriver =  vector_db.as_retriever()\n",
    "docs = retriver.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ac4304f4-4861-4db9-bd69-8e874ca711fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dynamic programming is a technique that can be used to optimize a sequence of decisions. It works by breaking down the problem into smaller subproblems, and then solving each subproblem in turn. This can be done recursively, or by using a dynamic programming table to store the results of the subproblems.'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'knowledge':' '.join([page.page_content for page in docs])\n",
    "              , 'question':query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9c1d2a2b-f9e6-4390-9960-8d2caeb48473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OpenCog system embody a greedy heuristic for executing this DDS.\\nTo express dynamic programming in this DDS framework is a little subtler,\\nas in DP one tries to choose actions with probability proportional to overall\\nexpected cost/reward. Estimating the overall expected cost/reward of an action\\nOpenCog system embody a greedy heuristic for executing this DDS.\\nTo express dynamic programming in this DDS framework is a little subtler,\\nas in DP one tries to choose actions with probability proportional to overall\\nexpected cost/reward. Estimating the overall expected cost/reward of an action\\n120 B. Goertzel\\naction. If probabilistic inference is used along with sampling, then one may have\\na peculiar sort of approximate stochastic dynamic programming in which thestep of choosing an action involves making an estimation that itself may be\\nusefully carried out by stochastic dynamic programming (but with a diﬀerent\\nobjective function than the objective function for whose optimization the actionis being chosen).\\nBasically, in the COFO framework one looks at the process of optimizing F\\nas an explicit dynamical decision process conducted via sequential applicationof an operation in which: Operations C\\nithat combine inputs chosen from a\\ndistribution induced by prior objective function evaluations, are used to get new\\ncandidate arguments to feed to Ffor evaluation. The reward function guiding\\nthis exploration is the quest for reduction of the entropy of the set of guesses at\\narguments that look promising to make Fnear-optimal based on the evaluations\\nmade so far.\\n120 B. Goertzel\\naction. If probabilistic inference is used along with sampling, then one may have\\na peculiar sort of approximate stochastic dynamic programming in which thestep of choosing an action involves making an estimation that itself may be\\nusefully carried out by stochastic dynamic programming (but with a diﬀerent\\nobjective function than the objective function for whose optimization the actionis being chosen).\\nBasically, in the COFO framework one looks at the process of optimizing F\\nas an explicit dynamical decision process conducted via sequential applicationof an operation in which: Operations C\\nithat combine inputs chosen from a\\ndistribution induced by prior objective function evaluations, are used to get new\\ncandidate arguments to feed to Ffor evaluation. The reward function guiding\\nthis exploration is the quest for reduction of the entropy of the set of guesses at\\narguments that look promising to make Fnear-optimal based on the evaluations\\nmade so far.'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'\\n'.join([page.page_content for page in docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90eb7e87",
   "metadata": {},
   "source": [
    "### OpenLLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cbe8a76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "chromadb 0.4.22 requires build>=1.0.3, but you have build 0.10.0 which is incompatible.\n",
      "opentelemetry-exporter-otlp-proto-grpc 1.22.0 requires opentelemetry-sdk~=1.22.0, but you have opentelemetry-sdk 1.20.0 which is incompatible.\n",
      "opentelemetry-instrumentation-fastapi 0.43b0 requires opentelemetry-instrumentation==0.43b0, but you have opentelemetry-instrumentation 0.41b0 which is incompatible.\n",
      "opentelemetry-instrumentation-fastapi 0.43b0 requires opentelemetry-instrumentation-asgi==0.43b0, but you have opentelemetry-instrumentation-asgi 0.41b0 which is incompatible.\n",
      "opentelemetry-instrumentation-fastapi 0.43b0 requires opentelemetry-semantic-conventions==0.43b0, but you have opentelemetry-semantic-conventions 0.41b0 which is incompatible.\n",
      "opentelemetry-instrumentation-fastapi 0.43b0 requires opentelemetry-util-http==0.43b0, but you have opentelemetry-util-http 0.41b0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet  openllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b9e03abb-99f4-4523-8591-3f443e4ccec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "instruct_pipeline.py: 100%|██████████| 9.16k/9.16k [00:00<?, ?B/s]\n",
      "Fetching 6 files:  33%|███▎      | 2/6 [00:02<00:04,  1.14s/it]"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import OpenLLM\n",
    "\n",
    "llm = OpenLLM(\n",
    "    model_name=\"dolly-v2\",\n",
    "    model_id=\"databricks/dolly-v2-3b\",\n",
    "    temperature=0.94,\n",
    "    repetition_penalty=1.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b791587",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
