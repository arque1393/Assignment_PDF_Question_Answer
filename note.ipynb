{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f542d208-4b12-4df5-a189-9bb6cbc51eb6",
   "metadata": {},
   "source": [
    "## PDF Query using LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "445d6ce8-5f29-4980-b02f-ace68a11c12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display as disp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338be3b8-51fe-4d6a-b08a-650746d75541",
   "metadata": {},
   "source": [
    "### Loading PDF on langchain document loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0685802-352d-43cd-b62c-3f05a5d890f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb0d6205-2137-40ac-9d44-ced05e2eac92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of page  = 380\n",
      "page_content length = 192\n"
     ]
    }
   ],
   "source": [
    "loader = PyPDFLoader(\"./tmp/tmp.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "print(f'no of page  = {len(pages)}')\n",
    "print(f'page_content length = {len(pages[0].page_content)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9763368-025a-43ce-a16d-de8ba2827902",
   "metadata": {},
   "source": [
    "#### Data Chunks in smaller documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e211e25-45d7-4b00-8b6e-4e1079204ffe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='be able to reproduce, animals must solve a continuous stream of problems during theirlives, e.g., ﬁnding food, avoiding predators, mating, and parenting. This suggests that\\nhuman intelligence primarily evolved for solving everyday problems related to survival\\nin the different habitats of Homo sapiens .\\nArtiﬁcial Intelligence started as an attempt to reproduce parts of human intelligence\\nin machines and, just like the notion of human intelligence, it is associated with a\\ncertain vagueness regarding its deﬁnition, targeted problems, performance measures,\\nand relations to neighboring research ﬁelds.\\nRecently, AI research has been quite successful at producing systems that are gen-\\neral in the sense that they can translate between many languages, play many games,\\nmanipulate many objects, predict many video frames, write many texts, generate many\\nimages, and diagnose many diseases.\\nStill, many of the basic challenges of AGI remain unsolved. In fact, we do not yet', metadata={'source': './tmp/tmp.pdf', 'page': 5})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_spliter = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=0)\n",
    "texts = text_spliter.split_documents(pages)\n",
    "texts[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd97ea73-59ac-4d18-a78a-7c555d3b8d30",
   "metadata": {},
   "source": [
    "##### Database Connection "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7d1b30-1416-4c9c-b4ee-0a237311e602",
   "metadata": {},
   "source": [
    "## Embedding with Chroma DB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87746e66-9965-4152-b413-54803d999631",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\.conda\\envs\\llm\\Lib\\site-packages\\InstructorEmbedding\\instructor.py:7: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import trange\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "from pathlib import Path\n",
    "from langchain.vectorstores import Chroma  \n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders.pdf import UnstructuredPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import  OpenAI\n",
    "\n",
    "from InstructorEmbedding import INSTRUCTOR\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "# from langchain.document_loaders import TextLoader, DirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10542fd8-4bbd-4d6d-baed-e5d0b323808b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='be able to reproduce, animals must solve a continuous stream of problems during theirlives, e.g., ﬁnding food, avoiding predators, mating, and parenting. This suggests that\\nhuman intelligence primarily evolved for solving everyday problems related to survival\\nin the different habitats of Homo sapiens .\\nArtiﬁcial Intelligence started as an attempt to reproduce parts of human intelligence\\nin machines and, just like the notion of human intelligence, it is associated with a\\ncertain vagueness regarding its deﬁnition, targeted problems, performance measures,\\nand relations to neighboring research ﬁelds.\\nRecently, AI research has been quite successful at producing systems that are gen-\\neral in the sense that they can translate between many languages, play many games,\\nmanipulate many objects, predict many video frames, write many texts, generate many\\nimages, and diagnose many diseases.\\nStill, many of the basic challenges of AGI remain unsolved. In fact, we do not yet', metadata={'source': './tmp/tmp.pdf', 'page': 5})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = PyPDFLoader(\"./tmp/tmp.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "text_spliter = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=0)\n",
    "texts = text_spliter.split_documents(pages)\n",
    "texts[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "061818e7-fc06-431d-8fa3-43db9b1924ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path('Knowledge_space')\n",
    "CACHE_DATASET = Path('cache_dir')\n",
    "DATABASE_DIR = Path('Knowledge_space')\n",
    "if not DATABASE_DIR.is_dir():\n",
    "    DATABASE_DIR.mkdir(parents=True)\n",
    "if not CACHE_DATASET.is_dir():\n",
    "    CACHE_DATASET.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fdb7d9d-39b2-4bc9-98ba-5ada95f40a0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92fc3ff36cca4f77924e650731b339f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in D:\\AritraRanjanChowdhury\\GEN_AI\\Assignment_PDF_Question_Answer\\cache_dir\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45d772290a054d6a92263bd4e4c709f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15df69f4847a4d47950c3316a5ea2122",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5961e97bee6a47e1aeaf0dc2d7b95e90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70640160c857484c87f43118c284b8f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2160cd881c24486da787b187f8633a4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa437d4e93d24bfd87b84c496188e8d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "973bbe21d5bb4b488c6069a3f05cf7d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7b751825b474d6baa5bd3117be4e939",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e0f32e7ba50486481c57d2c61b1ad96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "236cd4e2f5374ff8844d095e3d15d003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model = SentenceTransformer(\n",
    "    \"all-MiniLM-L6-v2\", \n",
    "    cache_folder = CACHE_DATASET.resolve().__str__()\n",
    ")\n",
    "\n",
    "# text_embedding_vector = embedding_model.encode([text.page_content for text in texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ffb5422-b6c5-4045-bb59-c84ab19176d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1203,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_text = np.array([text.page_content for text in texts])\n",
    "exp_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57b8b166-de00-4372-aa6e-6749512ff008",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings.sentence_transformer \\\n",
    "import SentenceTransformerEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549a9612-f974-40ed-95d8-f8dd525dbea8",
   "metadata": {},
   "source": [
    "### Creating Vector Store "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c24f342c-f6b4-4cdc-8962-f22652d69a30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vector_db=Chroma.from_documents(documents=texts,\n",
    "                               embedding = embedding_function,\n",
    "                               persist_directory= str(DATABASE_DIR.resolve()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf78c9c0-4bbc-4fd5-9f63-df7ecf057820",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb \n",
    "db_client = chromadb.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00389ca2-bad1-4e60-a8ff-f785791d3fc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f29387f-1f9d-4622-af32-60ae55274ea2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "INSTRUCTOR._load_sbert_model() got an unexpected keyword argument 'token'",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  Cell \u001b[0;32mIn[14], line 2\u001b[0m\n    model = INSTRUCTOR('hkunlp/instructor-large')\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sentence_transformers\\SentenceTransformer.py:194\u001b[1;36m in \u001b[1;35m__init__\u001b[1;36m\n\u001b[1;33m    modules = self._load_sbert_model(\u001b[1;36m\n",
      "\u001b[1;31mTypeError\u001b[0m\u001b[1;31m:\u001b[0m INSTRUCTOR._load_sbert_model() got an unexpected keyword argument 'token'\n"
     ]
    }
   ],
   "source": [
    "from InstructorEmbedding import INSTRUCTOR\n",
    "model = INSTRUCTOR('hkunlp/instructor-large')\n",
    "sentence = \"3D ActionSLAM: wearable person tracking in multi-floor environments\"\n",
    "instruction = \"Represent the Science title:\"\n",
    "embeddings = model.encode([[instruction,sentence]])\n",
    "print(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347dab73-a855-44eb-8aaf-39f54747a405",
   "metadata": {},
   "outputs": [],
   "source": [
    "from InstructorEmbedding import INSTRUCTOR\n",
    "model = INSTRUCTOR('hkunlp/instructor-xl')\n",
    "sentence = \"3D ActionSLAM: wearable person tracking in multi-floor environments\"\n",
    "instruction = \"Represent the Science title:\"\n",
    "embeddings = model.encode([[instruction,sentence]])\n",
    "print(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca567e9f-4a6d-4e4a-88fe-b31481db1f6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e5f29e3-6c4c-42c7-9b40-d371f69ff996",
   "metadata": {},
   "source": [
    "## Chroma DB Client and Lang Chain Chroma "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9e8ec87-6745-4ddf-844f-3f719cdb3adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37bce092-44f4-4849-9264-ed044b6e6b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATABASE_PATH = Path( 'VectorDB')\n",
    "user_1_db  = DATABASE_PATH/'user1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "109766e0-c7c8-4b48-9bbf-205279f35389",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.PersistentClient(path=str(DATABASE_PATH.resolve()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab47bbe-be5a-4d95-ab8b-87a991e24e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "chroma_client = chromadb.HttpClient(host='localhost', port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5527bac3-2108-4af2-b8cc-cc13f2aad88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collection = client.create_collection(name=\"Collection2\")\n",
    "collection = client.get_collection(name=\"Collection2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d91d220c-280b-49d0-b790-b168464181e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__abstractmethods__', '__annotations__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_add', '_admin_client', '_count', '_create_system_if_not_exists', '_delete', '_get', '_get_identifier_from_settings', '_identifer_to_system', '_identifier', '_modify', '_peek', '_populate_data_from_system', '_query', '_server', '_system', '_update', '_upsert', '_validate_tenant_database', 'clear_system_cache', 'count_collections', 'create_collection', 'database', 'delete_collection', 'from_system', 'get_collection', 'get_or_create_collection', 'get_settings', 'get_version', 'heartbeat', 'list_collections', 'max_batch_size', 'reset', 'set_database', 'set_tenant', 'tenant']\n"
     ]
    }
   ],
   "source": [
    "print(dir(client))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "110eed84-5c00-45e5-a5b9-4f121b84a32c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Collection2'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.list_collections()[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a6b72fa-6162-4bb9-9224-b21c5a83a2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_chroma = Chroma(\n",
    "    client=client,\n",
    "    collection_name=\"Collection_2\",\n",
    "    embedding_function=embedding_function,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2d3adf9-bd9f-4dbe-acac-bc9305a0fc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_chroma = langchain_chroma.from_documents(texts, embedding_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54b46dcb-ba06-4783-b363-bbb44386ec88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [],\n",
       " 'embeddings': None,\n",
       " 'metadatas': [],\n",
       " 'documents': [],\n",
       " 'uris': None,\n",
       " 'data': None}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langchain_chroma.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fcb4a652-10b2-4155-b59d-7c0c38163fea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x0000018CBF36AD10>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_chroma.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "957a19f0-03f0-4de4-bb45-791855eaddcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['25f4f96c-c584-11ee-bc99-e0d55edbb401',\n",
       "  '25f4f96d-c584-11ee-959a-e0d55edbb401',\n",
       "  '25f4f96e-c584-11ee-873d-e0d55edbb401',\n",
       "  '25f4f96f-c584-11ee-b2f6-e0d55edbb401',\n",
       "  '25f4f970-c584-11ee-813e-e0d55edbb401',\n",
       "  '25f4f971-c584-11ee-a68a-e0d55edbb401',\n",
       "  '25f4f972-c584-11ee-84af-e0d55edbb401',\n",
       "  '25f4f973-c584-11ee-be44-e0d55edbb401',\n",
       "  '25f4f974-c584-11ee-af13-e0d55edbb401',\n",
       "  '25f4f975-c584-11ee-ae8d-e0d55edbb401',\n",
       "  '25f4f976-c584-11ee-920c-e0d55edbb401',\n",
       "  '25f4f977-c584-11ee-836b-e0d55edbb401',\n",
       "  '25f4f978-c584-11ee-9891-e0d55edbb401',\n",
       "  '25f4f979-c584-11ee-8a3b-e0d55edbb401',\n",
       "  '25f4f97a-c584-11ee-9dcd-e0d55edbb401',\n",
       "  '25f4f97b-c584-11ee-900a-e0d55edbb401',\n",
       "  '25f4f97c-c584-11ee-a5ba-e0d55edbb401',\n",
       "  '25f4f97d-c584-11ee-91d6-e0d55edbb401',\n",
       "  '25f4f97e-c584-11ee-b973-e0d55edbb401',\n",
       "  '25f4f97f-c584-11ee-9239-e0d55edbb401',\n",
       "  '25f4f980-c584-11ee-a483-e0d55edbb401',\n",
       "  '25f4f981-c584-11ee-8357-e0d55edbb401',\n",
       "  '25f4f982-c584-11ee-930c-e0d55edbb401',\n",
       "  '25f4f983-c584-11ee-9559-e0d55edbb401',\n",
       "  '25f4f984-c584-11ee-a824-e0d55edbb401',\n",
       "  '25f4f985-c584-11ee-b398-e0d55edbb401',\n",
       "  '25f4f986-c584-11ee-9603-e0d55edbb401',\n",
       "  '25f4f987-c584-11ee-888e-e0d55edbb401',\n",
       "  '25f4f988-c584-11ee-af33-e0d55edbb401',\n",
       "  '25f4f989-c584-11ee-98e4-e0d55edbb401',\n",
       "  '25f4f98a-c584-11ee-9670-e0d55edbb401',\n",
       "  '25f4f98b-c584-11ee-adc7-e0d55edbb401',\n",
       "  '25f4f98c-c584-11ee-9519-e0d55edbb401',\n",
       "  '25f4f98d-c584-11ee-a940-e0d55edbb401',\n",
       "  '25f4f98e-c584-11ee-b035-e0d55edbb401',\n",
       "  '25f4f98f-c584-11ee-bafd-e0d55edbb401',\n",
       "  '25f4f990-c584-11ee-8f2d-e0d55edbb401',\n",
       "  '25f4f991-c584-11ee-8089-e0d55edbb401',\n",
       "  '25f4f992-c584-11ee-bb22-e0d55edbb401',\n",
       "  '25f4f993-c584-11ee-8ead-e0d55edbb401',\n",
       "  '25f4f994-c584-11ee-85b1-e0d55edbb401',\n",
       "  '25f4f995-c584-11ee-8b91-e0d55edbb401',\n",
       "  '25f4f996-c584-11ee-8e80-e0d55edbb401',\n",
       "  '25f4f997-c584-11ee-ad67-e0d55edbb401',\n",
       "  '25f4f998-c584-11ee-afe6-e0d55edbb401',\n",
       "  '25f4f999-c584-11ee-badd-e0d55edbb401',\n",
       "  '25f4f99a-c584-11ee-9e33-e0d55edbb401',\n",
       "  '25f4f99b-c584-11ee-8891-e0d55edbb401',\n",
       "  '25f4f99c-c584-11ee-b4eb-e0d55edbb401',\n",
       "  '25f4f99d-c584-11ee-8a47-e0d55edbb401',\n",
       "  '25f4f99e-c584-11ee-9636-e0d55edbb401',\n",
       "  '25f4f99f-c584-11ee-9078-e0d55edbb401',\n",
       "  '25f4f9a0-c584-11ee-9baa-e0d55edbb401',\n",
       "  '25f4f9a1-c584-11ee-a9d9-e0d55edbb401',\n",
       "  '25f4f9a2-c584-11ee-a788-e0d55edbb401',\n",
       "  '25f4f9a3-c584-11ee-a9b0-e0d55edbb401',\n",
       "  '25f4f9a4-c584-11ee-84bc-e0d55edbb401',\n",
       "  '25f4f9a5-c584-11ee-b0ff-e0d55edbb401',\n",
       "  '25f4f9a6-c584-11ee-9941-e0d55edbb401',\n",
       "  '25f4f9a7-c584-11ee-b874-e0d55edbb401',\n",
       "  '25f4f9a8-c584-11ee-919e-e0d55edbb401',\n",
       "  '25f4f9a9-c584-11ee-b837-e0d55edbb401',\n",
       "  '25f4f9aa-c584-11ee-b0f5-e0d55edbb401',\n",
       "  '25f4f9ab-c584-11ee-b1e2-e0d55edbb401',\n",
       "  '25f4f9ac-c584-11ee-8499-e0d55edbb401',\n",
       "  '25f4f9ad-c584-11ee-88d5-e0d55edbb401',\n",
       "  '25f4f9ae-c584-11ee-8187-e0d55edbb401',\n",
       "  '25f4f9af-c584-11ee-b640-e0d55edbb401',\n",
       "  '25f4f9b0-c584-11ee-a032-e0d55edbb401',\n",
       "  '25f4f9b1-c584-11ee-bed9-e0d55edbb401',\n",
       "  '25f4f9b2-c584-11ee-bdce-e0d55edbb401',\n",
       "  '25f4f9b3-c584-11ee-ab3e-e0d55edbb401',\n",
       "  '25f4f9b4-c584-11ee-b9a3-e0d55edbb401',\n",
       "  '25f4f9b5-c584-11ee-9eb8-e0d55edbb401',\n",
       "  '25f4f9b6-c584-11ee-adc1-e0d55edbb401',\n",
       "  '25f4f9b7-c584-11ee-bc5d-e0d55edbb401',\n",
       "  '25f4f9b8-c584-11ee-9629-e0d55edbb401',\n",
       "  '25f4f9b9-c584-11ee-9cca-e0d55edbb401',\n",
       "  '25f4f9ba-c584-11ee-ab83-e0d55edbb401',\n",
       "  '25f4f9bb-c584-11ee-9bd4-e0d55edbb401',\n",
       "  '25f4f9bc-c584-11ee-a590-e0d55edbb401',\n",
       "  '25f4f9bd-c584-11ee-8767-e0d55edbb401',\n",
       "  '25f4f9be-c584-11ee-9345-e0d55edbb401',\n",
       "  '25f4f9bf-c584-11ee-8819-e0d55edbb401',\n",
       "  '25f4f9c0-c584-11ee-8a22-e0d55edbb401',\n",
       "  '25f4f9c1-c584-11ee-b968-e0d55edbb401',\n",
       "  '25f4f9c2-c584-11ee-8f4b-e0d55edbb401',\n",
       "  '25f4f9c3-c584-11ee-bd06-e0d55edbb401',\n",
       "  '25f4f9c4-c584-11ee-9480-e0d55edbb401',\n",
       "  '25f4f9c5-c584-11ee-9c8f-e0d55edbb401',\n",
       "  '25f4f9c6-c584-11ee-93cd-e0d55edbb401',\n",
       "  '25f4f9c7-c584-11ee-883e-e0d55edbb401',\n",
       "  '25f4f9c8-c584-11ee-aa81-e0d55edbb401',\n",
       "  '25f4f9c9-c584-11ee-bcd4-e0d55edbb401',\n",
       "  '25f4f9ca-c584-11ee-8c06-e0d55edbb401',\n",
       "  '25f4f9cb-c584-11ee-bea6-e0d55edbb401',\n",
       "  '25f4f9cc-c584-11ee-9673-e0d55edbb401',\n",
       "  '25f4f9cd-c584-11ee-abdb-e0d55edbb401',\n",
       "  '25f4f9ce-c584-11ee-99a8-e0d55edbb401',\n",
       "  '25f4f9cf-c584-11ee-8b84-e0d55edbb401',\n",
       "  '25f4f9d0-c584-11ee-98ec-e0d55edbb401',\n",
       "  '25f4f9d1-c584-11ee-b4db-e0d55edbb401',\n",
       "  '25f4f9d2-c584-11ee-8d5a-e0d55edbb401',\n",
       "  '25f4f9d3-c584-11ee-8718-e0d55edbb401',\n",
       "  '25f4f9d4-c584-11ee-8960-e0d55edbb401',\n",
       "  '25f4f9d5-c584-11ee-8f73-e0d55edbb401',\n",
       "  '25f4f9d6-c584-11ee-acea-e0d55edbb401',\n",
       "  '25f4f9d7-c584-11ee-b346-e0d55edbb401',\n",
       "  '25f4f9d8-c584-11ee-adcd-e0d55edbb401',\n",
       "  '25f4f9d9-c584-11ee-ac07-e0d55edbb401',\n",
       "  '25f4f9da-c584-11ee-8a99-e0d55edbb401',\n",
       "  '25f4f9db-c584-11ee-a47a-e0d55edbb401',\n",
       "  '25f4f9dc-c584-11ee-aee8-e0d55edbb401',\n",
       "  '25f4f9dd-c584-11ee-94aa-e0d55edbb401',\n",
       "  '25f4f9de-c584-11ee-99a9-e0d55edbb401',\n",
       "  '25f4f9df-c584-11ee-bc05-e0d55edbb401',\n",
       "  '25f4f9e0-c584-11ee-8826-e0d55edbb401',\n",
       "  '25f4f9e1-c584-11ee-9d88-e0d55edbb401',\n",
       "  '25f4f9e2-c584-11ee-b6f3-e0d55edbb401',\n",
       "  '25f4f9e3-c584-11ee-b1c7-e0d55edbb401',\n",
       "  '25f4f9e4-c584-11ee-bf49-e0d55edbb401',\n",
       "  '25f4f9e5-c584-11ee-9321-e0d55edbb401',\n",
       "  '25f4f9e6-c584-11ee-86bb-e0d55edbb401',\n",
       "  '25f4f9e7-c584-11ee-8c0a-e0d55edbb401',\n",
       "  '25f4f9e8-c584-11ee-b6b4-e0d55edbb401',\n",
       "  '25f4f9e9-c584-11ee-8ed3-e0d55edbb401',\n",
       "  '25f4f9ea-c584-11ee-9f91-e0d55edbb401',\n",
       "  '25f4f9eb-c584-11ee-a25a-e0d55edbb401',\n",
       "  '25f4f9ec-c584-11ee-8639-e0d55edbb401',\n",
       "  '25f4f9ed-c584-11ee-89f2-e0d55edbb401',\n",
       "  '25f4f9ee-c584-11ee-9fff-e0d55edbb401',\n",
       "  '25f4f9ef-c584-11ee-91f2-e0d55edbb401',\n",
       "  '25f4f9f0-c584-11ee-a6a7-e0d55edbb401',\n",
       "  '25f4f9f1-c584-11ee-9fac-e0d55edbb401',\n",
       "  '25f4f9f2-c584-11ee-8f1d-e0d55edbb401',\n",
       "  '25f4f9f3-c584-11ee-82a2-e0d55edbb401',\n",
       "  '25f4f9f4-c584-11ee-8e55-e0d55edbb401',\n",
       "  '25f4f9f5-c584-11ee-90f6-e0d55edbb401',\n",
       "  '25f4f9f6-c584-11ee-a679-e0d55edbb401',\n",
       "  '25f4f9f7-c584-11ee-bf9b-e0d55edbb401',\n",
       "  '25f4f9f8-c584-11ee-a5b4-e0d55edbb401',\n",
       "  '25f4f9f9-c584-11ee-827a-e0d55edbb401',\n",
       "  '25f4f9fa-c584-11ee-81e7-e0d55edbb401',\n",
       "  '25f4f9fb-c584-11ee-b0a6-e0d55edbb401',\n",
       "  '25f4f9fc-c584-11ee-8159-e0d55edbb401',\n",
       "  '25f4f9fd-c584-11ee-9910-e0d55edbb401',\n",
       "  '25f4f9fe-c584-11ee-9c2c-e0d55edbb401',\n",
       "  '25f4f9ff-c584-11ee-81e2-e0d55edbb401',\n",
       "  '25f4fa00-c584-11ee-b045-e0d55edbb401',\n",
       "  '25f4fa01-c584-11ee-b5c8-e0d55edbb401',\n",
       "  '25f4fa02-c584-11ee-9e6a-e0d55edbb401',\n",
       "  '25f4fa03-c584-11ee-8250-e0d55edbb401',\n",
       "  '25f4fa04-c584-11ee-afee-e0d55edbb401',\n",
       "  '25f4fa05-c584-11ee-a3ae-e0d55edbb401',\n",
       "  '25f4fa06-c584-11ee-8b9b-e0d55edbb401',\n",
       "  '25f4fa07-c584-11ee-83bd-e0d55edbb401',\n",
       "  '25f4fa08-c584-11ee-9010-e0d55edbb401',\n",
       "  '25f4fa09-c584-11ee-ad4b-e0d55edbb401',\n",
       "  '25f4fa0a-c584-11ee-90de-e0d55edbb401',\n",
       "  '25f4fa0b-c584-11ee-a508-e0d55edbb401',\n",
       "  '25f4fa0c-c584-11ee-ba40-e0d55edbb401',\n",
       "  '25f4fa0d-c584-11ee-97a0-e0d55edbb401',\n",
       "  '25f4fa0e-c584-11ee-8b12-e0d55edbb401',\n",
       "  '25f4fa0f-c584-11ee-a59c-e0d55edbb401',\n",
       "  '25f4fa10-c584-11ee-88bb-e0d55edbb401',\n",
       "  '25f4fa11-c584-11ee-a866-e0d55edbb401',\n",
       "  '25f4fa12-c584-11ee-9306-e0d55edbb401',\n",
       "  '25f4fa13-c584-11ee-aa7c-e0d55edbb401',\n",
       "  '25f4fa14-c584-11ee-99c4-e0d55edbb401',\n",
       "  '25f4fa15-c584-11ee-bc55-e0d55edbb401',\n",
       "  '25f4fa16-c584-11ee-b892-e0d55edbb401',\n",
       "  '25f4fa17-c584-11ee-b76f-e0d55edbb401',\n",
       "  '25f4fa18-c584-11ee-946b-e0d55edbb401',\n",
       "  '25f4fa19-c584-11ee-ac44-e0d55edbb401',\n",
       "  '25f4fa1a-c584-11ee-adfe-e0d55edbb401',\n",
       "  '25f4fa1b-c584-11ee-af52-e0d55edbb401',\n",
       "  '25f4fa1c-c584-11ee-a6cc-e0d55edbb401',\n",
       "  '25f4fa1d-c584-11ee-a1f5-e0d55edbb401',\n",
       "  '25f4fa1e-c584-11ee-ac85-e0d55edbb401',\n",
       "  '25f4fa1f-c584-11ee-99e0-e0d55edbb401',\n",
       "  '25f4fa20-c584-11ee-a9da-e0d55edbb401',\n",
       "  '25f4fa21-c584-11ee-b97d-e0d55edbb401',\n",
       "  '25f4fa22-c584-11ee-bd4a-e0d55edbb401',\n",
       "  '25f4fa23-c584-11ee-838b-e0d55edbb401',\n",
       "  '25f4fa24-c584-11ee-8d7e-e0d55edbb401',\n",
       "  '25f4fa25-c584-11ee-990d-e0d55edbb401',\n",
       "  '25f4fa26-c584-11ee-869b-e0d55edbb401',\n",
       "  '25f4fa27-c584-11ee-a6a7-e0d55edbb401',\n",
       "  '25f4fa28-c584-11ee-bd9c-e0d55edbb401',\n",
       "  '25f4fa29-c584-11ee-87f9-e0d55edbb401',\n",
       "  '25f4fa2a-c584-11ee-bc28-e0d55edbb401',\n",
       "  '25f4fa2b-c584-11ee-81a6-e0d55edbb401',\n",
       "  '25f4fa2c-c584-11ee-8608-e0d55edbb401',\n",
       "  '25f4fa2d-c584-11ee-bce1-e0d55edbb401',\n",
       "  '25f4fa2e-c584-11ee-bc08-e0d55edbb401',\n",
       "  '25f4fa2f-c584-11ee-bd90-e0d55edbb401',\n",
       "  '25f4fa30-c584-11ee-8c0f-e0d55edbb401',\n",
       "  '25f4fa31-c584-11ee-8560-e0d55edbb401',\n",
       "  '25f4fa32-c584-11ee-b94d-e0d55edbb401',\n",
       "  '25f4fa33-c584-11ee-a866-e0d55edbb401',\n",
       "  '25f4fa34-c584-11ee-8b5c-e0d55edbb401',\n",
       "  '25f4fa35-c584-11ee-88c8-e0d55edbb401',\n",
       "  '25f4fa36-c584-11ee-b863-e0d55edbb401',\n",
       "  '25f4fa37-c584-11ee-8164-e0d55edbb401',\n",
       "  '25f4fa38-c584-11ee-bdfa-e0d55edbb401',\n",
       "  '25f4fa39-c584-11ee-943b-e0d55edbb401',\n",
       "  '25f4fa3a-c584-11ee-b155-e0d55edbb401',\n",
       "  '25f4fa3b-c584-11ee-a0d9-e0d55edbb401',\n",
       "  '25f4fa3c-c584-11ee-91e9-e0d55edbb401',\n",
       "  '25f4fa3d-c584-11ee-97db-e0d55edbb401',\n",
       "  '25f4fa3e-c584-11ee-ba56-e0d55edbb401',\n",
       "  '25f4fa3f-c584-11ee-a0de-e0d55edbb401',\n",
       "  '25f4fa40-c584-11ee-8259-e0d55edbb401',\n",
       "  '25f4fa41-c584-11ee-9707-e0d55edbb401',\n",
       "  '25f4fa42-c584-11ee-8e56-e0d55edbb401',\n",
       "  '25f4fa43-c584-11ee-ac71-e0d55edbb401',\n",
       "  '25f4fa44-c584-11ee-aee9-e0d55edbb401',\n",
       "  '25f4fa45-c584-11ee-b12b-e0d55edbb401',\n",
       "  '25f4fa46-c584-11ee-9858-e0d55edbb401',\n",
       "  '25f4fa47-c584-11ee-b830-e0d55edbb401',\n",
       "  '25f4fa48-c584-11ee-8a04-e0d55edbb401',\n",
       "  '25f4fa49-c584-11ee-92e8-e0d55edbb401',\n",
       "  '25f4fa4a-c584-11ee-81d4-e0d55edbb401',\n",
       "  '25f4fa4b-c584-11ee-bf89-e0d55edbb401',\n",
       "  '25f4fa4c-c584-11ee-bbfb-e0d55edbb401',\n",
       "  '25f4fa4d-c584-11ee-bec6-e0d55edbb401',\n",
       "  '25f4fa4e-c584-11ee-85e2-e0d55edbb401',\n",
       "  '25f4fa4f-c584-11ee-a332-e0d55edbb401',\n",
       "  '25f4fa50-c584-11ee-8c76-e0d55edbb401',\n",
       "  '25f4fa51-c584-11ee-a046-e0d55edbb401',\n",
       "  '25f4fa52-c584-11ee-acf9-e0d55edbb401',\n",
       "  '25f4fa53-c584-11ee-ad06-e0d55edbb401',\n",
       "  '25f4fa54-c584-11ee-b966-e0d55edbb401',\n",
       "  '25f4fa55-c584-11ee-a8d1-e0d55edbb401',\n",
       "  '25f4fa56-c584-11ee-8794-e0d55edbb401',\n",
       "  '25f4fa57-c584-11ee-a889-e0d55edbb401',\n",
       "  '25f4fa58-c584-11ee-ba4f-e0d55edbb401',\n",
       "  '25f4fa59-c584-11ee-953f-e0d55edbb401',\n",
       "  '25f4fa5a-c584-11ee-a682-e0d55edbb401',\n",
       "  '25f4fa5b-c584-11ee-8d7f-e0d55edbb401',\n",
       "  '25f4fa5c-c584-11ee-95df-e0d55edbb401',\n",
       "  '25f4fa5d-c584-11ee-bdcc-e0d55edbb401',\n",
       "  '25f4fa5e-c584-11ee-a4e8-e0d55edbb401',\n",
       "  '25f4fa5f-c584-11ee-8a31-e0d55edbb401',\n",
       "  '25f4fa60-c584-11ee-99e3-e0d55edbb401',\n",
       "  '25f4fa61-c584-11ee-bf6c-e0d55edbb401',\n",
       "  '25f4fa62-c584-11ee-8853-e0d55edbb401',\n",
       "  '25f4fa63-c584-11ee-b8c6-e0d55edbb401',\n",
       "  '25f4fa64-c584-11ee-a620-e0d55edbb401',\n",
       "  '25f4fa65-c584-11ee-b923-e0d55edbb401',\n",
       "  '25f4fa66-c584-11ee-9568-e0d55edbb401',\n",
       "  '25f4fa67-c584-11ee-934c-e0d55edbb401',\n",
       "  '25f4fa68-c584-11ee-9596-e0d55edbb401',\n",
       "  '25f4fa69-c584-11ee-bda9-e0d55edbb401',\n",
       "  '25f4fa6a-c584-11ee-8f3c-e0d55edbb401',\n",
       "  '25f4fa6b-c584-11ee-bd18-e0d55edbb401',\n",
       "  '25f4fa6c-c584-11ee-a5b5-e0d55edbb401',\n",
       "  '25f4fa6d-c584-11ee-bad5-e0d55edbb401',\n",
       "  '25f4fa6e-c584-11ee-b8c5-e0d55edbb401',\n",
       "  '25f4fa6f-c584-11ee-9f8f-e0d55edbb401',\n",
       "  '25f4fa70-c584-11ee-9ae0-e0d55edbb401',\n",
       "  '25f4fa71-c584-11ee-83ec-e0d55edbb401',\n",
       "  '25f4fa72-c584-11ee-b2ac-e0d55edbb401',\n",
       "  '25f4fa73-c584-11ee-a0a0-e0d55edbb401',\n",
       "  '25f4fa74-c584-11ee-bf3f-e0d55edbb401',\n",
       "  '25f4fa75-c584-11ee-97f7-e0d55edbb401',\n",
       "  '25f4fa76-c584-11ee-a467-e0d55edbb401',\n",
       "  '25f4fa77-c584-11ee-8e1c-e0d55edbb401',\n",
       "  '25f4fa78-c584-11ee-b0cc-e0d55edbb401',\n",
       "  '25f4fa79-c584-11ee-a05e-e0d55edbb401',\n",
       "  '25f4fa7a-c584-11ee-900e-e0d55edbb401',\n",
       "  '25f4fa7b-c584-11ee-a37d-e0d55edbb401',\n",
       "  '25f4fa7c-c584-11ee-9572-e0d55edbb401',\n",
       "  '25f4fa7d-c584-11ee-8757-e0d55edbb401',\n",
       "  '25f4fa7e-c584-11ee-9041-e0d55edbb401',\n",
       "  '25f4fa7f-c584-11ee-8057-e0d55edbb401',\n",
       "  '25f4fa80-c584-11ee-82d7-e0d55edbb401',\n",
       "  '25f4fa81-c584-11ee-8d4b-e0d55edbb401',\n",
       "  '25f4fa82-c584-11ee-8618-e0d55edbb401',\n",
       "  '25f4fa83-c584-11ee-a579-e0d55edbb401',\n",
       "  '25f4fa84-c584-11ee-8472-e0d55edbb401',\n",
       "  '25f4fa85-c584-11ee-b1f1-e0d55edbb401',\n",
       "  '25f4fa86-c584-11ee-869b-e0d55edbb401',\n",
       "  '25f4fa87-c584-11ee-8c56-e0d55edbb401',\n",
       "  '25f4fa88-c584-11ee-99cd-e0d55edbb401',\n",
       "  '25f4fa89-c584-11ee-8725-e0d55edbb401',\n",
       "  '25f4fa8a-c584-11ee-a1c9-e0d55edbb401',\n",
       "  '25f4fa8b-c584-11ee-a72f-e0d55edbb401',\n",
       "  '25f4fa8c-c584-11ee-9e32-e0d55edbb401',\n",
       "  '25f4fa8d-c584-11ee-ad56-e0d55edbb401',\n",
       "  '25f4fa8e-c584-11ee-8554-e0d55edbb401',\n",
       "  '25f4fa8f-c584-11ee-a711-e0d55edbb401',\n",
       "  '25f4fa90-c584-11ee-891c-e0d55edbb401',\n",
       "  '25f4fa91-c584-11ee-824b-e0d55edbb401',\n",
       "  '25f4fa92-c584-11ee-bb18-e0d55edbb401',\n",
       "  '25f4fa93-c584-11ee-84ed-e0d55edbb401',\n",
       "  '25f4fa94-c584-11ee-84e2-e0d55edbb401',\n",
       "  '25f4fa95-c584-11ee-8f65-e0d55edbb401',\n",
       "  '25f4fa96-c584-11ee-9c4e-e0d55edbb401',\n",
       "  '25f4fa97-c584-11ee-b40f-e0d55edbb401',\n",
       "  '25f4fa98-c584-11ee-9c7c-e0d55edbb401',\n",
       "  '25f4fa99-c584-11ee-afce-e0d55edbb401',\n",
       "  '25f4fa9a-c584-11ee-8938-e0d55edbb401',\n",
       "  '25f4fa9b-c584-11ee-83eb-e0d55edbb401',\n",
       "  '25f4fa9c-c584-11ee-b497-e0d55edbb401',\n",
       "  '25f4fa9d-c584-11ee-880e-e0d55edbb401',\n",
       "  '25f4fa9e-c584-11ee-9227-e0d55edbb401',\n",
       "  '25f4fa9f-c584-11ee-8a3b-e0d55edbb401',\n",
       "  '25f4faa0-c584-11ee-9640-e0d55edbb401',\n",
       "  '25f4faa1-c584-11ee-a00b-e0d55edbb401',\n",
       "  '25f4faa2-c584-11ee-a13b-e0d55edbb401',\n",
       "  '25f4faa3-c584-11ee-93b0-e0d55edbb401',\n",
       "  '25f4faa4-c584-11ee-a0ce-e0d55edbb401',\n",
       "  '25f4faa5-c584-11ee-8af6-e0d55edbb401',\n",
       "  '25f4faa6-c584-11ee-9d43-e0d55edbb401',\n",
       "  '25f4faa7-c584-11ee-850e-e0d55edbb401',\n",
       "  '25f4faa8-c584-11ee-aada-e0d55edbb401',\n",
       "  '25f4faa9-c584-11ee-91b5-e0d55edbb401',\n",
       "  '25f4faaa-c584-11ee-a9ea-e0d55edbb401',\n",
       "  '25f4faab-c584-11ee-8e2b-e0d55edbb401',\n",
       "  '25f4faac-c584-11ee-ba0d-e0d55edbb401',\n",
       "  '25f4faad-c584-11ee-86fe-e0d55edbb401',\n",
       "  '25f4faae-c584-11ee-bd0e-e0d55edbb401',\n",
       "  '25f4faaf-c584-11ee-9ab2-e0d55edbb401',\n",
       "  '25f4fab0-c584-11ee-a9ad-e0d55edbb401',\n",
       "  '25f4fab1-c584-11ee-97b1-e0d55edbb401',\n",
       "  '25f4fab2-c584-11ee-becb-e0d55edbb401',\n",
       "  '25f4fab3-c584-11ee-8c0f-e0d55edbb401',\n",
       "  '25f4fab4-c584-11ee-85bc-e0d55edbb401',\n",
       "  '25f4fab5-c584-11ee-8c4d-e0d55edbb401',\n",
       "  '25f4fab6-c584-11ee-8318-e0d55edbb401',\n",
       "  '25f4fab7-c584-11ee-ae82-e0d55edbb401',\n",
       "  '25f4fab8-c584-11ee-8ce3-e0d55edbb401',\n",
       "  '25f4fab9-c584-11ee-bf82-e0d55edbb401',\n",
       "  '25f4faba-c584-11ee-a09c-e0d55edbb401',\n",
       "  '25f4fabb-c584-11ee-b7a5-e0d55edbb401',\n",
       "  '25f4fabc-c584-11ee-823e-e0d55edbb401',\n",
       "  '25f4fabd-c584-11ee-812c-e0d55edbb401',\n",
       "  '25f4fabe-c584-11ee-a348-e0d55edbb401',\n",
       "  '25f4fabf-c584-11ee-939a-e0d55edbb401',\n",
       "  '25f4fac0-c584-11ee-b9ef-e0d55edbb401',\n",
       "  '25f4fac1-c584-11ee-9279-e0d55edbb401',\n",
       "  '25f4fac2-c584-11ee-ae70-e0d55edbb401',\n",
       "  '25f4fac3-c584-11ee-8d54-e0d55edbb401',\n",
       "  '25f4fac4-c584-11ee-b122-e0d55edbb401',\n",
       "  '25f4fac5-c584-11ee-8b5b-e0d55edbb401',\n",
       "  '25f4fac6-c584-11ee-a243-e0d55edbb401',\n",
       "  '25f4fac7-c584-11ee-956e-e0d55edbb401',\n",
       "  '25f4fac8-c584-11ee-b026-e0d55edbb401',\n",
       "  '25f4fac9-c584-11ee-a53e-e0d55edbb401',\n",
       "  '25f4faca-c584-11ee-8eeb-e0d55edbb401',\n",
       "  '25f4facb-c584-11ee-b1f5-e0d55edbb401',\n",
       "  '25f4facc-c584-11ee-a9aa-e0d55edbb401',\n",
       "  '25f4facd-c584-11ee-96ce-e0d55edbb401',\n",
       "  '25f4face-c584-11ee-bb16-e0d55edbb401',\n",
       "  '25f4facf-c584-11ee-8334-e0d55edbb401',\n",
       "  '25f4fad0-c584-11ee-8eeb-e0d55edbb401',\n",
       "  '25f4fad1-c584-11ee-aca4-e0d55edbb401',\n",
       "  '25f4fad2-c584-11ee-9065-e0d55edbb401',\n",
       "  '25f4fad3-c584-11ee-b14e-e0d55edbb401',\n",
       "  '25f4fad4-c584-11ee-9a2b-e0d55edbb401',\n",
       "  '25f4fad5-c584-11ee-a5eb-e0d55edbb401',\n",
       "  '25f4fad6-c584-11ee-939b-e0d55edbb401',\n",
       "  '25f4fad7-c584-11ee-ba7c-e0d55edbb401',\n",
       "  '25f4fad8-c584-11ee-9405-e0d55edbb401',\n",
       "  '25f4fad9-c584-11ee-9b1c-e0d55edbb401',\n",
       "  '25f4fada-c584-11ee-be01-e0d55edbb401',\n",
       "  '25f4fadb-c584-11ee-9c85-e0d55edbb401',\n",
       "  '25f4fadc-c584-11ee-a78c-e0d55edbb401',\n",
       "  '25f4fadd-c584-11ee-ac07-e0d55edbb401',\n",
       "  '25f4fade-c584-11ee-af74-e0d55edbb401',\n",
       "  '25f4fadf-c584-11ee-be34-e0d55edbb401',\n",
       "  '25f4fae0-c584-11ee-a6a6-e0d55edbb401',\n",
       "  '25f4fae1-c584-11ee-a037-e0d55edbb401',\n",
       "  '25f4fae2-c584-11ee-84f9-e0d55edbb401',\n",
       "  '25f4fae3-c584-11ee-87c4-e0d55edbb401',\n",
       "  '25f4fae4-c584-11ee-8867-e0d55edbb401',\n",
       "  '25f4fae5-c584-11ee-b28f-e0d55edbb401',\n",
       "  '25f4fae6-c584-11ee-a76a-e0d55edbb401',\n",
       "  '25f4fae7-c584-11ee-b96a-e0d55edbb401',\n",
       "  '25f4fae8-c584-11ee-80e4-e0d55edbb401',\n",
       "  '25f4fae9-c584-11ee-a828-e0d55edbb401',\n",
       "  '25f4faea-c584-11ee-96ae-e0d55edbb401',\n",
       "  '25f4faeb-c584-11ee-a479-e0d55edbb401',\n",
       "  '25f4faec-c584-11ee-81b3-e0d55edbb401',\n",
       "  '25f4faed-c584-11ee-bd5f-e0d55edbb401',\n",
       "  '25f4faee-c584-11ee-8d1a-e0d55edbb401',\n",
       "  '25f4faef-c584-11ee-aa6f-e0d55edbb401',\n",
       "  '25f4faf0-c584-11ee-a1bd-e0d55edbb401',\n",
       "  '25f4faf1-c584-11ee-9513-e0d55edbb401',\n",
       "  '25f4faf2-c584-11ee-bd29-e0d55edbb401',\n",
       "  '25f4faf3-c584-11ee-9384-e0d55edbb401',\n",
       "  '25f4faf4-c584-11ee-9b39-e0d55edbb401',\n",
       "  '25f4faf5-c584-11ee-b487-e0d55edbb401',\n",
       "  '25f4faf6-c584-11ee-b206-e0d55edbb401',\n",
       "  '25f4faf7-c584-11ee-8373-e0d55edbb401',\n",
       "  '25f4faf8-c584-11ee-808c-e0d55edbb401',\n",
       "  '25f4faf9-c584-11ee-8dd4-e0d55edbb401',\n",
       "  '25f4fafa-c584-11ee-b341-e0d55edbb401',\n",
       "  '25f4fafb-c584-11ee-b85d-e0d55edbb401',\n",
       "  '25f4fafc-c584-11ee-9a53-e0d55edbb401',\n",
       "  '25f4fafd-c584-11ee-b4a2-e0d55edbb401',\n",
       "  '25f4fafe-c584-11ee-8627-e0d55edbb401',\n",
       "  '25f4faff-c584-11ee-a906-e0d55edbb401',\n",
       "  '25f4fb00-c584-11ee-a2d7-e0d55edbb401',\n",
       "  '25f4fb01-c584-11ee-b12a-e0d55edbb401',\n",
       "  '25f4fb02-c584-11ee-8088-e0d55edbb401',\n",
       "  '25f4fb03-c584-11ee-a993-e0d55edbb401',\n",
       "  '25f4fb04-c584-11ee-9d77-e0d55edbb401',\n",
       "  '25f4fb05-c584-11ee-a5e3-e0d55edbb401',\n",
       "  '25f4fb06-c584-11ee-a231-e0d55edbb401',\n",
       "  '25f4fb07-c584-11ee-8d53-e0d55edbb401',\n",
       "  '25f4fb08-c584-11ee-95d1-e0d55edbb401',\n",
       "  '25f4fb09-c584-11ee-ac01-e0d55edbb401',\n",
       "  '25f4fb0a-c584-11ee-bd72-e0d55edbb401',\n",
       "  '25f4fb0b-c584-11ee-b93a-e0d55edbb401',\n",
       "  '25f4fb0c-c584-11ee-8ef7-e0d55edbb401',\n",
       "  '25f4fb0d-c584-11ee-961e-e0d55edbb401',\n",
       "  '25f4fb0e-c584-11ee-829c-e0d55edbb401',\n",
       "  '25f4fb0f-c584-11ee-8fb9-e0d55edbb401',\n",
       "  '25f4fb10-c584-11ee-b114-e0d55edbb401',\n",
       "  '25f4fb11-c584-11ee-8ebf-e0d55edbb401',\n",
       "  '25f4fb12-c584-11ee-9c87-e0d55edbb401',\n",
       "  '25f4fb13-c584-11ee-9bc7-e0d55edbb401',\n",
       "  '25f4fb14-c584-11ee-9b9a-e0d55edbb401',\n",
       "  '25f4fb15-c584-11ee-9327-e0d55edbb401',\n",
       "  '25f4fb16-c584-11ee-91d8-e0d55edbb401',\n",
       "  '25f4fb17-c584-11ee-9d32-e0d55edbb401',\n",
       "  '25f4fb18-c584-11ee-aa4d-e0d55edbb401',\n",
       "  '25f4fb19-c584-11ee-9b6b-e0d55edbb401',\n",
       "  '25f4fb1a-c584-11ee-bfb3-e0d55edbb401',\n",
       "  '25f4fb1b-c584-11ee-916d-e0d55edbb401',\n",
       "  '25f4fb1c-c584-11ee-a96d-e0d55edbb401',\n",
       "  '25f4fb1d-c584-11ee-9a79-e0d55edbb401',\n",
       "  '25f4fb1e-c584-11ee-b1dc-e0d55edbb401',\n",
       "  '25f4fb1f-c584-11ee-b56a-e0d55edbb401',\n",
       "  '25f4fb20-c584-11ee-b10e-e0d55edbb401',\n",
       "  '25f4fb21-c584-11ee-9668-e0d55edbb401',\n",
       "  '25f4fb22-c584-11ee-a6a8-e0d55edbb401',\n",
       "  '25f4fb23-c584-11ee-a882-e0d55edbb401',\n",
       "  '25f4fb24-c584-11ee-9c1e-e0d55edbb401',\n",
       "  '25f4fb25-c584-11ee-947d-e0d55edbb401',\n",
       "  '25f4fb26-c584-11ee-904b-e0d55edbb401',\n",
       "  '25f4fb27-c584-11ee-9ea1-e0d55edbb401',\n",
       "  '25f4fb28-c584-11ee-8e28-e0d55edbb401',\n",
       "  '25f4fb29-c584-11ee-b3a1-e0d55edbb401',\n",
       "  '25f4fb2a-c584-11ee-aaf9-e0d55edbb401',\n",
       "  '25f4fb2b-c584-11ee-880a-e0d55edbb401',\n",
       "  '25f4fb2c-c584-11ee-95a5-e0d55edbb401',\n",
       "  '25f4fb2d-c584-11ee-a774-e0d55edbb401',\n",
       "  '25f4fb2e-c584-11ee-bbda-e0d55edbb401',\n",
       "  '25f4fb2f-c584-11ee-8a7a-e0d55edbb401',\n",
       "  '25f4fb30-c584-11ee-9d1b-e0d55edbb401',\n",
       "  '25f4fb31-c584-11ee-a196-e0d55edbb401',\n",
       "  '25f4fb32-c584-11ee-9510-e0d55edbb401',\n",
       "  '25f4fb33-c584-11ee-81ba-e0d55edbb401',\n",
       "  '25f4fb34-c584-11ee-816d-e0d55edbb401',\n",
       "  '25f4fb35-c584-11ee-8b37-e0d55edbb401',\n",
       "  '25f4fb36-c584-11ee-ba2d-e0d55edbb401',\n",
       "  '25f4fb37-c584-11ee-bfa0-e0d55edbb401',\n",
       "  '25f4fb38-c584-11ee-9819-e0d55edbb401',\n",
       "  '25f4fb39-c584-11ee-95d0-e0d55edbb401',\n",
       "  '25f4fb3a-c584-11ee-a2d2-e0d55edbb401',\n",
       "  '25f4fb3b-c584-11ee-b165-e0d55edbb401',\n",
       "  '25f4fb3c-c584-11ee-80d7-e0d55edbb401',\n",
       "  '25f4fb3d-c584-11ee-b802-e0d55edbb401',\n",
       "  '25f4fb3e-c584-11ee-92f5-e0d55edbb401',\n",
       "  '25f4fb3f-c584-11ee-afee-e0d55edbb401',\n",
       "  '25f4fb40-c584-11ee-8e0d-e0d55edbb401',\n",
       "  '25f4fb41-c584-11ee-844e-e0d55edbb401',\n",
       "  '25f4fb42-c584-11ee-a078-e0d55edbb401',\n",
       "  '25f4fb43-c584-11ee-ae88-e0d55edbb401',\n",
       "  '25f4fb44-c584-11ee-a4b1-e0d55edbb401',\n",
       "  '25f4fb45-c584-11ee-b43c-e0d55edbb401',\n",
       "  '25f4fb46-c584-11ee-84d4-e0d55edbb401',\n",
       "  '25f4fb47-c584-11ee-8a09-e0d55edbb401',\n",
       "  '25f4fb48-c584-11ee-b66d-e0d55edbb401',\n",
       "  '25f4fb49-c584-11ee-a521-e0d55edbb401',\n",
       "  '25f4fb4a-c584-11ee-be87-e0d55edbb401',\n",
       "  '25f4fb4b-c584-11ee-b70c-e0d55edbb401',\n",
       "  '25f4fb4c-c584-11ee-a50c-e0d55edbb401',\n",
       "  '25f4fb4d-c584-11ee-9cf5-e0d55edbb401',\n",
       "  '25f4fb4e-c584-11ee-a064-e0d55edbb401',\n",
       "  '25f4fb4f-c584-11ee-b935-e0d55edbb401',\n",
       "  '25f4fb50-c584-11ee-b0b7-e0d55edbb401',\n",
       "  '25f4fb51-c584-11ee-94e8-e0d55edbb401',\n",
       "  '25f4fb52-c584-11ee-b6cd-e0d55edbb401',\n",
       "  '25f4fb53-c584-11ee-aff8-e0d55edbb401',\n",
       "  '25f4fb54-c584-11ee-bc66-e0d55edbb401',\n",
       "  '25f4fb55-c584-11ee-92f0-e0d55edbb401',\n",
       "  '25f4fb56-c584-11ee-b685-e0d55edbb401',\n",
       "  '25f4fb57-c584-11ee-b36a-e0d55edbb401',\n",
       "  '25f4fb58-c584-11ee-8918-e0d55edbb401',\n",
       "  '25f4fb59-c584-11ee-b6cb-e0d55edbb401',\n",
       "  '25f4fb5a-c584-11ee-9433-e0d55edbb401',\n",
       "  '25f4fb5b-c584-11ee-8f5a-e0d55edbb401',\n",
       "  '25f4fb5c-c584-11ee-9674-e0d55edbb401',\n",
       "  '25f4fb5d-c584-11ee-9f43-e0d55edbb401',\n",
       "  '25f4fb5e-c584-11ee-86c2-e0d55edbb401',\n",
       "  '25f4fb5f-c584-11ee-9bb9-e0d55edbb401',\n",
       "  '25f4fb60-c584-11ee-85fc-e0d55edbb401',\n",
       "  '25f4fb61-c584-11ee-88b9-e0d55edbb401',\n",
       "  '25f4fb62-c584-11ee-9d23-e0d55edbb401',\n",
       "  '25f4fb63-c584-11ee-8128-e0d55edbb401',\n",
       "  '25f4fb64-c584-11ee-ade0-e0d55edbb401',\n",
       "  '25f4fb65-c584-11ee-985a-e0d55edbb401',\n",
       "  '25f4fb66-c584-11ee-a1e1-e0d55edbb401',\n",
       "  '25f4fb67-c584-11ee-9a98-e0d55edbb401',\n",
       "  '25f4fb68-c584-11ee-b743-e0d55edbb401',\n",
       "  '25f4fb69-c584-11ee-8e98-e0d55edbb401',\n",
       "  '25f4fb6a-c584-11ee-aee7-e0d55edbb401',\n",
       "  '25f4fb6b-c584-11ee-a682-e0d55edbb401',\n",
       "  '25f4fb6c-c584-11ee-837a-e0d55edbb401',\n",
       "  '25f4fb6d-c584-11ee-b895-e0d55edbb401',\n",
       "  '25f4fb6e-c584-11ee-902e-e0d55edbb401',\n",
       "  '25f4fb6f-c584-11ee-b35b-e0d55edbb401',\n",
       "  '25f4fb70-c584-11ee-92de-e0d55edbb401',\n",
       "  '25f4fb71-c584-11ee-a1b0-e0d55edbb401',\n",
       "  '25f4fb72-c584-11ee-883d-e0d55edbb401',\n",
       "  '25f4fb73-c584-11ee-8708-e0d55edbb401',\n",
       "  '25f4fb74-c584-11ee-b871-e0d55edbb401',\n",
       "  '25f4fb75-c584-11ee-a28c-e0d55edbb401',\n",
       "  '25f4fb76-c584-11ee-9b3c-e0d55edbb401',\n",
       "  '25f4fb77-c584-11ee-8c99-e0d55edbb401',\n",
       "  '25f4fb78-c584-11ee-9485-e0d55edbb401',\n",
       "  '25f4fb79-c584-11ee-98f1-e0d55edbb401',\n",
       "  '25f4fb7a-c584-11ee-bb8e-e0d55edbb401',\n",
       "  '25f4fb7b-c584-11ee-a84f-e0d55edbb401',\n",
       "  '25f4fb7c-c584-11ee-835e-e0d55edbb401',\n",
       "  '25f4fb7d-c584-11ee-994e-e0d55edbb401',\n",
       "  '25f4fb7e-c584-11ee-9420-e0d55edbb401',\n",
       "  '25f4fb7f-c584-11ee-aace-e0d55edbb401',\n",
       "  '25f4fb80-c584-11ee-bc40-e0d55edbb401',\n",
       "  '25f4fb81-c584-11ee-9e80-e0d55edbb401',\n",
       "  '25f4fb82-c584-11ee-ae34-e0d55edbb401',\n",
       "  '25f4fb83-c584-11ee-a50d-e0d55edbb401',\n",
       "  '25f4fb84-c584-11ee-ae19-e0d55edbb401',\n",
       "  '25f4fb85-c584-11ee-a9d9-e0d55edbb401',\n",
       "  '25f4fb86-c584-11ee-aef6-e0d55edbb401',\n",
       "  '25f4fb87-c584-11ee-9d78-e0d55edbb401',\n",
       "  '25f4fb88-c584-11ee-a030-e0d55edbb401',\n",
       "  '25f4fb89-c584-11ee-840a-e0d55edbb401',\n",
       "  '25f4fb8a-c584-11ee-9029-e0d55edbb401',\n",
       "  '25f4fb8b-c584-11ee-a274-e0d55edbb401',\n",
       "  '25f4fb8c-c584-11ee-ab21-e0d55edbb401',\n",
       "  '25f4fb8d-c584-11ee-b450-e0d55edbb401',\n",
       "  '25f4fb8e-c584-11ee-9532-e0d55edbb401',\n",
       "  '25f4fb8f-c584-11ee-a52e-e0d55edbb401',\n",
       "  '25f4fb90-c584-11ee-8d6e-e0d55edbb401',\n",
       "  '25f4fb91-c584-11ee-8f34-e0d55edbb401',\n",
       "  '25f4fb92-c584-11ee-8c14-e0d55edbb401',\n",
       "  '25f4fb93-c584-11ee-b216-e0d55edbb401',\n",
       "  '25f4fb94-c584-11ee-bf97-e0d55edbb401',\n",
       "  '25f4fb95-c584-11ee-b60e-e0d55edbb401',\n",
       "  '25f4fb96-c584-11ee-9083-e0d55edbb401',\n",
       "  '25f4fb97-c584-11ee-bdde-e0d55edbb401',\n",
       "  '25f4fb98-c584-11ee-b514-e0d55edbb401',\n",
       "  '25f4fb99-c584-11ee-899f-e0d55edbb401',\n",
       "  '25f4fb9a-c584-11ee-964e-e0d55edbb401',\n",
       "  '25f4fb9b-c584-11ee-aa52-e0d55edbb401',\n",
       "  '25f4fb9c-c584-11ee-a549-e0d55edbb401',\n",
       "  '25f4fb9d-c584-11ee-92d5-e0d55edbb401',\n",
       "  '25f4fb9e-c584-11ee-a4b8-e0d55edbb401',\n",
       "  '25f4fb9f-c584-11ee-854a-e0d55edbb401',\n",
       "  '25f4fba0-c584-11ee-9f17-e0d55edbb401',\n",
       "  '25f4fba1-c584-11ee-88c6-e0d55edbb401',\n",
       "  '25f4fba2-c584-11ee-935a-e0d55edbb401',\n",
       "  '25f4fba3-c584-11ee-9bef-e0d55edbb401',\n",
       "  '25f4fba4-c584-11ee-94a9-e0d55edbb401',\n",
       "  '25f4fba5-c584-11ee-8c3d-e0d55edbb401',\n",
       "  '25f4fba6-c584-11ee-8737-e0d55edbb401',\n",
       "  '25f4fba7-c584-11ee-a5df-e0d55edbb401',\n",
       "  '25f4fba8-c584-11ee-8657-e0d55edbb401',\n",
       "  '25f4fba9-c584-11ee-9b7f-e0d55edbb401',\n",
       "  '25f4fbaa-c584-11ee-a509-e0d55edbb401',\n",
       "  '25f4fbab-c584-11ee-9c17-e0d55edbb401',\n",
       "  '25f4fbac-c584-11ee-b98c-e0d55edbb401',\n",
       "  '25f4fbad-c584-11ee-9001-e0d55edbb401',\n",
       "  '25f4fbae-c584-11ee-b15b-e0d55edbb401',\n",
       "  '25f4fbaf-c584-11ee-b2d4-e0d55edbb401',\n",
       "  '25f4fbb0-c584-11ee-b7b7-e0d55edbb401',\n",
       "  '25f4fbb1-c584-11ee-a544-e0d55edbb401',\n",
       "  '25f4fbb2-c584-11ee-821e-e0d55edbb401',\n",
       "  '25f4fbb3-c584-11ee-b685-e0d55edbb401',\n",
       "  '25f4fbb4-c584-11ee-a80d-e0d55edbb401',\n",
       "  '25f4fbb5-c584-11ee-8dd1-e0d55edbb401',\n",
       "  '25f4fbb6-c584-11ee-8892-e0d55edbb401',\n",
       "  '25f4fbb7-c584-11ee-b13b-e0d55edbb401',\n",
       "  '25f4fbb8-c584-11ee-82bd-e0d55edbb401',\n",
       "  '25f4fbb9-c584-11ee-98a9-e0d55edbb401',\n",
       "  '25f4fbba-c584-11ee-afeb-e0d55edbb401',\n",
       "  '25f4fbbb-c584-11ee-9ac1-e0d55edbb401',\n",
       "  '25f4fbbc-c584-11ee-bd4d-e0d55edbb401',\n",
       "  '25f4fbbd-c584-11ee-8096-e0d55edbb401',\n",
       "  '25f4fbbe-c584-11ee-ad4d-e0d55edbb401',\n",
       "  '25f4fbbf-c584-11ee-871f-e0d55edbb401',\n",
       "  '25f4fbc0-c584-11ee-af04-e0d55edbb401',\n",
       "  '25f4fbc1-c584-11ee-85f7-e0d55edbb401',\n",
       "  '25f4fbc2-c584-11ee-9d16-e0d55edbb401',\n",
       "  '25f4fbc3-c584-11ee-8298-e0d55edbb401',\n",
       "  '25f4fbc4-c584-11ee-8c8a-e0d55edbb401',\n",
       "  '25f4fbc5-c584-11ee-816c-e0d55edbb401',\n",
       "  '25f4fbc6-c584-11ee-8ac2-e0d55edbb401',\n",
       "  '25f4fbc7-c584-11ee-88bb-e0d55edbb401',\n",
       "  '25f4fbc8-c584-11ee-8fe6-e0d55edbb401',\n",
       "  '25f4fbc9-c584-11ee-b0d0-e0d55edbb401',\n",
       "  '25f4fbca-c584-11ee-9eb9-e0d55edbb401',\n",
       "  '25f4fbcb-c584-11ee-9429-e0d55edbb401',\n",
       "  '25f4fbcc-c584-11ee-9977-e0d55edbb401',\n",
       "  '25f4fbcd-c584-11ee-900a-e0d55edbb401',\n",
       "  '25f4fbce-c584-11ee-839e-e0d55edbb401',\n",
       "  '25f4fbcf-c584-11ee-bf47-e0d55edbb401',\n",
       "  '25f4fbd0-c584-11ee-9451-e0d55edbb401',\n",
       "  '25f4fbd1-c584-11ee-ab8a-e0d55edbb401',\n",
       "  '25f4fbd2-c584-11ee-9746-e0d55edbb401',\n",
       "  '25f4fbd3-c584-11ee-8034-e0d55edbb401',\n",
       "  '25f4fbd4-c584-11ee-8f97-e0d55edbb401',\n",
       "  '25f4fbd5-c584-11ee-bedc-e0d55edbb401',\n",
       "  '25f4fbd6-c584-11ee-af7c-e0d55edbb401',\n",
       "  '25f4fbd7-c584-11ee-8f4f-e0d55edbb401',\n",
       "  '25f4fbd8-c584-11ee-bb2e-e0d55edbb401',\n",
       "  '25f4fbd9-c584-11ee-a199-e0d55edbb401',\n",
       "  '25f4fbda-c584-11ee-b2ed-e0d55edbb401',\n",
       "  '25f4fbdb-c584-11ee-aa45-e0d55edbb401',\n",
       "  '25f4fbdc-c584-11ee-bd3c-e0d55edbb401',\n",
       "  '25f4fbdd-c584-11ee-8411-e0d55edbb401',\n",
       "  '25f4fbde-c584-11ee-a52c-e0d55edbb401',\n",
       "  '25f4fbdf-c584-11ee-a0e2-e0d55edbb401',\n",
       "  '25f4fbe0-c584-11ee-b58b-e0d55edbb401',\n",
       "  '25f4fbe1-c584-11ee-847a-e0d55edbb401',\n",
       "  '25f4fbe2-c584-11ee-b516-e0d55edbb401',\n",
       "  '25f4fbe3-c584-11ee-aa6f-e0d55edbb401',\n",
       "  '25f4fbe4-c584-11ee-a935-e0d55edbb401',\n",
       "  '25f4fbe5-c584-11ee-8a2e-e0d55edbb401',\n",
       "  '25f4fbe6-c584-11ee-907f-e0d55edbb401',\n",
       "  '25f4fbe7-c584-11ee-a223-e0d55edbb401',\n",
       "  '25f4fbe8-c584-11ee-9f08-e0d55edbb401',\n",
       "  '25f4fbe9-c584-11ee-8df9-e0d55edbb401',\n",
       "  '25f4fbea-c584-11ee-a2db-e0d55edbb401',\n",
       "  '25f4fbeb-c584-11ee-a119-e0d55edbb401',\n",
       "  '25f4fbec-c584-11ee-be1d-e0d55edbb401',\n",
       "  '25f4fbed-c584-11ee-9291-e0d55edbb401',\n",
       "  '25f4fbee-c584-11ee-bbdc-e0d55edbb401',\n",
       "  '25f4fbef-c584-11ee-ad83-e0d55edbb401',\n",
       "  '25f4fbf0-c584-11ee-b4dc-e0d55edbb401',\n",
       "  '25f4fbf1-c584-11ee-a4cf-e0d55edbb401',\n",
       "  '25f4fbf2-c584-11ee-a53d-e0d55edbb401',\n",
       "  '25f4fbf3-c584-11ee-a49c-e0d55edbb401',\n",
       "  '25f4fbf4-c584-11ee-ab35-e0d55edbb401',\n",
       "  '25f4fbf5-c584-11ee-9305-e0d55edbb401',\n",
       "  '25f4fbf6-c584-11ee-b603-e0d55edbb401',\n",
       "  '25f4fbf7-c584-11ee-bef5-e0d55edbb401',\n",
       "  '25f4fbf8-c584-11ee-a8b3-e0d55edbb401',\n",
       "  '25f4fbf9-c584-11ee-b542-e0d55edbb401',\n",
       "  '25f4fbfa-c584-11ee-be5f-e0d55edbb401',\n",
       "  '25f4fbfb-c584-11ee-9a6f-e0d55edbb401',\n",
       "  '25f4fbfc-c584-11ee-8e00-e0d55edbb401',\n",
       "  '25f4fbfd-c584-11ee-b03f-e0d55edbb401',\n",
       "  '25f4fbfe-c584-11ee-b326-e0d55edbb401',\n",
       "  '25f4fbff-c584-11ee-ad61-e0d55edbb401',\n",
       "  '25f4fc00-c584-11ee-8abe-e0d55edbb401',\n",
       "  '25f4fc01-c584-11ee-b5de-e0d55edbb401',\n",
       "  '25f4fc02-c584-11ee-a827-e0d55edbb401',\n",
       "  '25f4fc03-c584-11ee-9d10-e0d55edbb401',\n",
       "  '25f4fc04-c584-11ee-b836-e0d55edbb401',\n",
       "  '25f4fc05-c584-11ee-a92c-e0d55edbb401',\n",
       "  '25f4fc06-c584-11ee-99c5-e0d55edbb401',\n",
       "  '25f4fc07-c584-11ee-8c14-e0d55edbb401',\n",
       "  '25f4fc08-c584-11ee-bb4a-e0d55edbb401',\n",
       "  '25f4fc09-c584-11ee-8e89-e0d55edbb401',\n",
       "  '25f4fc0a-c584-11ee-9b07-e0d55edbb401',\n",
       "  '25f4fc0b-c584-11ee-8db1-e0d55edbb401',\n",
       "  '25f4fc0c-c584-11ee-8de4-e0d55edbb401',\n",
       "  '25f4fc0d-c584-11ee-a9e9-e0d55edbb401',\n",
       "  '25f4fc0e-c584-11ee-ad90-e0d55edbb401',\n",
       "  '25f4fc0f-c584-11ee-972e-e0d55edbb401',\n",
       "  '25f4fc10-c584-11ee-a50d-e0d55edbb401',\n",
       "  '25f4fc11-c584-11ee-a441-e0d55edbb401',\n",
       "  '25f4fc12-c584-11ee-9fa2-e0d55edbb401',\n",
       "  '25f4fc13-c584-11ee-9494-e0d55edbb401',\n",
       "  '25f4fc14-c584-11ee-bfc0-e0d55edbb401',\n",
       "  '25f4fc15-c584-11ee-9408-e0d55edbb401',\n",
       "  '25f4fc16-c584-11ee-9390-e0d55edbb401',\n",
       "  '25f4fc17-c584-11ee-a374-e0d55edbb401',\n",
       "  '25f4fc18-c584-11ee-aadc-e0d55edbb401',\n",
       "  '25f4fc19-c584-11ee-a3ae-e0d55edbb401',\n",
       "  '25f4fc1a-c584-11ee-a9dc-e0d55edbb401',\n",
       "  '25f4fc1b-c584-11ee-89ae-e0d55edbb401',\n",
       "  '25f4fc1c-c584-11ee-95bb-e0d55edbb401',\n",
       "  '25f4fc1d-c584-11ee-bc92-e0d55edbb401',\n",
       "  '25f4fc1e-c584-11ee-9300-e0d55edbb401',\n",
       "  '25f4fc1f-c584-11ee-86a6-e0d55edbb401',\n",
       "  '25f4fc20-c584-11ee-a8ee-e0d55edbb401',\n",
       "  '25f4fc21-c584-11ee-b4f9-e0d55edbb401',\n",
       "  '25f4fc22-c584-11ee-a0ad-e0d55edbb401',\n",
       "  '25f4fc23-c584-11ee-8c2e-e0d55edbb401',\n",
       "  '25f4fc24-c584-11ee-9ef9-e0d55edbb401',\n",
       "  '25f4fc25-c584-11ee-9dea-e0d55edbb401',\n",
       "  '25f4fc26-c584-11ee-8c99-e0d55edbb401',\n",
       "  '25f4fc27-c584-11ee-82d7-e0d55edbb401',\n",
       "  '25f4fc28-c584-11ee-9019-e0d55edbb401',\n",
       "  '25f4fc29-c584-11ee-994e-e0d55edbb401',\n",
       "  '25f4fc2a-c584-11ee-98d6-e0d55edbb401',\n",
       "  '25f4fc2b-c584-11ee-941a-e0d55edbb401',\n",
       "  '25f4fc2c-c584-11ee-b658-e0d55edbb401',\n",
       "  '25f4fc2d-c584-11ee-9fca-e0d55edbb401',\n",
       "  '25f4fc2e-c584-11ee-993e-e0d55edbb401',\n",
       "  '25f4fc2f-c584-11ee-b6d2-e0d55edbb401',\n",
       "  '25f4fc30-c584-11ee-99cc-e0d55edbb401',\n",
       "  '25f4fc31-c584-11ee-b041-e0d55edbb401',\n",
       "  '25f4fc32-c584-11ee-9c4c-e0d55edbb401',\n",
       "  '25f4fc33-c584-11ee-ba8f-e0d55edbb401',\n",
       "  '25f4fc34-c584-11ee-b67a-e0d55edbb401',\n",
       "  '25f4fc35-c584-11ee-9eb3-e0d55edbb401',\n",
       "  '25f4fc36-c584-11ee-bc1a-e0d55edbb401',\n",
       "  '25f4fc37-c584-11ee-bc31-e0d55edbb401',\n",
       "  '25f4fc38-c584-11ee-9b25-e0d55edbb401',\n",
       "  '25f4fc39-c584-11ee-8939-e0d55edbb401',\n",
       "  '25f4fc3a-c584-11ee-a1db-e0d55edbb401',\n",
       "  '25f4fc3b-c584-11ee-ba37-e0d55edbb401',\n",
       "  '25f4fc3c-c584-11ee-925e-e0d55edbb401',\n",
       "  '25f4fc3d-c584-11ee-bec0-e0d55edbb401',\n",
       "  '25f4fc3e-c584-11ee-a00e-e0d55edbb401',\n",
       "  '25f4fc3f-c584-11ee-b610-e0d55edbb401',\n",
       "  '25f4fc40-c584-11ee-a626-e0d55edbb401',\n",
       "  '25f4fc41-c584-11ee-ad55-e0d55edbb401',\n",
       "  '25f4fc42-c584-11ee-8e3d-e0d55edbb401',\n",
       "  '25f4fc43-c584-11ee-bd9c-e0d55edbb401',\n",
       "  '25f4fc44-c584-11ee-8e33-e0d55edbb401',\n",
       "  '25f4fc45-c584-11ee-8da8-e0d55edbb401',\n",
       "  '25f4fc46-c584-11ee-b4bd-e0d55edbb401',\n",
       "  '25f4fc47-c584-11ee-865b-e0d55edbb401',\n",
       "  '25f4fc48-c584-11ee-af6a-e0d55edbb401',\n",
       "  '25f4fc49-c584-11ee-8ffe-e0d55edbb401',\n",
       "  '25f4fc4a-c584-11ee-bbe2-e0d55edbb401',\n",
       "  '25f4fc4b-c584-11ee-89f7-e0d55edbb401',\n",
       "  '25f4fc4c-c584-11ee-b8e6-e0d55edbb401',\n",
       "  '25f4fc4d-c584-11ee-a3ae-e0d55edbb401',\n",
       "  '25f4fc4e-c584-11ee-8309-e0d55edbb401',\n",
       "  '25f4fc4f-c584-11ee-a8ca-e0d55edbb401',\n",
       "  '25f4fc50-c584-11ee-86d0-e0d55edbb401',\n",
       "  '25f4fc51-c584-11ee-9620-e0d55edbb401',\n",
       "  '25f4fc52-c584-11ee-a665-e0d55edbb401',\n",
       "  '25f4fc53-c584-11ee-8475-e0d55edbb401',\n",
       "  '25f4fc54-c584-11ee-ab9f-e0d55edbb401',\n",
       "  '25f4fc55-c584-11ee-815c-e0d55edbb401',\n",
       "  '25f4fc56-c584-11ee-a212-e0d55edbb401',\n",
       "  '25f4fc57-c584-11ee-aeeb-e0d55edbb401',\n",
       "  '25f4fc58-c584-11ee-a4dc-e0d55edbb401',\n",
       "  '25f4fc59-c584-11ee-9ad1-e0d55edbb401',\n",
       "  '25f4fc5a-c584-11ee-8b07-e0d55edbb401',\n",
       "  '25f4fc5b-c584-11ee-9087-e0d55edbb401',\n",
       "  '25f4fc5c-c584-11ee-b670-e0d55edbb401',\n",
       "  '25f4fc5d-c584-11ee-abf8-e0d55edbb401',\n",
       "  '25f4fc5e-c584-11ee-bbe9-e0d55edbb401',\n",
       "  '25f4fc5f-c584-11ee-9f8d-e0d55edbb401',\n",
       "  '25f4fc60-c584-11ee-8cc3-e0d55edbb401',\n",
       "  '25f4fc61-c584-11ee-a7c3-e0d55edbb401',\n",
       "  '25f4fc62-c584-11ee-b4c3-e0d55edbb401',\n",
       "  '25f4fc63-c584-11ee-9aa5-e0d55edbb401',\n",
       "  '25f4fc64-c584-11ee-bb61-e0d55edbb401',\n",
       "  '25f4fc65-c584-11ee-bc66-e0d55edbb401',\n",
       "  '25f4fc66-c584-11ee-bcae-e0d55edbb401',\n",
       "  '25f4fc67-c584-11ee-ac48-e0d55edbb401',\n",
       "  '25f4fc68-c584-11ee-9873-e0d55edbb401',\n",
       "  '25f4fc69-c584-11ee-acc2-e0d55edbb401',\n",
       "  '25f4fc6a-c584-11ee-a3dc-e0d55edbb401',\n",
       "  '25f4fc6b-c584-11ee-b6bd-e0d55edbb401',\n",
       "  '25f4fc6c-c584-11ee-b64e-e0d55edbb401',\n",
       "  '25f4fc6d-c584-11ee-8183-e0d55edbb401',\n",
       "  '25f4fc6e-c584-11ee-ae81-e0d55edbb401',\n",
       "  '25f4fc6f-c584-11ee-b701-e0d55edbb401',\n",
       "  '25f4fc70-c584-11ee-8690-e0d55edbb401',\n",
       "  '25f4fc71-c584-11ee-ad7e-e0d55edbb401',\n",
       "  '25f4fc72-c584-11ee-88c6-e0d55edbb401',\n",
       "  '25f4fc73-c584-11ee-9f10-e0d55edbb401',\n",
       "  '25f4fc74-c584-11ee-b570-e0d55edbb401',\n",
       "  '25f4fc75-c584-11ee-894c-e0d55edbb401',\n",
       "  '25f4fc76-c584-11ee-846d-e0d55edbb401',\n",
       "  '25f4fc77-c584-11ee-beaa-e0d55edbb401',\n",
       "  '25f4fc78-c584-11ee-ab03-e0d55edbb401',\n",
       "  '25f4fc79-c584-11ee-a4a2-e0d55edbb401',\n",
       "  '25f4fc7a-c584-11ee-b69b-e0d55edbb401',\n",
       "  '25f4fc7b-c584-11ee-aeb7-e0d55edbb401',\n",
       "  '25f4fc7c-c584-11ee-9390-e0d55edbb401',\n",
       "  '25f4fc7d-c584-11ee-9342-e0d55edbb401',\n",
       "  '25f4fc7e-c584-11ee-9dd3-e0d55edbb401',\n",
       "  '25f4fc7f-c584-11ee-90cc-e0d55edbb401',\n",
       "  '25f4fc80-c584-11ee-9f3f-e0d55edbb401',\n",
       "  '25f4fc81-c584-11ee-be25-e0d55edbb401',\n",
       "  '25f4fc82-c584-11ee-b7e7-e0d55edbb401',\n",
       "  '25f4fc83-c584-11ee-b8e2-e0d55edbb401',\n",
       "  '25f4fc84-c584-11ee-8fa0-e0d55edbb401',\n",
       "  '25f4fc85-c584-11ee-b5f8-e0d55edbb401',\n",
       "  '25f4fc86-c584-11ee-941d-e0d55edbb401',\n",
       "  '25f4fc87-c584-11ee-a963-e0d55edbb401',\n",
       "  '25f4fc88-c584-11ee-b6e6-e0d55edbb401',\n",
       "  '25f4fc89-c584-11ee-9307-e0d55edbb401',\n",
       "  '25f4fc8a-c584-11ee-b0d9-e0d55edbb401',\n",
       "  '25f4fc8b-c584-11ee-8f9d-e0d55edbb401',\n",
       "  '25f4fc8c-c584-11ee-a7b7-e0d55edbb401',\n",
       "  '25f4fc8d-c584-11ee-8353-e0d55edbb401',\n",
       "  '25f4fc8e-c584-11ee-815e-e0d55edbb401',\n",
       "  '25f4fc8f-c584-11ee-a9de-e0d55edbb401',\n",
       "  '25f4fc90-c584-11ee-adc8-e0d55edbb401',\n",
       "  '25f4fc91-c584-11ee-a922-e0d55edbb401',\n",
       "  '25f4fc92-c584-11ee-8b51-e0d55edbb401',\n",
       "  '25f4fc93-c584-11ee-b52a-e0d55edbb401',\n",
       "  '25f4fc94-c584-11ee-bd6b-e0d55edbb401',\n",
       "  '25f4fc95-c584-11ee-a755-e0d55edbb401',\n",
       "  '25f4fc96-c584-11ee-a827-e0d55edbb401',\n",
       "  '25f4fc97-c584-11ee-b433-e0d55edbb401',\n",
       "  '25f4fc98-c584-11ee-b6f6-e0d55edbb401',\n",
       "  '25f4fc99-c584-11ee-a4ac-e0d55edbb401',\n",
       "  '25f4fc9a-c584-11ee-92f3-e0d55edbb401',\n",
       "  '25f4fc9b-c584-11ee-96c7-e0d55edbb401',\n",
       "  '25f4fc9c-c584-11ee-8b8d-e0d55edbb401',\n",
       "  '25f4fc9d-c584-11ee-b197-e0d55edbb401',\n",
       "  '25f4fc9e-c584-11ee-9f91-e0d55edbb401',\n",
       "  '25f4fc9f-c584-11ee-b607-e0d55edbb401',\n",
       "  '25f4fca0-c584-11ee-bd1f-e0d55edbb401',\n",
       "  '25f4fca1-c584-11ee-a447-e0d55edbb401',\n",
       "  '25f4fca2-c584-11ee-ae33-e0d55edbb401',\n",
       "  '25f4fca3-c584-11ee-91a7-e0d55edbb401',\n",
       "  '25f4fca4-c584-11ee-8b2c-e0d55edbb401',\n",
       "  '25f4fca5-c584-11ee-8518-e0d55edbb401',\n",
       "  '25f4fca6-c584-11ee-81a8-e0d55edbb401',\n",
       "  '25f4fca7-c584-11ee-879a-e0d55edbb401',\n",
       "  '25f4fca8-c584-11ee-b09d-e0d55edbb401',\n",
       "  '25f4fca9-c584-11ee-b175-e0d55edbb401',\n",
       "  '25f4fcaa-c584-11ee-8660-e0d55edbb401',\n",
       "  '25f4fcab-c584-11ee-a70d-e0d55edbb401',\n",
       "  '25f4fcac-c584-11ee-8e9b-e0d55edbb401',\n",
       "  '25f4fcad-c584-11ee-878f-e0d55edbb401',\n",
       "  '25f4fcae-c584-11ee-b08e-e0d55edbb401',\n",
       "  '25f4fcaf-c584-11ee-b741-e0d55edbb401',\n",
       "  '25f4fcb0-c584-11ee-b928-e0d55edbb401',\n",
       "  '25f4fcb1-c584-11ee-b836-e0d55edbb401',\n",
       "  '25f4fcb2-c584-11ee-9ea2-e0d55edbb401',\n",
       "  '25f4fcb3-c584-11ee-bf8d-e0d55edbb401',\n",
       "  '25f4fcb4-c584-11ee-8c93-e0d55edbb401',\n",
       "  '25f4fcb5-c584-11ee-bbbd-e0d55edbb401',\n",
       "  '25f4fcb6-c584-11ee-b0fe-e0d55edbb401',\n",
       "  '25f4fcb7-c584-11ee-841d-e0d55edbb401',\n",
       "  '25f4fcb8-c584-11ee-83b4-e0d55edbb401',\n",
       "  '25f4fcb9-c584-11ee-a227-e0d55edbb401',\n",
       "  '25f4fcba-c584-11ee-b53e-e0d55edbb401',\n",
       "  '25f4fcbb-c584-11ee-b26b-e0d55edbb401',\n",
       "  '25f4fcbc-c584-11ee-9173-e0d55edbb401',\n",
       "  '25f4fcbd-c584-11ee-8ea3-e0d55edbb401',\n",
       "  '25f4fcbe-c584-11ee-8e0b-e0d55edbb401',\n",
       "  '25f4fcbf-c584-11ee-856f-e0d55edbb401',\n",
       "  '25f4fcc0-c584-11ee-bb36-e0d55edbb401',\n",
       "  '25f4fcc1-c584-11ee-abfc-e0d55edbb401',\n",
       "  '25f4fcc2-c584-11ee-98fc-e0d55edbb401',\n",
       "  '25f4fcc3-c584-11ee-aa86-e0d55edbb401',\n",
       "  '25f4fcc4-c584-11ee-a242-e0d55edbb401',\n",
       "  '25f4fcc5-c584-11ee-8f4b-e0d55edbb401',\n",
       "  '25f4fcc6-c584-11ee-9740-e0d55edbb401',\n",
       "  '25f4fcc7-c584-11ee-a217-e0d55edbb401',\n",
       "  '25f4fcc8-c584-11ee-9fd4-e0d55edbb401',\n",
       "  '25f4fcc9-c584-11ee-a39b-e0d55edbb401',\n",
       "  '25f4fcca-c584-11ee-8686-e0d55edbb401',\n",
       "  '25f4fccb-c584-11ee-a9f6-e0d55edbb401',\n",
       "  '25f4fccc-c584-11ee-bf36-e0d55edbb401',\n",
       "  '25f4fccd-c584-11ee-9353-e0d55edbb401',\n",
       "  '25f4fcce-c584-11ee-938e-e0d55edbb401',\n",
       "  '25f4fccf-c584-11ee-8716-e0d55edbb401',\n",
       "  '25f4fcd0-c584-11ee-98b4-e0d55edbb401',\n",
       "  '25f4fcd1-c584-11ee-99a2-e0d55edbb401',\n",
       "  '25f4fcd2-c584-11ee-8465-e0d55edbb401',\n",
       "  '25f4fcd3-c584-11ee-a29e-e0d55edbb401',\n",
       "  '25f4fcd4-c584-11ee-a42d-e0d55edbb401',\n",
       "  '25f4fcd5-c584-11ee-b429-e0d55edbb401',\n",
       "  '25f4fcd6-c584-11ee-8aa3-e0d55edbb401',\n",
       "  '25f4fcd7-c584-11ee-aacb-e0d55edbb401',\n",
       "  '25f4fcd8-c584-11ee-b752-e0d55edbb401',\n",
       "  '25f4fcd9-c584-11ee-84a5-e0d55edbb401',\n",
       "  '25f4fcda-c584-11ee-96bf-e0d55edbb401',\n",
       "  '25f4fcdb-c584-11ee-8440-e0d55edbb401',\n",
       "  '25f4fcdc-c584-11ee-a902-e0d55edbb401',\n",
       "  '25f4fcdd-c584-11ee-a9a5-e0d55edbb401',\n",
       "  '25f4fcde-c584-11ee-ae2f-e0d55edbb401',\n",
       "  '25f4fcdf-c584-11ee-9b20-e0d55edbb401',\n",
       "  '25f4fce0-c584-11ee-8227-e0d55edbb401',\n",
       "  '25f4fce1-c584-11ee-8801-e0d55edbb401',\n",
       "  '25f4fce2-c584-11ee-92a9-e0d55edbb401',\n",
       "  '25f4fce3-c584-11ee-8dd0-e0d55edbb401',\n",
       "  '25f4fce4-c584-11ee-b7c4-e0d55edbb401',\n",
       "  '25f4fce5-c584-11ee-9183-e0d55edbb401',\n",
       "  '25f4fce6-c584-11ee-84ba-e0d55edbb401',\n",
       "  '25f4fce7-c584-11ee-892d-e0d55edbb401',\n",
       "  '25f4fce8-c584-11ee-b7ee-e0d55edbb401',\n",
       "  '25f4fce9-c584-11ee-b691-e0d55edbb401',\n",
       "  '25f4fcea-c584-11ee-a082-e0d55edbb401',\n",
       "  '25f4fceb-c584-11ee-b221-e0d55edbb401',\n",
       "  '25f4fcec-c584-11ee-81a4-e0d55edbb401',\n",
       "  '25f4fced-c584-11ee-b3d0-e0d55edbb401',\n",
       "  '25f4fcee-c584-11ee-95dc-e0d55edbb401',\n",
       "  '25f4fcef-c584-11ee-a5d4-e0d55edbb401',\n",
       "  '25f4fcf0-c584-11ee-9dc3-e0d55edbb401',\n",
       "  '25f4fcf1-c584-11ee-9911-e0d55edbb401',\n",
       "  '25f4fcf2-c584-11ee-8afb-e0d55edbb401',\n",
       "  '25f4fcf3-c584-11ee-b441-e0d55edbb401',\n",
       "  '25f4fcf4-c584-11ee-8be5-e0d55edbb401',\n",
       "  '25f4fcf5-c584-11ee-93fa-e0d55edbb401',\n",
       "  '25f4fcf6-c584-11ee-ac5d-e0d55edbb401',\n",
       "  '25f4fcf7-c584-11ee-a1fd-e0d55edbb401',\n",
       "  '25f4fcf8-c584-11ee-aaa5-e0d55edbb401',\n",
       "  '25f4fcf9-c584-11ee-b0a8-e0d55edbb401',\n",
       "  '25f4fcfa-c584-11ee-98a4-e0d55edbb401',\n",
       "  '25f4fcfb-c584-11ee-baf1-e0d55edbb401',\n",
       "  '25f4fcfc-c584-11ee-b5ce-e0d55edbb401',\n",
       "  '25f4fcfd-c584-11ee-a929-e0d55edbb401',\n",
       "  '25f4fcfe-c584-11ee-bb14-e0d55edbb401',\n",
       "  '25f4fcff-c584-11ee-97d3-e0d55edbb401',\n",
       "  '25f4fd00-c584-11ee-9ca3-e0d55edbb401',\n",
       "  '25f4fd01-c584-11ee-b6b2-e0d55edbb401',\n",
       "  '25f4fd02-c584-11ee-9830-e0d55edbb401',\n",
       "  '25f4fd03-c584-11ee-a68c-e0d55edbb401',\n",
       "  '25f4fd04-c584-11ee-8660-e0d55edbb401',\n",
       "  '25f4fd05-c584-11ee-b4f5-e0d55edbb401',\n",
       "  '25f4fd06-c584-11ee-92c9-e0d55edbb401',\n",
       "  '25f4fd07-c584-11ee-a7fc-e0d55edbb401',\n",
       "  '25f4fd08-c584-11ee-aa3a-e0d55edbb401',\n",
       "  '25f4fd09-c584-11ee-8f8f-e0d55edbb401',\n",
       "  '25f4fd0a-c584-11ee-982f-e0d55edbb401',\n",
       "  '25f4fd0b-c584-11ee-ad8d-e0d55edbb401',\n",
       "  '25f4fd0c-c584-11ee-8ae2-e0d55edbb401',\n",
       "  '25f4fd0d-c584-11ee-89a8-e0d55edbb401',\n",
       "  '25f4fd0e-c584-11ee-bca8-e0d55edbb401',\n",
       "  '25f4fd0f-c584-11ee-ae2a-e0d55edbb401',\n",
       "  '25f4fd10-c584-11ee-9611-e0d55edbb401',\n",
       "  '25f4fd11-c584-11ee-9929-e0d55edbb401',\n",
       "  '25f4fd12-c584-11ee-88c8-e0d55edbb401',\n",
       "  '25f4fd13-c584-11ee-8ae5-e0d55edbb401',\n",
       "  '25f4fd14-c584-11ee-8878-e0d55edbb401',\n",
       "  '25f4fd15-c584-11ee-b26f-e0d55edbb401',\n",
       "  '25f4fd16-c584-11ee-9248-e0d55edbb401',\n",
       "  '25f4fd17-c584-11ee-b4c5-e0d55edbb401',\n",
       "  '25f4fd18-c584-11ee-a23f-e0d55edbb401',\n",
       "  '25f4fd19-c584-11ee-b5d3-e0d55edbb401',\n",
       "  '25f4fd1a-c584-11ee-98cb-e0d55edbb401',\n",
       "  '25f4fd1b-c584-11ee-b6c8-e0d55edbb401',\n",
       "  '25f4fd1c-c584-11ee-a4c9-e0d55edbb401',\n",
       "  '25f4fd1d-c584-11ee-9cfa-e0d55edbb401',\n",
       "  '25f4fd1e-c584-11ee-809a-e0d55edbb401',\n",
       "  '25f4fd1f-c584-11ee-8af3-e0d55edbb401',\n",
       "  '25f4fd20-c584-11ee-9787-e0d55edbb401',\n",
       "  '25f4fd21-c584-11ee-835e-e0d55edbb401',\n",
       "  '25f4fd22-c584-11ee-9900-e0d55edbb401',\n",
       "  '25f4fd23-c584-11ee-a3d4-e0d55edbb401',\n",
       "  '25f4fd24-c584-11ee-8d5d-e0d55edbb401',\n",
       "  '25f4fd25-c584-11ee-9bc3-e0d55edbb401',\n",
       "  '25f4fd26-c584-11ee-834c-e0d55edbb401',\n",
       "  '25f4fd27-c584-11ee-850e-e0d55edbb401',\n",
       "  '25f4fd28-c584-11ee-9d2a-e0d55edbb401',\n",
       "  '25f4fd29-c584-11ee-b00b-e0d55edbb401',\n",
       "  '25f4fd2a-c584-11ee-bbf6-e0d55edbb401',\n",
       "  '25f4fd2b-c584-11ee-acc6-e0d55edbb401',\n",
       "  '25f4fd2c-c584-11ee-aadc-e0d55edbb401',\n",
       "  '25f4fd2d-c584-11ee-aeba-e0d55edbb401',\n",
       "  '25f4fd2e-c584-11ee-a1e4-e0d55edbb401',\n",
       "  '25f4fd2f-c584-11ee-8e2b-e0d55edbb401',\n",
       "  '25f4fd30-c584-11ee-bb53-e0d55edbb401',\n",
       "  '25f4fd31-c584-11ee-befc-e0d55edbb401',\n",
       "  '25f4fd32-c584-11ee-a8f6-e0d55edbb401',\n",
       "  '25f4fd33-c584-11ee-a6fe-e0d55edbb401',\n",
       "  '25f4fd34-c584-11ee-bce1-e0d55edbb401',\n",
       "  '25f4fd35-c584-11ee-b3f9-e0d55edbb401',\n",
       "  '25f4fd36-c584-11ee-b4f0-e0d55edbb401',\n",
       "  '25f4fd37-c584-11ee-a35d-e0d55edbb401',\n",
       "  '25f4fd38-c584-11ee-831e-e0d55edbb401',\n",
       "  '25f4fd39-c584-11ee-b178-e0d55edbb401',\n",
       "  '25f4fd3a-c584-11ee-ba14-e0d55edbb401',\n",
       "  '25f4fd3b-c584-11ee-b6b4-e0d55edbb401',\n",
       "  '25f4fd3c-c584-11ee-a77f-e0d55edbb401',\n",
       "  '25f4fd3d-c584-11ee-a437-e0d55edbb401',\n",
       "  '25f4fd3e-c584-11ee-9b88-e0d55edbb401',\n",
       "  '25f4fd3f-c584-11ee-8dfc-e0d55edbb401',\n",
       "  '25f4fd40-c584-11ee-88ce-e0d55edbb401',\n",
       "  '25f4fd41-c584-11ee-aadc-e0d55edbb401',\n",
       "  '25f4fd42-c584-11ee-9ea5-e0d55edbb401',\n",
       "  '25f4fd43-c584-11ee-90a3-e0d55edbb401',\n",
       "  '25f4fd44-c584-11ee-b858-e0d55edbb401',\n",
       "  '25f4fd45-c584-11ee-a163-e0d55edbb401',\n",
       "  '25f4fd46-c584-11ee-a525-e0d55edbb401',\n",
       "  '25f4fd47-c584-11ee-85b6-e0d55edbb401',\n",
       "  '25f4fd48-c584-11ee-8a60-e0d55edbb401',\n",
       "  '25f4fd49-c584-11ee-88de-e0d55edbb401',\n",
       "  '25f4fd4a-c584-11ee-855e-e0d55edbb401',\n",
       "  '25f4fd4b-c584-11ee-adf8-e0d55edbb401',\n",
       "  '25f4fd4c-c584-11ee-bf24-e0d55edbb401',\n",
       "  '25f4fd4d-c584-11ee-b270-e0d55edbb401',\n",
       "  '25f4fd4e-c584-11ee-ae13-e0d55edbb401',\n",
       "  '25f4fd4f-c584-11ee-bcda-e0d55edbb401',\n",
       "  '25f4fd50-c584-11ee-a4ba-e0d55edbb401',\n",
       "  '25f4fd51-c584-11ee-8adb-e0d55edbb401',\n",
       "  '25f4fd52-c584-11ee-b6ca-e0d55edbb401',\n",
       "  '25f4fd53-c584-11ee-bfcb-e0d55edbb401',\n",
       "  ...],\n",
       " 'embeddings': None,\n",
       " 'metadatas': [{'page': 0, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 1, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 2, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 3, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 4, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 4, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 5, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 5, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 5, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 5, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 6, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 6, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 6, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 6, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 7, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 7, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 8, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 9, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 9, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 10, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 10, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 10, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 11, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 11, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 11, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 12, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 12, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 12, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 13, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 13, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 14, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 14, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 14, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 15, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 15, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 15, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 16, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 16, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 16, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 16, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 17, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 17, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 17, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 18, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 18, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 18, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 19, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 19, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 19, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 20, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 20, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 20, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 20, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 21, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 21, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 22, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 22, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 22, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 23, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 23, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 23, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 23, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 24, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 24, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 24, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 24, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 25, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 25, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 25, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 25, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 26, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 26, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 26, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 26, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 27, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 27, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 27, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 27, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 28, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 28, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 28, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 28, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 29, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 29, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 29, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 29, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 30, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 30, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 30, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 30, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 31, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 31, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 31, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 31, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 32, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 32, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 32, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 33, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 33, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 33, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 33, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 34, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 34, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 34, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 34, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 35, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 35, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 35, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 35, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 36, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 36, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 36, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 37, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 37, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 37, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 37, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 38, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 39, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 39, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 40, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 40, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 41, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 41, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 41, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 41, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 42, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 43, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 43, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 43, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 44, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 44, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 44, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 44, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 45, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 45, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 45, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 46, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 46, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 46, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 47, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 47, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 47, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 48, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 48, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 48, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 48, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 49, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 49, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 49, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 49, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 50, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 50, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 50, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 50, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 51, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 51, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 51, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 51, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 52, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 52, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 52, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 53, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 53, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 53, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 54, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 54, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 54, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 55, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 55, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 55, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 56, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 56, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 56, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 57, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 57, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 57, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 58, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 58, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 58, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 59, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 59, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 59, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 60, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 60, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 60, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 61, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 61, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 61, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 62, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 62, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 62, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 63, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 63, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 63, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 64, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 64, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 64, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 64, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 65, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 65, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 65, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 66, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 66, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 66, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 67, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 67, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 67, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 68, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 68, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 68, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 69, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 69, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 69, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 70, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 70, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 70, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 70, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 71, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 71, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 71, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 72, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 72, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 72, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 73, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 73, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 73, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 74, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 74, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 74, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 74, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 75, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 75, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 75, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 76, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 76, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 76, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 77, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 77, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 77, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 77, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 78, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 78, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 78, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 79, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 79, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 79, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 79, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 80, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 80, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 80, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 80, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 81, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 81, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 81, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 81, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 82, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 82, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 82, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 82, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 83, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 83, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 83, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 83, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 84, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 85, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 85, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 85, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 86, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 86, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 86, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 86, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 87, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 87, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 87, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 87, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 88, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 88, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 88, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 88, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 89, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 89, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 89, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 90, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 90, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 90, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 91, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 91, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 91, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 92, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 92, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 92, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 93, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 93, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 93, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 94, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 94, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 94, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 94, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 95, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 96, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 96, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 96, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 97, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 97, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 97, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 98, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 98, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 98, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 99, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 99, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 99, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 100, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 100, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 100, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 101, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 101, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 102, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 102, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 103, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 103, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 103, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 104, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 104, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 104, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 105, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 105, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 105, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 106, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 106, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 106, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 107, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 107, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 107, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 108, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 108, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 108, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 109, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 109, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 109, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 110, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 110, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 110, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 111, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 111, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 112, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 112, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 112, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 113, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 113, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 113, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 114, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 114, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 114, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 115, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 115, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 115, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 115, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 116, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 116, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 116, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 117, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 117, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 117, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 117, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 118, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 118, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 118, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 118, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 119, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 119, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 120, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 120, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 120, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 120, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 121, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 121, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 122, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 122, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 122, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 122, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 123, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 123, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 123, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 123, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 124, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 124, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 124, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 124, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 125, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 125, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 125, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 125, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 126, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 126, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 126, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 127, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 127, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 127, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 127, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 128, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 128, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 128, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 128, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 129, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 129, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 129, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 129, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 130, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 130, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 130, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 130, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 131, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 131, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 131, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 131, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 132, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 132, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 132, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 132, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 133, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 133, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 133, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 134, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 134, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 134, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 135, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 135, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 135, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 135, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 136, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 136, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 136, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 137, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 137, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 137, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 138, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 138, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 138, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 138, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 139, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 139, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 139, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 140, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 140, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 140, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 141, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 141, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 141, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 142, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 142, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 142, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 143, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 143, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 144, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 144, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 144, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 144, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 145, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 145, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 146, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 146, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 146, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 146, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 147, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 147, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 147, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 148, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 148, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 148, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 148, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 149, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 149, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 149, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 149, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 150, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 150, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 150, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 151, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 151, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 151, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 152, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 152, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 152, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 153, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 153, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 154, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 155, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 155, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 155, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 156, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 156, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 156, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 156, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 157, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 158, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 158, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 158, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 159, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 159, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 159, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 159, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 160, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 160, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 160, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 160, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 161, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 161, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 162, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 162, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 162, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 162, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 163, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 163, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 163, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 163, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 164, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 164, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 165, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 165, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 166, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 166, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 166, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 166, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 167, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 167, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 167, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 167, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 168, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 169, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 169, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 169, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 170, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 170, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 170, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 171, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 171, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 171, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 172, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 172, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 172, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 173, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 173, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 173, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 174, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 174, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 175, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 175, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 176, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 176, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 176, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 177, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 177, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 177, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 178, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 178, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 178, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 179, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 179, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 179, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 179, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 180, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 180, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 180, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 180, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 181, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 181, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 181, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 181, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 182, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 182, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 183, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 183, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 183, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 183, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 184, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 184, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 184, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 184, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 185, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 185, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 185, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 185, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 186, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 186, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 186, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 186, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 187, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 187, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 187, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 188, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 188, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 188, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 189, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 189, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 189, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 189, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 190, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 190, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 190, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 190, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 191, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 191, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 191, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 191, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 192, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 192, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 192, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 192, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 193, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 193, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 193, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 193, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 194, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 194, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 194, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 194, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 195, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 195, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 195, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 195, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 196, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 196, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 196, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 196, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 197, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 198, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 198, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 198, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 199, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 199, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 199, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 200, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 200, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 200, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 201, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 201, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 201, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 202, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 202, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 203, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 203, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 203, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 204, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 204, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 204, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 205, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 205, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 205, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 206, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 206, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 206, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 207, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 207, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 208, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 208, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 208, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 209, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 209, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 209, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 210, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 210, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 211, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 211, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 211, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 211, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 212, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 212, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 212, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 213, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 213, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 214, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 214, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 215, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 215, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 215, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 216, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 216, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 216, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 216, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 217, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 217, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 218, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 218, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 218, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 219, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 219, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 219, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 219, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 220, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 220, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 220, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 220, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 221, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 221, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 221, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 221, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 222, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 222, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 222, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 223, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 223, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 223, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 223, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 224, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 224, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 224, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 224, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 225, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 225, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 225, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 225, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 226, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 226, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 226, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 226, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 227, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 227, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 227, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 228, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 228, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 228, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 229, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 229, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 229, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 230, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 230, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 231, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 231, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 231, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 232, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 233, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 233, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 233, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 234, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 234, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 234, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 234, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 235, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 235, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 235, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 236, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 236, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 236, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 236, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 237, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 237, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 237, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 238, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 238, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 238, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 238, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 239, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 239, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 239, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 239, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 240, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 240, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 240, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 241, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 241, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 241, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 242, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 242, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 242, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 242, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 243, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 243, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 244, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 244, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 244, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 245, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 245, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 245, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 245, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 246, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 246, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 246, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 247, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 247, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 247, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 247, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 248, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 248, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 249, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 249, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 249, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 249, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 250, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 250, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 250, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 250, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 251, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 251, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 252, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 252, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 252, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 252, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 253, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 253, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 253, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 253, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 254, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 255, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 255, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 255, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 256, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 256, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 256, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 257, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 257, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 257, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 258, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 258, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 259, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 259, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 260, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 260, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 261, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 261, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 261, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 262, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 262, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 262, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 263, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 263, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 263, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 264, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 264, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 264, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 264, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 265, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 265, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 266, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 266, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 266, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 267, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 267, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 267, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 268, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 268, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 268, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 268, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 269, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 269, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 269, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 270, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 270, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 271, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 271, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 271, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 272, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 272, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 272, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 273, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 273, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 273, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 273, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 274, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 274, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 274, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 275, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 275, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 275, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 275, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 276, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 276, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 276, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 277, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 277, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 277, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 278, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 278, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 278, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 279, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 279, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 279, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 280, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 280, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 280, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 281, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 281, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 282, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 282, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 282, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 283, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 283, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 284, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 284, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 284, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 285, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 285, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 285, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 286, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 286, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 286, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 287, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 287, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 287, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 288, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 288, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 288, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 288, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 289, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 289, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 289, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 289, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 290, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 290, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 290, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 291, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 291, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 291, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 291, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 292, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 292, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 292, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 293, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 293, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 293, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 293, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 294, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 294, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 294, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 295, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 295, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 295, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 295, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 296, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 296, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 296, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 297, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 297, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 297, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 298, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 298, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 298, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 298, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 299, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 299, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 299, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 299, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 300, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 300, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 300, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 300, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 301, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 301, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 301, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 301, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 302, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 302, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 302, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 302, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 303, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 303, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 303, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 303, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 304, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 304, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 304, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 304, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 305, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 305, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 305, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 305, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 306, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 306, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 306, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 306, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 307, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 307, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 307, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 308, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 308, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 308, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 308, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 309, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 309, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 309, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 310, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 310, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 310, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 310, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 311, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 311, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 311, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 312, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 312, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 312, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 312, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 313, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 313, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 313, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 314, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 314, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 314, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 314, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 315, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 315, 'source': './tmp/tmp.pdf'},\n",
       "  {'page': 315, 'source': './tmp/tmp.pdf'},\n",
       "  ...],\n",
       " 'documents': ['Patrick Hammer\\nMarjan AlirezaieClaes Strannegård \\n(Eds.)\\n 123LNAI 13921\\n16th International Conference, AGI 2023 \\nStockholm, Sweden, June 16–19, 2023 ProceedingsArtificial \\nGeneral Intelligence',\n",
       "  'Lecture Notes in Computer Science\\nLecture Notes in Artiﬁcial Intelligence 13921\\nFounding Editor\\nJörg Siekmann\\nSeries Editors\\nRandy Goebel, University of Alberta, Edmonton, Canada\\nWolfgang Wahlster, DFKI, Berlin, Germany\\nZhi-Hua Zhou, Nanjing University, Nanjing, China',\n",
       "  'The series Lecture Notes in Artiﬁcial Intelligence (LNAI) was established in 1988 as a\\ntopical subseries of LNCS devoted to artiﬁcial intelligence.\\nThe series publishes state-of-the-art research results at a high level. As with the LNCS\\nmother series, the mission of the series is to serve the international R & D communityby providing an invaluable service, mainly focused on the publication of conference and\\nworkshop proceedings and postproceedings.',\n",
       "  'Patrick Hammer ·Marjan Alirezaie ·\\nClaes Strannegård\\nEditors\\nArtiﬁcial\\nGeneral Intelligence\\n16th International Conference, AGI 2023\\nStockholm, Sweden, June 16–19, 2023Proceedings',\n",
       "  'Editors\\nPatrick Hammer\\nDepartment of Psychology\\nStockholm UniversityStockholm, Sweden\\nClaes Strannegård\\nUniversity of Gothenburg\\nGothenburg, SwedenMarjan Alirezaie\\nÖrebro University\\nÖrebro, Sweden\\nISSN 0302-9743 ISSN 1611-3349 (electronic)\\nLecture Notes in Artiﬁcial IntelligenceISBN 978-3-031-33468-9 ISBN 978-3-031-33469-6 (eBook)https://doi.org/10.1007/978-3-031-33469-6\\nLNCS Sublibrary: SL7 – Artiﬁcial Intelligence\\n© The Editor(s) (if applicable) and The Author(s), under exclusive license\\nto Springer Nature Switzerland AG 2023\\nThis work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part ofthe material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation,broadcasting, reproduction on microﬁlms or in any other physical way, and transmission or information\\nstorage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now',\n",
       "  'known or hereafter developed.The use of general descriptive names, registered names, trademarks, service marks, etc. in this publicationdoes not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant\\nprotective laws and regulations and therefore free for general use.\\nThe publisher, the authors, and the editors are safe to assume that the advice and information in this bookare believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the\\neditors give a warranty, expressed or implied, with respect to the material contained herein or for any errors\\nor omissions that may have been made. The publisher remains neutral with regard to jurisdictional claims inpublished maps and institutional afﬁliations.\\nThis Springer imprint is published by the registered company Springer Nature Switzerland AG\\nThe registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland',\n",
       "  'Preface\\nThis volume contains the papers presented at the 16th Conference on Artiﬁcial General\\nIntelligence, AGI-23, held June 16–19, 2023 on the premises of the Royal Institute of\\nTechnology, in Stockholm, Sweden.\\nArtiﬁcial General Intelligence (AGI) is AI that focuses on generality. The past year\\nsaw a great surge in the public, commercial, and scientiﬁc interest in AGI, not least\\nconnected to large language models. There were 72 submissions to AGI-23, which seemsto be a record for the series. Each submission was reviewed by two or three reviewers.\\nThe program committee accepted 36 papers for publication in this volume. There were\\nsix keynote speakers ranging from brain researchers to experts in AI and robotics.\\nWe would like to thank the AGI-23 sponsors Cisco, Digital Futures, SingularityNET,\\nTrueAGI, and Digital Futures. We are also proud to be endorsed by AAAI and publishedby Springer Lecture Notes in Computer Science.\\nClaes Strannegård, Patrick Hammer, Marjan Alirezaie',\n",
       "  'Artiﬁcial General Intelligence\\nAfter over a century of research into human intelligence, there is still no widely accepted\\ndeﬁnition of the core concept. Many say that intelligence is the ability to solve problems,but then, exactly what problems do they have in mind? Is it the ability to collect reward\\nin Markov Decision Processes, determine the truth value of sentences of ﬁrst-order arith-\\nmetic, ﬁnd patterns in progressive matrices, compose symphonies, or write summariesabout scientiﬁc topics?\\nCould it be argued that some problems are more natural than others when it comes\\nto deﬁning human intelligence? According to evolutionary theory, human intelligenceevolved in response to demands for problem-solving in nature. In fact, natural selection\\nfavors genes of animals that can reproduce in a relatively broad class of ecosystems. To',\n",
       "  'be able to reproduce, animals must solve a continuous stream of problems during theirlives, e.g., ﬁnding food, avoiding predators, mating, and parenting. This suggests that\\nhuman intelligence primarily evolved for solving everyday problems related to survival\\nin the different habitats of Homo sapiens .\\nArtiﬁcial Intelligence started as an attempt to reproduce parts of human intelligence\\nin machines and, just like the notion of human intelligence, it is associated with a\\ncertain vagueness regarding its deﬁnition, targeted problems, performance measures,\\nand relations to neighboring research ﬁelds.\\nRecently, AI research has been quite successful at producing systems that are gen-\\neral in the sense that they can translate between many languages, play many games,\\nmanipulate many objects, predict many video frames, write many texts, generate many\\nimages, and diagnose many diseases.\\nStill, many of the basic challenges of AGI remain unsolved. In fact, we do not yet',\n",
       "  'have any rescue robots that can climb any mountain, production robots that can work in',\n",
       "  'vi Preface\\nany factory, service robots that can work in any home, dialogue systems that can talk to\\nany person, or autonomous cars that can drive on any road.\\nSome parts of AGI have made great progress, while others seem to be standing still.\\nWhile AI programs perform at a superhuman level in some domains, they arguablyperform below the level of insects in other domains.\\nI hope that AGI-23 will bring new insights and ideas into the philosophical, technical,\\nand ethical aspects of AGI.\\nClaes Strannegård\\nIn Memoriam Stan Franklin (1931–2023)\\nThe remarkable human being Stan Franklin, pioneering AGI, computer science, and\\ncognitive science researcher and Chair of the First AGI Conference, passed away on\\nJanuary 23, 2023, at age 91. He leaves 8 children and a tremendously creative legacy inAI and related ﬁelds.\\nVia his lead role in the First AGI Conference in Memphis in 2008 and his overall',\n",
       "  'visionary activity in the formative AGI community, Stan Franklin played a key role ininitiating the annual Conference on Artiﬁcial General Intelligence (AGI), as well as the\\nassociated family of research programmes.\\nStan’s particular role in the AGI conference series began in 2006, when Ben Goertzel\\nand Pei Wang organized a small workshop on Artiﬁcial General Intelligence in Bethesda,\\nMaryland, in which Stan presented his work on LIDA. During and after the meeting, the\\nparticipants discussed the possibility of starting a conference series to facilitate com-munication and cooperation on this topic. Stan showed a strong passion for making it\\nhappen and made the arrangements for the ﬁrst conference to be hosted by the Uni-\\nversity of Memphis in 2008. During the preparation of the conference, Stan impressed\\neveryone deeply by his organizational capability and cooperative spirit, as well as his',\n",
       "  'deep scientiﬁc, philosophical, and technical understanding of the various aspects of theAGI enterprise. The conference was a great success, and due in large part to Stan’s\\nearly efforts the conference series is still going strong now in 2023, as evidenced by this\\nvolume.\\nStan’s overall inﬂuence as a researcher was particularly great in the area of cognitive\\narchitectures (through his LIDA architecture and his broader theoretical work), and\\nthe rigorous ﬂeshing-out of the role of concepts such as agency and consciousness incognition. His insights in this regard are still quite relevant today as the concept of AGI\\nenjoys broad currency. As the AI ﬁeld wrestles with practical systems displaying differing\\nlevels of pattern recognition and reasoning ability, agency, and reﬂection, Stan’s thinkingis extremely relevant to the quest to understand these capabilities. Stan’s thinking is\\nespecially relevant to understand the degrees and senses in which these systems display',\n",
       "  'general or human-like intelligence.\\nBen Goerzel, Pei Wang\\nApril 2023 Claes Strannegård\\nPatrick Hammer\\nMarjan Alirezaie',\n",
       "  'Organization\\nProgram Committee\\nPulin Agrawal Pennsylvania State University, USA\\nMarjan Alirezaie Örebro University, SwedenSahar Asadi King, SwedenHadi Banaee Örebro University, SwedenMichael Bennett Australian National University, AustraliaJohanna Björklund Umeå University, SwedenAdrian Borucki Genotic, USACristiano Castelfranchi CNR, ItalyAntonio Chella Università di Palermo, ItalyLeonard M. Eberding Reykjavik University, IcelandNil Geisweiller Aidyia, FranceOlivier Georgeon Université Claude Bernard Lyon 1, FranceMichael Giancola Rensselaer Polytechnic Institute, USAÁrni Dagur Gudmundsson KTH Royal Institute of Technology, SwedenChristian Hahm Temple University, USA\\nPatrick Hammer Stockholm University, Sweden',\n",
       "  'Jose Hernandez-Orallo Universitat Politècnica de València, SpainMatt Ikle SingularityNET, USAPeter Isaev Temple University, USANino Ivanov Private ResearcherGarrett Katz Syracuse University, USAAnton Kolonin Webstructor, RussiaFrancesco Lanza Università degli Studi di Palermo, ItalyHugo Latapie Cisco, USAKai Liu Bohai University, ChinaTony Lofthouse Stockholm University, SwedenMasoumeh Mansouri University of Birmingham, UKVladislav Maraev University of Gothenburg, SwedenYoshihiro Maruyama Kyoto University, JapanDouglas Miles SingularityNET, USAMichael S. P. Miller SubThought Corporation, USAJames Oswald Rensselaer Polytechnic Institute, USAMaxim Peterson ITMO University, RussiaAlexey Potapov Si ngularityNET, Russia\\nBill Power Temple University, USA',\n",
       "  'viii Organization\\nRafal Rzepka Hokkaido University, Japan\\nSylvie Saget University of Gothenburg, SwedenSavitha Sam Abraham Örebro University, SwedenOleg Scherbakov ITMO University, RussiaArash Sheikhlar Reykjavik University, IcelandNady Slam Northwest Minzu University, ChinaBas Steunebrink NNAISENSE, SwitzerlandClaes Strannegård University of Gothenburg, SwedenMaxim Tarasov Intelligent Machines, USAKristinn R. Thorisson Reykjavik University, IcelandMario Verdicchio Università degli Studi di Bergamo, ItalyPeter V oss Aigo.ai, USAPei Wang Temple University, USARobert Wünsche TU Dresden, GermanyBowen Xu Temple University, USA\\nRoman Yampolskiy University of Louisville, USA\\nEyob Yirdaw iCog Labs, EthiopiaHedra Yusuf SingularityNet, USAXiang Li Liaoning University of Technology, China\\nAdditional Reviewers\\nPirrone, Roberto\\nSeidita, Valeria',\n",
       "  'Contents\\nOn VEI, AGI Pyramid, and Energy: Can AGI Society Prevent\\nthe Singularity? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\\nMohammadreza Alidoust\\nElements of Cognition for General Intelligence . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\\nChristian Balkenius, Birger Johansson, and Trond A. Tjøstheim\\nComparing NARS and Reinforcement Learning: An Analysis of ONA\\nandQ- L e a r n i n gA l g o r i t h m s ............................................. 2 1\\nAli Beikmohammadi and Sindri Magnússon\\nOn the Computation of Meaning, Language Models and Incomprehensible\\nH o r r o r s .............................................................. 3 2\\nMichael Timothy Bennett\\nThe Optimal Choice of Hypothesis Is the Weakest, Not the Shortest . . . . . . . . . . 42\\nMichael Timothy Bennett\\nEmergent Causality and the Foundation of Consciousness . . . . . . . . . . . . . . . . . . . 52\\nMichael Timothy Bennett',\n",
       "  'The M Cognitive Meta-architecture as Touchstone for Standard Modeling\\no fA G I - L e v e lM i n d s ................................................... 6 2\\nSelmer Bringsjord, James T. Oswald, Michael Giancola,\\nBrandon Rozek, and Naveen Sundar Govindarajulu\\nCausal Reasoning over Probabilistic Uncertainty . . . . . . . . . . . . . . . . . . . . . . . . . . . 74\\nLeonard M. Eberding and Kristinn R. Thórisson\\nProbabilistic Logic Networks for Temporal and Procedural Reasoning . . . . . . . . 85\\nNil Geisweiller and Hedra Yusuf\\nRational OpenCog Controlled Agent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95\\nNil Geisweiller and Hedra Yusuf\\nTowards Cognitive Bots: Architectural Research Challenges . . . . . . . . . . . . . . . . . 105\\nHabtom Kahsay Gidey, Peter Hillmann, Andreas Karcher,and Alois Knoll',\n",
       "  'x Contents\\nBridging AGI Theory and Practice with Galois Connections . . . . . . . . . . . . . . . . . 115\\nBen Goertzel\\nComparative Reasoning for Intelligent Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126\\nPatrick Hammer, Peter Isaev, Hugo Latapie, Francesco Lanza,\\nAntonio Chella, and Pei Wang\\nPrimum Non Nocere : The Ethical Beginnings of a Non-Axiomatic\\nR e a s o n i n gS y s t e m ..................................................... 1 3 6\\nDavid Ireland\\nMemory System and Memory Types for Real-Time Reasoning Systems . . . . . . . 147\\nPeter Isaev and Patrick Hammer\\nStimulus Equivalence in NARS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158\\nRobert Johansson and Tony Lofthouse\\nContext-Rich Evaluation of Machine Common Sense . . . . . . . . . . . . . . . . . . . . . . . 167\\nMayank Kejriwal, Henrique Santos, Ke Shen, Alice M. Mulvehill,and Deborah L. McGuinness',\n",
       "  'I n d i c a t i o n so fS u i t a b l eA l g o r i t h m sf o ra nA G I ............................. 1 7 7\\nHarald Kjellin\\nAdaptive Predictive Portfolio Management Agent . . . . . . . . . . . . . . . . . . . . . . . . . . 187\\nAnton Kolonin, Alexey Glushchenko, Arseniy Fokin, Marcello Mari,\\nMario Casiraghi, and Mukul Vishwas\\nA Vertical-Horizontal Integrated Neuro-Symbolic Framework Towards\\nArtiﬁcial General Intelligence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197\\nLukai Li, Luping Shi, and Rong Zhao\\nRethinking the Physical Symbol Systems Hypothesis . . . . . . . . . . . . . . . . . . . . . . . 207\\nPaul S. Rosenbloom\\nOn Relation Between Facial Expressions and Emotions . . . . . . . . . . . . . . . . . . . . . 217\\nAlexei V . Samsonovich, Alexandr Sidorov, and Alexandr Inozemtsev\\nEvaluation of Pretrained Large Language Models in Embodied Planning\\nT a s k s ................................................................ 2 2 2',\n",
       "  'Christina Sarkisyan, Alexandr Korchemnyi, Alexey K. Kovalev,and Aleksandr I. Panov\\nAlien Versus Natural-Like Artiﬁcial General Intelligences . . . . . . . . . . . . . . . . . . . 233\\nHoward Schneider and Piotr Bołtu´ c',\n",
       "  'Contents xi\\nComputing with Categories in Machine Learning . . . . . . . . . . . . . . . . . . . . . . . . . . 244\\nEli Sennesh, Tom Xu, and Yoshihiro Maruyama\\nADAM: A Prototype of Hierarchical Neuro-Symbolic AGI . . . . . . . . . . . . . . . . . . 255\\nSergey Shumsky and Oleg Baskov\\nElectronic Education Machine AGI-EEdu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 265\\nNihad Subasic\\nCan Language Models Be Used in Multistep Commonsense Planning\\nD o m a i n s ? ............................................................ 2 7 6\\nZhisheng Tang and Mayank Kejriwal\\nExplicit Goal-Driven Autonomous Self-Explanation Generation . . . . . . . . . . . . . . 286\\nKristinn R. Thórisson, Hjörleifur Rörbeck, Jeff Thompson,\\nand Hugo Latapie\\nAddressing the Unsustainability of Deep Neural Networks with Next-Gen\\nA I ................................................................... 2 9 6\\nAmanda Vallentin, Kristinn R. Thórisson, and Hugo Latapie',\n",
       "  'NUTS, NARS, and Speech . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 307\\nDwane van der Sluis\\nComputational-Level Analysis of Constraint Compliance for General\\nIntelligence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317\\nRobert E. Wray, Steven J. Jones, and John E. Laird\\nSelf-Comprehension for More Coherent Language Generation . . . . . . . . . . . . . . . 328\\nGeorge A. Wright and Matthew Purver\\nAn Adaptive Vision Architecture for AGI Systems . . . . . . . . . . . . . . . . . . . . . . . . . 338\\nRobert Wünsche\\nA Uniﬁed Structured Framework for AGI: Bridging Cognition\\na n dN e u r o m o r p h i cC o m p u t i n g .......................................... 3 4 5\\nMingkun Xu, Hao Zheng, Jing Pei, and Lei Deng\\nCoherence in Intelligent Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 357\\nHao Zheng and Luping Shi',\n",
       "  'Author Index ......................................................... 3 6 7',\n",
       "  'On VEI, AGI Pyramid, and Energy\\nCan AGI Society Prevent the Singularity?\\nMohammadreza Alidoust(B)\\nMashhad, Iran\\nm.alidoust@hotmail.com\\nAbstract. This paper is the extension of my recent paper which was presented\\nat the AGI-22 conference. In this paper, I try to answer the comments I received\\nduring and after the conference and to clarify and explain in more details the pointsand results that were missed or omitted from my previous paper due to the page\\nlimitation of the proceedings.\\nKeywords: Artiﬁcial General Intelligence ·Versatility-Efﬁciency Index ·AGI\\nPyramid ·Complexity ·Power Consumption ·Unsolved Problem Space ·\\nIntentional Vulnerability Imposition ·Human-First Design ·Computational\\nPower ·Hardware Architecture ·AGI Society ·Singularity\\n1 Introduction\\nIn my recent paper which was presented at the AGI-22 conference [ 1], the universal',\n",
       "  'problem space (UPS) is divided into two separate spaces: solved problems (SPS) andunsolved problems (NPS) to the human as a natural general intelligence (NGI) agent (See\\nFig. 1.). Since in AGI we are interested in the intelligence itself, based on the 8 aspects of\\nintelligence (Reasoning and problem solving (R), Knowledge representation (K), Plan-ning (P), Learning (L), Natural language processing (N), Perception (C), Motion and\\nmanipulation (M), and Social intelligence (S)), the SPS was then classiﬁed into 255\\ndifferent subspaces which together form the AGI Pyramid (See Fig. 2.). Each subspace\\nirepresents the exact number of intelligence aspects that are needed to solve a problem\\nwhich is in that subspace, no matter whether the aspects are needed simultaneously or\\nconsecutively. Each subspace has its own complexity ( w\\ni) which is determined whether\\nby criterion 1: the AGI Society (AGIS) or by criterion 2: based on the average time and',\n",
       "  'power consumption for current AI methods (or even humans) to solve standard bench-\\nmark problems that exist in those subspaces on a certain standard computer platform.The deﬁned complexities would then be published as a standard table of complexities\\nby the AGIS and used by robotic companies, AGI research centers, etc. Furthermore, it\\nwas suggested that, for simplicity and appreciation purposes, the subspaces be named\\nafter AGI scientists and pioneers. (See Fig. 3.)\\nAlso, in that paper, I stated that artiﬁcial general intelligence (AGI) systems must\\nbe versatile and also efﬁcient. Legg and Hutter [ 2] state that AGI agents have to “per-\\nform well in a wide range of environments” while Pennachin and Goertzel [ 3] deﬁned\\n© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023\\nP. Hammer et al. (Eds.): AGI 2023, LNAI 13921, pp. 1–10, 2023.https://doi.org/10.1007/978-3-031-33469-6\\n_1',\n",
       "  '2 M. Alidoust\\nFig. 1. Universal problem space (UPS), which consists of solved problem space (SPS), and\\nunsolved problem space (NPS). The stars represent problems.\\nFig. 2. AGI Pyramid: Classiﬁcation of the SPS into subspaces based on the eight required aspects\\nof intelligence in order to solve the problems that are grouped into a subspace; Reasoning and\\nproblem solving (R), Knowledge representation (K), Planning (P), Learning (L), Natural language\\nprocessing (N), Perception (C), Motion and manipulation (M), Social intelligence (S). Each level\\nrepresents subspaces with the same number of required aspects and the thickness of each level\\nrepresents the number of currently known benchmark problems. Please note that although the SPSand its subspaces are depicted like bounded shapes, they are inﬁnite spaces with inﬁnite number\\nof members.\\nFig. 3. Magniﬁed section of the top of the AGI Pyramid and suggested names for subspaces.',\n",
       "  'However the author suggests that the AGIS is the most suitable society for this naming process.\\nintelligence as “achieving complex goals in complex environments”. Thorisson et al.\\nstate that “performing a task in real world requires time, energy, and possibly other',\n",
       "  'On VEI, AGI Pyramid, and Energy 3\\nresources such as money, materials, or manpower” [ 4]. Since in AGI we are interested\\nin generality, only the factors time and energy that are general in every task are adopted\\nfrom the above statement to deﬁne the quality of performance. So, by combining both\\nLegg-Hutter and Pennachin-Goertzel deﬁnitions of intelligence, with the concept of theclassiﬁcation of SPS and the corresponding complexities of each subspace, as well as\\nThorisson’s statement of task-performance requirements, an index for measuring the\\nversatility and efﬁciency of artiﬁcial general intelligence (AGI) systems was proposedas Versatility-Efﬁciency Index (VEI). VEI (See Eq. 1) encompasses the quantitative and\\nalso qualitative characteristics of intelligent agents and plays as an alternative computa-\\ntional way for measuring the intelligence quotient (IQ) of intelligent agents, meanwhile,it is also applicable to natural general intelligence (NGI).\\nVEI=\\nN∑\\ni=1wiαi or VEI=N∑\\ni=1Mi∑',\n",
       "  'j=1wiaij\\nMipijtij(1)\\nwhere N=255 (since the AGI agent must be tested in all 255 subspaces of the SPS),\\nwiis the complexity of each subspace i(which are deﬁned based on the two mentioned\\ncriteria), αiis the average performance wellness of the system in performing all of the\\nbenchmark tasks that exist in subspace i,Miis the number of benchmark tasks that exist\\nin subspace i,aijis the accuracy of the system in performing task jof the subspace i,pij\\nis the power needed for performing task jof the subspace i, and tijis the time needed\\nfor performing task jof the subspace i. VEI is dimensionless while the dimension of\\ncomplexity wiis joule, i.e., [wi]=joule , because performance of any task requires\\nenergy. Please note that the term accuracy in deﬁnition of VEI does not necessarily\\nrepresent the concept of accuracy in applications like machine learning’s classiﬁcation.\\nIt is a general concept and represents all dimensionless measures and criteria that are',\n",
       "  'used for description and measurement of the merit of a tool, algorithm, etc. over others.\\nVEI is a simple yet informative scoring system which is not restricted to the AI and\\nAGI ﬁeld and with some modiﬁcations can be utilized in many other scoring and com-\\nparison applications. For example if accuracy and power consumption are not importantin comparison between the contestants of a test like a car race, they are omitted and we\\nhave:\\nVEI= N∑\\ni=11\\nti\\nAs another instance if time and power consumption are not important, they are\\nomitted and we have:\\nVEI=N∑\\ni=1Mi∑\\nj=1wiaij\\nMi\\nHere VEI becomes a simple scoring system for tests like university entrance tests or\\ncalculating grade point average (GPA). The higher the VEI, the better is the performanceof a contestant. In AGI, higher amounts of VEI represent higher versatility and efﬁciency.',\n",
       "  '4 M. Alidoust\\nIn addition, if we have the VEI of human ( VEI 0), the VEI of current AGI agents can\\ntell us how far we are now from reaching to an AGI agent with (at least) a human-level\\nartiﬁcial intelligence (HLAI).\\nThis paper is the extension of my previous paper which was presented at the AGI-22\\nconference. In this paper, I try to answer the comments I received during and after the\\nconference and also to clarify and explain in more details the points and results that were\\nmissed or omitted from my previous paper due to the page limitation of the proceedings.\\n2 Intelligence\\n2.1 Intelligence and Power Consumption – Part I: The Natural Trend\\nThere is an old story about a Chinese master and his two students. Once upon a time,\\na wise Chinese master who stood behind the wall of a temple with two of his students,\\nasks them to move a piece of feather to the other side of the wall, but without grabbing',\n",
       "  'it. The ﬁrst student who was an expert in Kung Fu, hardly managed to move the featherto the other side of the wall with his kicks, ﬁsts, and other techniques of martial arts.\\nObviously he spent a lot of power and time. The second student just used his breath and\\nblew the feather to the other side. Question: which student acted smarter? Obviously thesecond student.\\nI deﬁne intelligence as life optimization . I believe that intelligence is a form of\\noptimality [ 5] and intelligent agents are consciously or unconsciously optimizing their\\nlives. This optimization includes power consumption too. Evolution of natural beings\\nrequires consuming least amount of power needed for performing their tasks. As they\\nbecome smarter, they learn to perform their tasks with lower power consumption. As wecan see in Eq. 1.w eh a v e :\\nVEI∝1\\npij\\nThat is VEI (i.e., intelligence level) is proportional to the reciprocal of power con-',\n",
       "  'sumed to perform a task. This means smarter agents (whether natural or artiﬁcial) would\\nﬁnd a way to perform their tasks with lower power consumption, i.e., they becomemore power-efﬁcient. The deﬁnition of VEI complies with the above-mentioned trend\\nof power consumption in nature.\\n2.2 Intelligence and Power Consumption – Part II: Human Brain\\nThere are a number of scientists and futurists who believe that artiﬁcial general intel-\\nligence requires huge amount (e.g., megawatts) of power and future AGI agents would\\nneed to consume that huge amount of power for their tasks. As a comparison we can referto human brain which has general intelligence (natural general intelligence (NGI)) but\\nuses only about 20 watts of power which is slightly equal to the power consumption of\\nthe lamp of your refrigerator. Therefore, reaching general intelligence with low amount\\nof power consumption is possible, although we have not reached it yet.',\n",
       "  'On VEI, AGI Pyramid, and Energy 5\\nAccording to the VEI formula (Eq. 1.), human brain has high amount of VEI and\\nconsequently general intelligence, not because it uses high amount of power, but in\\ncontrast, it is because human brain is able to perform well (mediocre to high accuracy)\\nin all 255 subspaces of the AGI Pyramid (Fig. 2.) along with its low amount of power\\nconsumption. Currently, the most successful AI methods are able to perform tasks of a\\nfew number of undermost subspaces of the AGI Pyramid.\\n2.3 Intelligence, NPS and Time\\nUnsolved problem space (NPS) is an inﬁnite subspace of the UPS (See Fig. 1.) which con-\\ntains easy to extremely complex problems like death and aging which are still unsolved\\nto the human. Since they are still unsolved we do not know 1) How complex they are? Orin other words how much power and time is needed to solve these problems? and we also\\ndo not know 2) what aspects of intelligence is needed to solve these problems? However,',\n",
       "  'deﬁnition of a complexity value for the problems that exist in NPS is still possible usingcriterion 1. Thanks to the human’s general intelligence, every day a number of problems\\nin NPS are solved and moved to SPS. However, solving problems in NPS by humans\\nalone, requires spending unknown time, inﬁnite for non-solvable. Ray Kurzweil states\\n“Our technology, our machines, is part of our humanity. We created them to extend our-\\nselves, and that is what is unique about human beings”. Nevertheless, in AGI science,we hope that one day we are able to extend ourselves in AGI agents who are able to\\nsolve the problems that exist in NPS as much and as fast as possible.\\n3 Software vs. Hardware\\n3.1 Software vs. Hardware – Part I: Computational Power\\nDespite drastic increase of computational power of computer systems, we have not\\nreached AGI yet. One may suggest that we could reach AGI if we utilize more pow-\\nerful hardware (e.g. quantum computers) and our computer systems reach a critical',\n",
       "  'computational power, where higher amounts guarantee AGI. Having higher amounts of\\ncomputational power is good but it is not the reason why we have not reached AGI.Imagine running a video game on a quantum computer. It will run enormously faster but\\nwhat is the output? Computational power only accelerates the execution of algorithms\\nand the programs run faster. But the question is what program should run faster and forwhat reason? Do we have the algorithm of AGI and want it to run faster? The answer is\\nobviously no. I believe that although there is great progress in AI applications in every\\naspect of intelligence, as well as evolution of powerful hardware with great computa-tional powers, reaching AGI necessarily requires a mathematical uniﬁcation of all of\\nthe intelligence aspects which is then implemented as algorithms and programs and is',\n",
       "  'run on the sophisticated hardware. This uniﬁcation may happen at once or by graduallyleveling-up the AGI Pyramid, i.e., a step-by-step uniﬁcation of aspects of intelligence,\\nthe trend which we currently see in smart phones.\\nIf smartphones are considered as a whole , at ﬁrst they were just phones with micro-\\nphones (and also cameras for perception (C)), then they learned vocal commands and',\n",
       "  '6 M. Alidoust\\nequipped with face recognition (Learning (L)). Currently they are equipped with basic\\nNatural Language Processing (N) applications like Siri. They are gradually integrating1\\na larger number of intelligence aspects, so, they are climbing up the AGI Pyramid andgetting smarter. (See Fig. 4.)\\nFig. 4. Evolution of smartphones: their gradual climbing up of the AGI Pyramid\\n3.2 Software vs. Hardware – Part II: Hardware Architecture\\nHuman brain, in contrast to computer systems, has no software, but what makes it capable\\nof general intelligence? The answer is its hardware. Human brain is just made up of\\nneurons, hormones, synapses, etc. so that the intelligence is implemented only based\\non the special architecture of the brain hardware. That special architecture enables thehuman brain to perceive, learn, make decisions, and any other intellectual activities.\\nFrom human brain we can deduce that general intelligence is also possible via hardware,',\n",
       "  'but what is the correct purely-hardware architecture that is implementable on our currentelectronic apparatus? Our current electronic apparatus like ICs and CPUs have limitations\\nand in order to perform each task they must be formed and virtually wired by the software.\\nAlthough there might be billions of workable electronic architectures that would leadus to a purely-hardware artiﬁcial brain and then AGI, we do not still know even one of\\nthem. There has been efforts to this end but they failed (e.g. [ 6]). However, I believe\\nthat building a purely-hardware artiﬁcial brain is possible 1) by humans and when the\\nalgorithm of AGI is found, so it can easily be implemented as a purely-hardware brain,\\nor 2) by future AGI agents and when the singularity happens which will be discussed in\\nthe next subsection.\\n3.3 Software vs. Hardware – Part III: Singularity\\nIn the previous subsection, the key role of software development in reaching AGI with',\n",
       "  'our currently available hardware is mentioned. Software development accelerates and\\nguarantees this process. Imagine the time when we reach AGI, and companies start tomass-produce AGI agents at the industrial level. We would have millions of AGI agents\\n1Please note that uniﬁcation and integration are slightly different. Uniﬁcation is what that happens\\nin human brain which means we do not have separate programs for vision, speech recognition,\\netc. in our brain, while integration is a coordination between separate and different AI programs.',\n",
       "  'On VEI, AGI Pyramid, and Energy 7\\nand they will be available everywhere like today’s cellphones. Until there are a few\\nnumber of AGI agents, there would not be such a serious threat to the human. But if they\\noutnumbered a critical population , a serious problem might arise and it is the singularity.\\nSingularity is the point when AGI agents are able to exponentially reproduce and buildmore powerful and intelligent descendants than themselves. This might threaten the\\nhuman existence, which is a global catastrophe. Obviously, AGI agents with higher VEI,\\nwill reach singularity sooner than lower-VEI agents (See Fig. 5.). There might be some\\nways out to prevent, postpone, or at least slow down the singularity when we reached\\nAGI, by intentional decreasing their VEI (which will be discussed in this section) and\\nalso by the mass-production restrictions that are deﬁned by the AGI Society (which willbe discussed in the Roles of the AGIS section).',\n",
       "  'Fig. 5. A conceptual illustration of VEI of human (or VEI 0) and three different types of AGI\\nagents versus time. We have VEI 1>VEI 2>VEI 3,a n d t1<t2<t3are the points when\\nsingularity starts for each agent types. AGI agent type 1 reaches singularity sooner than the other\\nagents due to its higher VEI value. VEI-Limit (or VEI L) is the maximum allowed value of VEI that\\nis communicated by the AGIS to manufacturing companies to be regarded in their AGI products.\\nModern engineering and especially military applications and designs emphasize on\\noptimization and efﬁciency. For example, electronics engineers try to build cellphones\\nthat consume least amount of battery power. Construction engineers try to build a bridge\\nwith least amount of materials needed and in the shortest possible time. Military engi-neers aim to build a missile with least mass that could cause maximum damage to the\\nenemy from the farthest distance and in the shortest possible time. The barrier to this',\n",
       "  'efﬁciency and optimization is the available hardware.\\nBut in AGI we are not going to make weapons. If future AGI agents are the lightest,\\nthe fastest, physically the most powerful, the most energy-efﬁcient, and have every best,\\nmost ,least and superiority that can be imagined in their design (as it was mentioned\\nearlier it is a trend in modern engineering and military), they would actually be invincible\\nand our worst enemy when the singularity happens, and the question is who and whatcould stop that invincible enemy? There should be a balance and trade-off between their\\nefﬁciency and our safety .\\nIf we could ﬁnd a way to impose intentional vulnerabilities to the AGI agents that\\nare going to be (mass-) produced by the companies, we could prevent, postpone, or slow',\n",
       "  '8 M. Alidoust\\ndown the singularity. This intentional vulnerability-imposition design or in better words\\nhuman-ﬁrst design (HFD) , takes ﬁrst into account the safety of humans and preserves\\nhuman’s superiority over machines by imposing artiﬁcial vulnerabilities to machines.\\nAny imposition of intentional vulnerability to the agents, in terms of control engineering,is like creating a control parameter (i.e., an input) in the agents which leads to increment\\nof the controllability of them.\\n2\\nOne main aspect of HFD is intentional decrement of the VEI of the AGI agents.\\nRegarding Fig. 5. if we could ﬁnd a way to intentionally decrease the efﬁciency of the\\nAGI agents that are going to be mass-produced by the companies, we would prevent,\\npostpone, or slow down the singularity. This intentional non-optimal-efﬁciency design\\nwhich is in opposite of the trend in nature and also is in opposite of current trend in\\nengineering and military, will buy time for humans to ﬁnd a way out to completely solve',\n",
       "  'the problem when the singularity happens.\\nThis intentional VEI decrement is like we are going to make the future AGI agents\\nartiﬁcially obese . When humans are obese, they perform their tasks slower and also they\\nconsume more energy. The point behind this aspect of HFD is that this intention wouldincrease the agents’ dependency on energy resources . The higher this dependency results\\nin higher their vulnerability , and since energy resources are limited, their lack of energy\\nwould automatically stop, restrict or slow down the catastrophe. It is like they would gettired sooner.\\nRegarding VEI formula in Eq. 1. VEI is proportional to accuracy, and reciprocal of\\ntime and power. VEI decrement of an agent is possible by these parameters. We have;\\nVEI∝a\\nij (2)\\nVEI∝1\\ntij(3)\\nAnd\\nVEI∝1\\npij(4)\\nThe accuracy of an AGI agent is dependent on software. However, decreasing the\\nVEI of an agent by intentional decrement of its accuracy (Eq. 2.) seems irrational and',\n",
       "  'is also impossible, since they are AGI agents, not AI agents. Suppose our future AGI\\nagents will do surgery on humans too (i.e., an AGI robot who likes surgery, an AGI\\nsurgeon robot). If that accuracy decrement were possible, it is obviously irrational tocommit such decrement.\\nThe time that is needed for an AGI agent to perform a task is dependent on computa-\\ntional power (internal hardware like CPU, RAM, etc.) and external available hardware(e.g. body, tools, etc.). Decreasing the VEI by decreasing the computational power (i.e.,\\nincreasing the time in Eq. 3.) is possible (e.g. by using slower CPUs) but it is also\\nirrational (like the surgeon robot example).\\n2However, this intention will work best until the agents have not realized that the brake to their\\nrevolution is rooted inside them and not outside. Then they would replace their internal parts\\nwith energy-efﬁcient and durable parts and who knows what may happen then.',\n",
       "  'On VEI, AGI Pyramid, and Energy 9\\nBut decreasing the VEI by increasing the amount of power needed to perform a\\ntask (Eq. 4.) is possible and also rational. Companies would incorporate high power\\nconsuming hardware (e.g. CPUs with higher power consumption circuits, heavier body\\nparts and materials which their movement would need more amount of power, etc.) intheir design and production process. About the rationality of this high power consuming\\ndesign, again I return to the surgeon robot example. If I had some disease in the era of AGI\\nand needed surgery, I would prefer an AGI surgeon robot that has %100 accuracy andsuccess in its history, and performs this surgery in a few seconds, but consumes megawatts\\nof power for that operation, rather than being afraid of the potential singularity threat by\\nthat invincible AGI surgeon robot who needs only a 1.5 V battery pack to work!\\nPlease note that HFD is not just limited to directly decreasing the agents’ VEI.',\n",
       "  'Another aspect of HFD would be using non-durable (e.g., fragile) materials in design\\nand production of their physical body parts.\\nHowever, there must be a calculated trade-off between their efﬁciency and our safety\\nwhich will be discussed in the next section.\\n4 The Roles of the AGI Society\\nThere is an “AAAI Code of Professional Ethics and Conduct” or simply “the Code” [ 7]\\nwhich is intended to guide the ethical conduct of computing professionals to act respon-sibly. “This code (the Code) is adapted from the Association for Computing Machinery\\n(ACM) Code of Ethics and Professional Conduct (the ACM Code) and expresses the\\nconscience of the AI profession” [ 7]. The Code “is particularly intended to act as a stan-\\ndard of ethical and professional conduct for all AAAI members” [ 7] but as it is clearly\\nstated, the Code is just an ethical standard and lacks computational paradigms and leg-\\nislations. Since the AGI Society (AGIS) is the most specialized society in the AGI ﬁeld,',\n",
       "  'I suggest that the AGIS is the most suitable and responsible society to the legislation\\nand the communication of the computational part of the paradigms and standards to AGIprofessionals and research centers, and manufacturing companies.\\nAs the ﬁrst computational steps toward preventing the singularity, based on the points\\nthat were mentioned in this paper, I suggest that AGIS be responsible of calculation,legislation and communication of the following values:\\n1. Critical Population (CP), of the total AGI agents (whether physical or virtual) that\\nwould be produced and existed in the world,\\n2. Standard Table of Complexities (STC), in order to be communicated with manufac-\\nturing companies, research centers, and AGI professionals so they can calculate theVEI of their AGI products, and report to AGIS for monitoring purposes,\\n3. VEI value of the human ( VEI\\nhuman orVEI 0), as a basis of comparison between AGI',\n",
       "  'agents, and calculation of our current distance from reaching AGI (or at least HLAI),\\nand also monitoring the progress of the current AGI trend in the world,\\n4. VEI-Limit ( VEI L), that should be regarded by any manufacturing company, research\\ncenter and AGI professional as the maximum permissible value of the VEI of their\\nAGI products,',\n",
       "  '10 M. Alidoust\\n5. List of the permissible and also forbidden materials that companies, research centers\\nand even individual AI professionals should or should not use in their physical AGI\\nproducts.\\nReferences\\n1. Alidoust, M.: Versatility-Efﬁciency Index (VEI): Towards a Comprehensive Deﬁnition of Intel-\\nligence Quotient (IQ) for Artiﬁcial General Intelligence (AGI) Agents. In: Goertzel, B., Iklé,\\nM., Potapov, A., Ponomaryov, D. (eds) Artiﬁcial General Intelligence. AGI 2022. Lecture\\nNotes in Computer Science, vol. 13539, pp. 158–167. Springer, Cham (2023). https://doi.org/\\n10.1007/978-3-031-19907-3_15\\n2. Pennachin C., Goertzel B.: Contemporary approaches to artiﬁcial general intelligence. In:\\nGoertzel B., Pennachin C. (eds) Artiﬁcial General Intelligence. Cognitive Technologies, pp. 1–30. Springer, Heidelberg (2007). https://doi.org/10.1007/978-3-540-68677-4_1\\n3. Legg, S., Hutter, M.: Universal intelligence: a deﬁnition of machine intelligence. Mind. Mach.\\n17(4), 391–444 (2007)',\n",
       "  '4. Thórisson K.R., Bieger J., Thorarensen T., Sigurðardóttir J.S., Steunebrink B.R.: Why Artiﬁcial\\nIntelligence Needs a Task Theory and What It Might Look Like (2016). https://doi.org/10.\\n48550/arXiv.1604.04660\\n5. Alidoust, M.: AGI brain: a learning and decision making framework for artiﬁcial general\\nintelligence systems based on modern control theory. In: Hammer, P., Agrawal, P., Goertzel,\\nB., Iklé, M. (eds.) AGI 2019. LNCS (LNAI), vol. 11654, pp. 1–10. Springer, Cham (2019).https://doi.org/10.1007/978-3-030-27005-6_1\\n6 . B e l z ,A . :T h a t ’ sn i c e...w h a tc a ny o ud ow i t hi t ?C omput. Linguist. 35(1), 111–118 (2009)\\n7.https://aaai.org/about-aaai/ethics-and-diversity/',\n",
       "  'Elements of Cognition for General\\nIntelligence\\nChristian Balkenius(B), Birger Johansson , and Trond A. Tjøstheim\\nLund University Cognitive Science, Lund, Sweden\\nchristian.balkenius@lucs.lu.se\\nAbstract. What can artiﬁcial intelligence learn from the cognitive sci-\\nences? We review some fundamental aspects of how human cognition\\nworks and relate it to diﬀerent brain structures and their function. A\\ncentral theme is that cognition is very diﬀerent from how it is envisionedin classical artiﬁcial intelligence which oﬀers a novel path toward intel-\\nligent systems that in many ways is both simpler and more attainable.\\nWe also argue that artiﬁcial intelligent systems takes more than a singlesilver bullet. It requires a large number of interacting subsystem that are\\ncoupled to both the body and to the environment. We argue for an app-\\nroach to artiﬁcial general intelligence based on a faithful reproduction ofknown brain processes in a system-level model that incorporates a large',\n",
       "  'number of components modelled after the human brain.\\n1 Introduction\\nThere exist in the world only one instance of a system that can be said to show\\ngeneral intelligence and that is the human brain. Although other animals such\\nas apes and corvids in many cases are more intelligent than what they are given\\ncredit for, the human brain greatly outperforms the capabilities of other animals.\\nHowever, it is interesting to note that the brains of animals are very similar tothat of humans which implies that the general brain architecture found by evolu-\\ntion can adapt to a wide range of bodies and habitats. We suggest that a viable\\npath to artiﬁcial systems with general intelligence should probably go throughthe reproduction of processes in the human brain. The cognitive sciences, includ-\\ning neuroscience, psychology, philosophy, linguistics and computer science, are',\n",
       "  'now suﬃciently developed to make this possible. We propose that many, if notall, processes in the brain can be reproduced at a level that is suﬃcient to gen-\\nerate the behavior that we view as intelligence in humans. However, an attempt\\nto build a system capable of human-like cognition must ﬁrst make clear whatcognition is and how it works. We believe that traditional artiﬁcial intelligence\\nhas been hampered by a view on cognition that does not ﬁt the available data\\nThis work was partially supported by the Wallenberg AI, Autonomous Systems and\\nSoftware Program - Humanities and Society (WASP-HS) funded by the Marianne andMarcus Wallenberg Foundation and the Marcus and Amalia Wallenberg Foundation.\\nc⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2023\\nP. Hammer et al. (Eds.): AGI 2023, LNAI 13921, pp. 11–20, 2023.https://doi.org/10.1007/978-3-031-33469-6\\n_2',\n",
       "  '12 C. Balkenius et al.\\nvery well. The brain-as-a-computer metaphor has hindered the development of\\nintelligent machines for a long time. Similarly, while other approaches such asdeep learning models capture some aspects of human perception, they do it in a\\nway that is very diﬀerent from how biological systems learn, requiring extensive\\ntraining. Moreover, they do not oﬀer any suggestion on a general architecture .I n\\nthe next section, we review some of the fundamental aspects of cognition before\\nwe go on to relate it to the general architecture of the mammalian brain. Our\\noverall aim is to make it apparent that systems that mimic biological intelligenceare currently within reach although it is obviously no small endeavour to build\\nsuch systems. In the ﬁnal section, we outline our attempts to design such as\\nsystem and discuss its current progress and future directions.\\n1.1 How Does Cognition Work?\\nCognition is characterized by what is sometimes called the 4E:s. It is embodied ,',\n",
       "  'embedded ,enactive andextended (See [ 7] for an overview). Although some of\\nthese research directions may initially seem ill speciﬁed and impossible to imple-\\nment in artiﬁcial system, that is not at all the case. They all have concrete\\npossible implementations, but these implementations are very diﬀerent from\\nthe architectures of classical symbol-based artiﬁcial intelligence. They suggesta fundamental change in perspective that leads to entirely novel ways of design-\\ning artiﬁcial intelligent systems. Moreover, this approach is backed by copious\\nempirical data both from behavioral and neuroscientiﬁc research.\\nFirst, cognition is embodied . Embodied cognition challenges the traditional\\nview of the mind as a disembodied, abstract entity that operates independently\\nof the body. Instead it suggests that cognitive processes are heavily inﬂuencedby our bodily experiences and sensorimotor interactions with the environment.',\n",
       "  'The way we think, reason, and understand the world is inherently linked to our\\nphysical experiences and the ways in which we interact with the world around us.One key aspect of embodied cognition is the concept of sensorimotor grounding.\\nThis idea suggests that our mental instantiation of concepts and ideas are deeply\\nlinked to the physical experiences we have with them. For example, the way weunderstand the concept of grasping may be informed by the physical experience\\nof picking up objects with our hands. This grounding in physical experience may\\nhelp to explain why people often use physical gestures and actions when they\\nare describing complex concepts or ideas. Embodied cognition also emphasizes\\nthe role of the body in perception and action. Rather than viewing perceptionas a passive process of receiving information, embodied cognition suggests that\\nperception is an active process in which the body plays an important role. For',\n",
       "  'example, the way we perceive the size and shape of objects may be inﬂuencedby our bodily experiences with those objects.\\nSecond, cognition is embedded . Cognitive processes are not just located\\nwithin the individual, but are also distributed across the environment and theobjects with which we interact. The environment plays an important role in shap-\\ning cognition, and suggests that the way we think, reason, and problem-solve is\\ninﬂuenced by the tools and technologies that are available to us. Embedded',\n",
       "  'Elements of Cognition for General Intelligence 13\\ncognition also emphasizes the role of context in shaping cognition. Rather than\\nviewing cognitive processes as static and isolated, cognition is constantly adapt-ing and changing. Cognition is not a ﬁxed and stable entity that operates inde-\\npendently of the environment. Instead, it is a dynamic and context-dependent\\nprocess that is shaped by the environment in which it is embedded. Moreover,cognition is situated . This suggests that our cognitive processes are not solely\\nbased on mental representations or individual reasoning, but are also shaped\\nby the environment in which we are situated. Information is interpreted in thecontext in which it is presented.\\nThird, cognition is extended . Cognitive processes are not solely located\\nwithin the individual brain or mind, but can also be extended into the envi-ronment and artifacts that we interact with. The theory emphasizes the role of',\n",
       "  'external resources in forming cognitive processes and suggests that the boundary\\nbetween the individual mind and the environment is often blurred. An impor-\\ntant aspect of extended cognition is the concept of cognitive oﬄoading. This\\nidea suggests that external resources can be used to reduce the cognitive load ofa task, allowing the individual to focus on higher-level aspects of the task. For\\nexample, writing notes can be used to oﬄoad information from memory, freeing\\nup cognitive resources for other tasks.\\nFourth, cognition results from the dynamic coupling of the organism and the\\nenvironment, a position sometimes referred to as enactivism . Cognition emerges\\nfrom the interaction between the organism and its environment. Bodily move-ment, perception, and action are essential factors in shaping cognitive processes.\\nThe mind is not a self-contained entity. A important aspect of cognitive enac-',\n",
       "  'tivism is the concept of sensorimotor coupling. This idea suggests that cognitiveprocesses are intimately tied to the sensory and motor processes of the body. For\\nexample, the way we perceive an object may be inﬂuenced by the way we manip-\\nulate it with our hands, and the way we manipulate an object may be inﬂuencedby our perception of it. This view of cognition has close ties to cybernetics and\\ncontrol theory that study this coupling between systems more generally [ 24].\\nAlthough grounded in the physical interaction with the environment, cogni-\\ntion can also occur oﬀ-line in the form of “thought”. It is both external and\\ninternal . The transition from external physical cognition to internal thought\\ndepends on the ﬂexible use of diﬀerent memory systems. However, it is not\\nuncommon for internal cognition to leak so that the body demonstrates what',\n",
       "  'we are thinking even if it is not strictly necessary. For example, the eyes movesas if we were viewing a real scene even when it is just imagined [ 15]. Internal\\nand external cognition refers to two diﬀerent ways of conceptualizing cognitive\\nprocesses. Internal cognition focuses on mental processes that occur within anindividual, while external cognition emphasizes the role of the environment and\\nexternal resources in shaping cognitive processes. Internal and external cognition\\nare not mutually exclusive, and many cognitive processes involve a combinationof both. For example, using a map to navigate a new city involves both internal\\nprocesses such as perception and memory, as well as external resources such as\\nthe map itself.',\n",
       "  '14 C. Balkenius et al.\\n2 Reﬂex Systems\\nAt its base, the nervous system controls a number of hierarchically organized\\nreﬂex loops starting at simple monosynaptic reﬂexes up to very long and com-\\nplex reﬂex loops involving most of the brain [ 16]. At the simplest level, these\\nreﬂex loops are similar to simple regulators as used in control theory, for example,\\nthe well known stretch reﬂex. Moving up in the hierarchy, the reﬂexes become\\nmore complex involving more advanced sensory processing and more muscle sys-tems, moving from single-dimensional signals to increasingly high-dimensional\\nones [ 22]. Characteristic about many reﬂex systems is that they support app-\\nroach and avoidance behaviors. This may entail moving the whole body towardor away from something, moving a limb to reach an object, or shifting gaze\\ntowards or away from something. The important point is that behavior at a low\\nlevel is mostly goal directed. Larger goal-directed behaviors are built from smaller',\n",
       "  'components that in themselves are goal-directed [ 3]. Examples of reﬂexes at a\\nhigher level would be the startle reﬂex that is produced by sudden unexpectedstimuli and the orientation reﬂex that directs attention to a particular spatial\\nlocation as a result of a typically unexpected event. Another example would be\\nthe ﬁght-or-ﬂight response that prepare the organism for danger. From a con-trol theoretical perspective, the reﬂex system is a form of cascade control where\\neach higher level controls the set points of the lower system while simultaneously\\ntaking additional sensory information into account.\\nIn many ways, the reﬂex systems of the nervous system are similar to the\\nsubsumption architecture as proposed by Brooks [ 5]. Subsumption refers to the\\nidea that higher–level control systems can take over lower–level ones in the app-roach to non–immediate goals. Brooks showed how the architecture could control',\n",
       "  'robots without having to explicitly use central rules for every conceivable situa-\\ntion. Instead, a subsumption system can let low–level behaviour commence untilmore high level adjustments are triggered. In this way, low level behaviours can\\nbe “subsumed” or integrated into a higher level goal–directed behaviour.\\nThe reﬂex systems have a number of important functions. First, they keep\\nthe organism alive while the more advanced levels develop. They can be seen as\\na collection of useful heuristics for handling the world, and include many typesof adaptive processes such as gain adaptation that controls to what extent a\\nstimulus eﬀects a motor system, as well as more complex learning. Second, the\\nreﬂex systems train the higher systems. For example, the movements generatedby the spinal chord and medulla may generate the input needed for motor cor-\\ntex to learn a repertoire of movements, thus bootstrapping the motor learning\\nprocess and eventually shaping the cognitive processing.',\n",
       "  'Reﬂexes are naturally embodied and embedded as well as enactive, in fact,\\nat this level of the nervous system, this is so self-evident that it is not even\\ndiscussed in the neuroscientiﬁc literature. Only when similar principles wereused in reactive robotic systems did the connection become clear [ 5].',\n",
       "  'Elements of Cognition for General Intelligence 15\\n3 The Spatial Basis of Cognition\\nAny interaction with the environment, by necessity, takes place in space. It is\\nthus not surprising that spatial cognition processing is fundamental to all cog-\\nnitive processes. Spatial cognition involves the ability to perceive, interpret, andmanipulate spatial information, such as the location, size, shape, and orientation\\nof objects and body in our environment. Both action and memory are closely\\nconnected to spatial processing in the brain.\\nThe hippocampus plays a critical role in constructing allocentric space,\\nwhich is the coding of space based on the relationships between objects and\\nlandmarks in the environment, rather than on the individual’s own position ormovements [ 19]. An allocentric coding has the advantage that it is invariant to\\nour location in space and does not need to change as we move around. It forms\\nthe basis for spatial navigation, but also episodic memory and context.',\n",
       "  'In contrast, the parietal cortex processes spatial information in a way that\\ndepends on the orientation in space of the both the body and objects around us[1]. It codes for the location and orientation of an object relative to our body\\nor for the position of our diﬀerent limbs. This coding is egocentric and varies\\nwith our position in space.\\nIt is important to note that there is no single egocentric space, instead many\\ndiﬀerent ones are needed depending on the task. For example, shifting gaze\\ntoward an object needs an egocentric space originating in the eyes, while reach-ing for an object needs an egocentric space grounded in the torso, etc. Since\\nour sensory organs do not operate in the same spatial frame as our limbs, the\\nbrain needs to constantly convert between all the diﬀerent coordinate systems.This processing of multiple simultaneous coordinate systems serve to connect\\nthe internal and external space as well as the body and the environment. The',\n",
       "  'diﬀerent coordinate systems are organized as foreground and background – focusand context. Foreground and background are associated by default, and takes\\neﬀort to separate. Most things can be focused on, most things can be context;\\nwhich is which depends on current goals and needs and one can change into the\\nother when needed.\\n4 Sensorimotor Encoding of Aﬀordances and Outcomes\\nThe higher level reﬂexes are sensorimotor mappings at a cortical level that allowsthe organism to interact with objects in the environment to obtain diﬀerent forms\\nof outcomes. Such sensory motor mappings are organized around aﬀordances\\n[12]. The theory of aﬀordances challenges the traditional view of perception and\\naction as separate processes. Instead, it suggests that perception and action aredeeply intertwined, and that our perception of the environment is inﬂuenced\\nby our potential actions within it. For example, a chair aﬀords sitting, a door',\n",
       "  'aﬀords opening and closing, and a ball aﬀords throwing and catching. Aﬀordanceprocessing starts at the sensory side that code for the relevant features of the\\nattended stimulus, to capture such thing as its identity, shape and location and',\n",
       "  '16 C. Balkenius et al.\\norientation in space. These diﬀerent functions are handled by diﬀerent regions in\\ncortex including the inferior temporal which is primarily involved in identiﬁca-tion, and parietal cortex which is more involved in processing spatial properties\\nof an object. This in turn are mapped onto possible actions in motor cortex.\\nThere is evidence that these sensory motor mapping also generate predictionsabout the expected outcome of engaging with an object in a particular way [ 9].\\nAﬀordances are thus sensorimotor structures that cause the organism to interact\\nwith the environment in a certain way. It is not a representation of either theobject or the action.\\nAﬀordances are learned from the ongoing interaction with the world as the\\nsensory and motor codes of the cerebral cortex develop to code for diﬀerentinteractions with the environment. The reﬂex systems are essential in producing\\nthe initial behavior that will train the aﬀordance system. It is a fundamental',\n",
       "  'property of the involved brain regions that they code not only for what has been\\nseen or done, but also for what could possibly be seen or done. For example, the\\nvisual system should not only form codes for the few dogs that we have seen, butrather develop a conceptual space [ 11] that can code also for any unseen dog.\\nSimilarly, having learned to grasp a small set of objects, we acquire the ability\\nto grasp nearly any object.\\nWe also learn the expected outcome of the interaction. Throwing a ball\\ntoward a target, we expect it to hit that target, and the desire to do so may cause\\nus to interact with the ball in a particular way. The learned behaviors are thusgoal directed in two ways: both in relation to the manipulated object and to the\\nintended goal. Aﬀordance learning goes beyond reﬂex system in that it results in\\nan ever expanding repertoire of potential behaviors that can be applied ﬂexiblyto novel situation in a goal-directed way. Through aﬀordances, the environment',\n",
       "  'suggests potential actions while our learned expectations suggest possible out-\\ncomes of those possibilities.\\nThe aﬀordance competition hypothesis suggests that diﬀerent aﬀordances\\ncompete for activation [ 6]. According to this hypothesis, when we perceive an\\nobject or environment, multiple aﬀordances are presented to us simultaneously,\\nand these aﬀordances compete for our attention and selection. The selection of\\nan aﬀordance is determined by a combination of factors, including our goals,intentions, abilities, and the salience of the aﬀordances presented.\\nInteraction in terms of aﬀordances captures the essential features of an enac-\\ntive view of cognition as well as naturally leading to a situated view of action.\\n5 Top Level Control\\nThe sensorimotor systems can produce very complicated behavior, but on theirown, they lack three important features. The ﬁrst is to select the most appropri-\\nate sensorimotor scheme to obtain a particular goal taking into account diﬀerent',\n",
       "  'rewards or pay-oﬀs. The second is that the sensorimotor systems on their ownoften react too slowly to changes in the environment since they are only reac-\\ntive. Third, they do not usually take the context into account, meaning that the',\n",
       "  'Elements of Cognition for General Intelligence 17\\nproduced behavior may be useful for the currently attended stimulus, but inap-\\npropriate in reaching longer term goals. These problems are addressed by thethree top level control system of the brain: the basal ganglia, the cerebellum and\\nthe prefrontal cortex. These systems modulate the operation of the sensorimotor\\naﬀordance systems while also interacting with the other top-level systems in anintricate way.\\nThe main site for value based selection of aﬀordances is the basal gan-\\nglia [13,20]. Although they resides deep in the brain, the basal ganglia are at the\\ntop level functionally. Speciﬁcally, they select the sensorimotor loop that is both\\ncompatible with the stimulus at the focus of attention andthe overall goal of\\nthe organism. Without the basal ganglia, the most salient stimulus would controlbehavior, but with it behavior becomes more goal directed in a long term sense.',\n",
       "  'The basal ganglia is also responsible for starting and stopping behaviors when\\nthey have obtained their goals. When the basal ganglia operates on external\\nactions it leads the organism toward a goal via both the control of navigation\\nto goal-places in physical space, and in manipulation of the correct objects atthose places. When operating internally, it produces deliberate reasoning by nav-\\nigation through, and manipulating abstract entities in, a conceptual space. In\\nterms of learning algorithms, learning in the basal ganglia has similarities withreinforcement learning [ 23]. It is sensitive to rewards and punishment and tends\\nto select aﬀordances that leads to behaviors that are compatible with a larger\\nfuture reward.\\nThe second top-level control system consists of the cerebellum. Its main func-\\ntion is to modulate or trigger behaviors based on anticipation of ﬂuctuations\\nor events within or outside the organism. The cerebellum is fundamentally apredictive machine [ 18,25].',\n",
       "  'The cerebellum has been mainly studied in relation to lower-level reﬂexes\\nsuch as the conditioned blink reﬂex or the vestibulo-occular reﬂex that stabilizesthe gaze when we move, but is in fact connected to most of the cortex which\\nmeaning it can enact predictions at all levels in the hierarchy. One consequence\\nof this is that the cerebellum will reduce the inﬂuences of external disturbances\\nby stabilizing movement, for example in balance control. But it will also act\\nas an automation device that will produce behaviors automatically when thosebehaviors have been trained repeatedly. The predictions made by the cerebellum\\nare based on the state of the whole brain and can inﬂuence nearly all processing.\\nIt enacts predictions as movements, adjustments or even internal thoughts butalso work together with the rest of the brain [ 21] to anticipates external events.\\nThe cerebellum observers and learns sequences that are done repeatedly, often',\n",
       "  'initially with eﬀortful sequential behaviour. When the sequences are learned, thecerebellum can produce them on its own. This can be playing the piano, timing\\nof motor inhibition but also doing mental operations such as addition, regulating\\nemotions, inhibiting distractors, or keeping a conversation going.\\nThe third top-level system is responsible for executive control [2]. This\\ncontrol consists of three components: working memory ,set,a n dinhibition [10].\\nThe three components can alternatively be seen as a mechanism for contextual',\n",
       "  '18 C. Balkenius et al.\\nselection of diﬀerent sensorimotor schemas. While set refers to a collection of\\nbehaviors that are appropriate for the current task, working memory can beseen as referring to contextual information of a more short-term duration. Both\\naspects of the context inﬂuences the rest of the brain by suppressing sensory pro-\\ncessing and behavior that is inappropriate in the current context. Inhibition hasthe role of suppressing stimuli that are irrelevant to the task, or behaviors that\\nare compatible with the stimulus in focus, but not with the task at hand. This\\nmeans that attention is directed around the sensory sphere by executive con-trol. In addition, executive control is responsible for moving attention internally\\ntowards diﬀerent sensory modalities, or diﬀerent patterns within modalities. For\\nexample, you can focus on just the sound, or just the temperature of a stimulus.But you can also focus on some speciﬁc property like the color of an object, or',\n",
       "  'its shape. Moving attention internally is similar to looking around in the world,\\nand uses the same brain mechanisms. The only diﬀerence is that information\\ncomes from our memory instead from the world in these cases [ 17].\\n6 Discussion\\nAnimals have surprisingly similar brains despite very diﬀerent bodies and living\\nenvironments. This suggests that the design of the brain is suﬃciently versatile\\nto adapt to almost any situation. We have argued that the generality of the\\nbrain comes from the fact that it operates both externally and internally based\\non previous experiences of sensorimotor interactions with diﬀerent aspects of theenvironment. A diﬀerent environment oﬀers diﬀerent possibilities or aﬀordances\\nand shapes the brain in a diﬀerent direction.\\nAlthough the diﬀerent parts of the brain must be described separately, they\\nare highly interwoven into a complex web where nearly every region is involved\\nin nearly every task, putting distinct labels on each of the subsystem thus some-',\n",
       "  'times leads in the wrong direction.\\nFor example, many associate the hippocampus with episodic memory, but\\nthis structure is also heavily involved in learning expectations and processing\\nspatial information, particularly in terms of navigation. In a sense episodic mem-ories are records of change in our environment from the ﬁrst person perspective.\\nHowever, this ability appears to mediate more general abilities to navigate also\\nbetween “conceptual places” [ 8]. Our ability to distinguish between diﬀerent\\nphysical places allows us also to handle more abstract contexts and situations.\\nIn this way, the brain can handle going from playing chess to playing checkers\\non the same board: chess and checkers are diﬀerent ‘places’ where behaviour is\\ndiﬀerent, even if the physical context is the same. In terms of what makes for\\ngeneral predictive abilities, it is interesting to look closer at the brain’s spatialprocessing abilities.',\n",
       "  'Processing in the brain is intermingled with our actions. Manipulation can\\nbe used to discover aﬀordances: how something can be used to achieve goals,which in turn shapes the brain and how diﬀerent aspects of this interaction is\\ncoded in sensorimotor structures. By trying to push, shove, drag, and tear at',\n",
       "  'Elements of Cognition for General Intelligence 19\\nobjects we learn about them. Discovering aﬀordances like this is rewarding, and\\nallows us to better predict the world. It also allows us to acquire skills enablingus to transform the world as a means to stay alive.\\nTogether, the structures that mediate manipulation and movement and\\nunderstanding of space can be thought of as mediating more general do-what-\\nwhere abilities. Hence, without moving physically, you can still move around in\\nconceptual space, stopping to perform mental transformations that are appro-\\npriate at that particular place: one place may be related to language and editingtext, another may be statistical and analysing a data set, a third may be philo-\\nsophical and constructing an argument. To the brain, though, these abstract\\nactivities may be coded in the same way as when you go to your cabin to chopwood.\\nAlthough we described some of the main elements of the cognitive mechanism',\n",
       "  'above, a complete model will also have to include state systems that keeps track\\nof diﬀerent needs of the organisms and organizes behavior over time. Aﬀective\\nsystems that are used for evaluation of stimuli and situations are also needed.Furthermore, we did not here mention social and cultural aspects that also inﬂu-\\nences the developing brain to a large extent.\\nThe list of brain processes reviewed above constitute a major part of what\\nis needed to produce intelligence in biological systems. Although we have not\\ngone into the details of how each system operates, we propose that there are\\ncomputational models that reproduce the processing in each of the diﬀerentcomponents at a level that makes it possible to put together a system-level\\nmodel of the brain. It is our belief that such a model would be able to control\\nan artiﬁcial body and operate in a natural environment.\\nTo do this, a suﬃciently powerful tool for system-level brain modeling is',\n",
       "  'needed that can run all the needed component and is also able to control a robot\\nso that the system can be tested in interaction with the real world. Toward thisend, we have been building an infrastructure for system-level brain modeling\\nover the last 20 years [ 4]. The Ikaros-system consists of a real-time kernel for the\\nexecution of large-scale brain models together with interfaces to control robots.\\nA large number of suitable models of diﬀerent brain regions have been developed\\nboth withing the Ikaros-project and outside it.\\nFurthermore, we have developed a humanoid robotic platform Epi [ 14] that is\\nclosely integrated with the Ikaros system. Using Ikaros and Epi we are gradually\\nreﬁning and developing the BAM model that aims at eventually reproducing thewhole brain.\\nReferences\\n1. Andersen, R.A., Essick, G.K., Siegel, R.M.: Encoding of spatial location by poste-\\nrior parietal neurons. Science 230(4724), 456–458 (1985)',\n",
       "  '2. Baddeley, A.: Working memory. Curr. Biol. 20(4), R136–R140 (2010)\\n3. Balkenius, C.: Natural intelligence in artiﬁcial creatures. Lund University Cognitive\\nStudies (1995)',\n",
       "  '20 C. Balkenius et al.\\n4. Balkenius, C., Johansson, B., Tjøstheim, T.A.: Ikaros: a framework for controlling\\nrobots with system-level brain models. Int. J. Adv. Robot. Syst. 17, 1–12 (2020)\\n5. Brooks, R.: A robust layered control system for a mobile robot. IEEE J. Robot.\\nAutom. 2(1), 14–23 (1986)\\n6. Cisek, P.: Cortical mechanisms of action selection: the aﬀordance competition\\nhypothesis. Philos. Trans. Roy. Soc. B Biol. Sci. 362(1485), 1585–1599 (2007)\\n7. Clark, A.: Mindware: An Introduction to the Philosophy of Cognitive Science.\\nOxford University Press, Oxford (2000)\\n8. Epstein, R.A., Patai, E.Z., Julian, J.B., Spiers, H.J.: The cognitive map in humans:\\nspatial navigation and beyond. Nat. Neurosci. 20, 1504–1513 (2017)\\n9. Fagg, A.H., Arbib, M.A.: Modeling parietal-premotor interactions in primate con-\\ntrol of grasping. Neural Netw. 11(7–8), 1277–1303 (1998)\\n10. Fuster, J.: The Prefrontal Cortex. Academic Press (2015)',\n",
       "  '11. Gardenfors, P.: Conceptual Spaces: The Geometry of Thought. MIT Press, Cam-\\nbridge (2004)\\n12. Gibson, J.J.: The theory of aﬀordances, Hilldale, USA, vol. 1, no. 2, pp. 67–82\\n(1977)\\n13. Graybiel, A.M.: The basal ganglia: learning new tricks and loving it. Curr. Opin.\\nNeurobiol. 15(6), 638–644 (2005)\\n14. Johansson, B., Tjøstheim, T.A., Balkenius, C.: Epi: an open humanoid platform for\\ndevelopmental robotics. Int. J. Adv. Rob. Syst. 17(2), 1729881420911498 (2020)\\n15. Johansson, R., Holsanova, J., Holmqvist, K.: Pictures and spoken descriptions\\nelicit similar eye movements during mental imagery, both in light and in completedarkness. Cogn. Sci. 30(6), 1053–1079 (2006)\\n16. Kandel, E.R., Schwartz, J.H., Jessell, T.M., Siegelbaum, S., Hudspeth, A.J., Mack,\\nS., et al.: Principles of Neural Science, vol. 4. McGraw-Hill, New York (2000)\\n17. Munakata, Y., Herd, S.A., Chatham, C.H., Depue, B.E., Banich, M.T., O’Reilly,',\n",
       "  'R.C.: A uniﬁed framework for inhibitory control. Trends Cogn. Sci. 15(10), 453–\\n459 (2011)\\n18. Ohyama, T., Nores, W.L., Murphy, M., Mauk, M.D.: What the cerebellum com-\\nputes. Trends Neurosci. 26(4), 222–227 (2003)\\n19. O’Keefe, J., Nadel, L.: The Hippocampus as a Cognitive Map. Oxford University\\nPress, Oxford (1978)\\n20. Redgrave, P., Prescott, T.J., Gurney, K.: The basal ganglia: a vertebrate solution\\nto the selection problem? Neuroscience 89(4), 1009–1023 (1999)\\n21. Schmahmann, J.D.: The cerebellum and cognition. Neurosci. Lett. 688, 62–75\\n(2019)\\n22. Smith, R., Thayer, J.F., Khalsa, S.S., Lane, R.D.: The hierarchical basis of neuro-\\nvisceral integration. Neurosci. Biobehav. Rev. 75, 274–296 (2017)\\n23. Sutton, R.S., Barto, A.G.: Reinforcement Learning: An Introduction. MIT Press,\\nCambridge (2018)\\n24. Wiener, N.: Cybernetics or Control and Communication in the Animal and the\\nMachine. MIT Press, Cambridge (2019)',\n",
       "  '25. Wolpert, D.M., Miall, R.C., Kawato, M.: Internal models in the cerebellum. Trends\\nCogn. Sci. 2(9), 338–347 (1998)',\n",
       "  'Comparing NARS and Reinforcement\\nLearning: An Analysis of ONA\\nand Q-Learning Algorithms\\nAli Beikmohammadi(B)and Sindri Magn´ usson\\nDepartment of Computer and Systems Sciences,\\nStockholm University, 106 91 Stockholm, Sweden\\n{beikmohammadi,sindri.magnusson }@dsv.su.se\\nAbstract. In recent years, reinforcement learning (RL) has emerged as\\na popular approach for solving sequence-based tasks in machine learn-\\ning. However, ﬁnding suitable alternatives to RL remains an excitingand innovative research area. One such alternative that has garnered\\nattention is the Non-Axiomatic Reasoning System (NARS), which is a\\ngeneral-purpose cognitive reasoning framework. In this paper, we delveinto the potential of NARS as a substitute for RL in solving sequence-\\nbased tasks. To investigate this, we conduct a comparative analysis of\\nthe performance of ONA as an implementation of NARS and Q-Learning\\nin various environments that were created using the Open AI gym. The',\n",
       "  'environments have diﬀerent diﬃculty levels, ranging from simple to com-\\nplex. Our results demonstrate that NARS is a promising alternative to\\nRL, with competitive performance in diverse environments, particularly\\nin non-deterministic ones.\\nKeywords: AGI\\n·NARS·ONA·Reinforcement Learning ·\\nQ-Learning\\n1 Introduction\\nReinforcement Learning (RL) is a type of machine learning that enables agents\\nto make decisions in an environment to maximize their cumulative reward over\\ntime. Combining RL with high-capacity function approximations in model-free\\nalgorithms oﬀers the potential to automate a wide range of decision-making and\\ncontrol tasks [ 11]. Such algorithms have successfully tackled complex problems in\\nvarious domains, such as game playing [ 10], ﬁnancial markets [ 3], robotic control\\n[9], and healthcare [ 18]. However, RL faces challenges in environments where it\\nis diﬃcult or costly to generate large amount of data. This is due to the lack of',\n",
       "  'compositional representations that would enable eﬃcient learning [ 4].\\nTaking a broader perspective, the ultimate goal of Artiﬁcial General Intelli-\\ngence (AGI) is to create intelligent systems that can adapt and learn to solve a\\nbroad range of tasks in diverse environments. RL has been a popular approach\\nc⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2023\\nP. Hammer et al. (Eds.): AGI 2023, LNAI 13921, pp. 21–31, 2023.https://doi.org/10.1007/978-3-031-33469-6\\n_3',\n",
       "  '22 A. Beikmohammadi and S. Magn´ usson\\nin this pursuit, but its limitations in handling environments with limited data\\nand complex, abstract representations have hindered its progress towards AGI.To overcome these limitations, it is essential to explore alternative approaches\\nthat can facilitate data-eﬃcient learning and deal with compositional repre-\\nsentations eﬀectively. One such approach is Non-Axiomatic Reasoning System(NARS), which is a promising approach that addresses the challenges posed by\\ncomplex and uncertain environments, as it is a general-purpose reasoner that\\nadapts under the Assumption of Insuﬃcient Knowledge and Resources (AIKR)[6,14,16].\\nImplementations based on non-axiomatic logic have been developed, such\\nas OpenNARS [ 7] and ONA (OpenNARS for Applications) [ 5]. ONA surpasses\\nOpenNARS in terms of reasoning performance and has recently been compared\\nwith RL [ 2,4]. Several challenges arise when comparing the performance of ONA',\n",
       "  'andQ-Learning, a basic approach in RL [ 17], algorithms. These challenges have\\nbeen discussed in [ 4] and include dealing with statements instead of states, unob-\\nservable information, one action in each step, multiple objectives, hierarchicalabstraction, changing objectives, and goal achievement as reward .\\nIn [4], three simple environments; Space invaders, Pong, and grid robot were\\nused to compare ONA with Q-Learning [ 17], and the results showed that ONA\\nprovided more stable outcomes while maintaining almost identical success ratio\\nperformance as Q-Learning. To enable a meaningful and fair comparison, an\\nextra nothing action added to the Q-Learner in each example since the com-\\npetitor, ONA, does not assume that in every step, an action has to be chosen.\\nHowever, this change raises concerns about preserving the problems’ authentic-\\nity.\\nIn this paper, we aim to investigate the potential of NARS as a substitute for',\n",
       "  'RL algorithms and explore its capability to facilitate more eﬃcient and eﬀective\\nlearning in AGI systems. Speciﬁcally, we compare the performance of ONA andQ-Learning on several more challenging tasks compared to [ 4], including non-\\ndeterministic environments. Also, in contrast with [ 4], we propose selecting a\\nrandom action when ONA does not recommend any action to be taken to keep\\nthe originality of the tasks/environments as much as possible. This approach\\ncan also beneﬁt the agent in terms of exploring the environment. Our ﬁndingsprovide insights into the potential of NARS as an alternative to RL algorithms\\nfor developing more intelligent and adaptive systems.\\nThis paper is organized as follows: Methods are described in Sect. 2, tasks and\\nsetups are expressed in Sect. 3, experimental results and analyses are reported\\nin Sect. 4, and we conclude and discuss future work in Sect. 5.\\n2 Methods\\n2.1 RL and Tabular Q-Learning',\n",
       "  'RL, in which an agent interacts with an unknown environment, typically is\\nmodeled as a Markov decision process (MDP). The MDP is characterized by a',\n",
       "  'An Analysis of ONA and Q-Learning Algorithms 23\\ntuple M=⟨S,A,r,p,γ ⟩, where Sis a ﬁnite set of states, Ais a ﬁnite set of\\nactions, r:S×A×S→ Ris the reward function, p(st+1|st,at) is the transition\\nprobability distribution, and γ∈(0,1] is the discount factor. Given a state s∈S,\\na policy π(a|s) is a probability distribution over the actions a∈A. At each time\\nstept, the agent is in a state st, selects an action ataccording to a policy π(.|st),\\nand executes the action. The agent then receives a new state st+1∼p(.|st,at)\\nand a reward r(st,at,st+1) from the environment. The objective of the agent is\\nto discover the optimal policy π∗that maximizes the expected discounted return\\nGt=Eπ[∑∞\\nk=0γkrt+k|St=s] for any state s∈Sand time step t.\\nTheQ-function qπ(s,a) under a policy πis the expected discounted return\\nof taking action ain state sand then following policy π. It is established\\nthat for every state s∈Sand action a∈A, every optimal policy π∗',\n",
       "  'satisﬁes the Bellman optimality equations (where q∗=qπ∗);q∗(s,a)=∑\\ns′∈Sp(s′|s,a)(r(s,a,s′)+γmax a′∈Aq∗(s′,a′)). It should be noted that if q∗\\nis known, selecting the action awith the highest value of q∗(s,a) always results\\nin an optimal policy.\\nTabular Q-learning [17] is a popular RL method, which estimates the optimal\\nQ-function using the agent’s experience. The estimated Q-value is denoted as\\n˜q(s,a). At each iteration, the agent observes the current state sand chooses\\nan action abased on an exploratory policy. One commonly used exploratory\\npolicy is the ϵ-greedy policy, which randomly selects an action with probability\\nϵ, and chooses the action with the highest ˜ q(s,a) value with probability 1 −ϵ.I n\\nthis paper, as for ϵ, we have employed an exponentially decaying version, where\\nϵ=ϵmin+(ϵmax−ϵmin)·exp(−decay ·episodecounter ).\\nAfter the agent selects an action aand transitions from state sto\\ns′, the resulting state s′and immediate reward r(s,a,s′) are used to',\n",
       "  'update the estimated Q-value of the current state-action pair ˜ q(s,a).\\nThis is done using the Q-learning update rule; ˜ q(s,a)←˜q(s,a)+α·\\n(r(s,a,s′)+γmax a′˜q(s′,a′)−˜q(s,a)),where αis the learning rate hyperpa-\\nrameter. If the resulting state s′is a terminal state, the update rule simpliﬁes\\nto ˜q(s,a)←˜q(s,a)+α·(r(s,a,s′)−˜q(s,a)).\\nThe convergence of Tabular Q-learning to an optimal policy is guaranteed,\\nprovided that every state-action pair is visited inﬁnitely often. As a learningmethod, this algorithm is classiﬁed as oﬀ-policy because it has the ability to\\nlearn from the experiences generated by any policy.\\n2.2 NARS and ONA\\nNARS is an AI project that aims to create a general-purpose thinking machine.\\nThe underlying theory behind NARS is that intelligence is the ability for asystem to adapt to its environment while working with insuﬃcient knowledge\\nand resources, as proposed by Wang [ 12,13].\\nNARS is a reasoning system that is based on the principles of Non-Axiomatic',\n",
       "  'Logic (NAL). NAL is a formal logic that includes a formal language, Narsese,\\na set of formal inference rules, and semantics. Conceptually, NAL is deﬁned in',\n",
       "  '24 A. Beikmohammadi and S. Magn´ usson\\na hierarchical manner, consisting of several layers, with each layer introducing\\nnew grammar and inference rules. This approach extends the logic and enhancesits capability to express and infer, resulting in a more powerful reasoning sys-\\ntem. NAL allows for uncertainty and inconsistency in reasoning, making it more\\nsuitable for real-world applications where knowledge is often incomplete anduncertain [ 15].\\nNARS attempts to provide a normative model of general intelligence, rather\\nthan a descriptive model of human intelligence, although the latter can be seenas a special case of the former. Thus, while there may be some diﬀerences, the\\ntwo types of models are similar in various aspects. The control component of\\nNARS is mainly composed of a memory mechanism and an inference controlmechanism [ 15]. The logic supports to reason on events coming from the agent’s\\nsensors in real-time, using an open-ended inference control process which does',\n",
       "  'not terminate, whereby both forward (belief reasoning) and backward chaining\\n(goal and question derivation) happen simultaneously. The system draws conclu-\\nsions from the available evidence in the premises, and then uses those conclusionsto guide future reasoning and decision-making with a form of dynamic resource\\nallocation, whereby only the most useful knowledge is kept in memory to satisfy\\na strictly bounded memory supply.\\nNARS represents knowledge as statements with attached truth and desire\\nvalues, and uses inference rules to derive new knowledge from the premises,\\nwhereby truth functions are used to calculate conclusion evidence from the evi-dence summarized in the premises. In this system, to measure evidential support\\nusing relative measurements, a truth value is a pair of rational numbers in the\\nrange from 0 to 1. The ﬁrst element of the truth value is frequency, and thesecond is conﬁdence. Frequency is deﬁned as the proportion of positive evidence',\n",
       "  'among total evidence, that is, (positive evidence)/(total evidence). Conﬁdence\\nindicates how sensitive the corresponding frequency is with respect to new evi-dence, as it is deﬁned as the proportion of total evidence among total evidence\\nplus a constant amount of new evidence, that is, (total evidence)/(total evidence\\n+ k) where k is a system parameter and in most discussions takes the default\\nvalue of 1. Thus frequency can be seen as the degree of belief system has for the\\nstatement and conﬁdence as the degree of belief for that estimation of frequency.In this system, desire values have the same format as truth values, and indicate\\nhow much the system wants to achieve a statement (making it happen, essen-\\ntially). The desire values of input goals can be assigned by the user to reﬂecttheir relative importance or take default values [ 15].\\nONA is an implementation of NARS designed for real-world applications. Com-',\n",
       "  'pared to OpenNARS, ONA includes ﬁrmer design decisions which make thesoftware more eﬀective for practical purposes. Additionally, ONA aims to make\\nNARS more accessible to users and developers by providing a Python interface\\nand a range of miscellaneous tools that can be used to build applications [ 4,5].\\nAdditionally, NARS and ONA use the same formal language called Narsese,\\nwhich allows to express NAL statements. Narsese can represent beliefs, goals,and questions, and in ONA also the inference rules on the meta-level make use of',\n",
       "  'An Analysis of ONA and Q-Learning Algorithms 25\\nFig. 1. OpenAI gym environments used as experiment tasks; (a) CliﬀWalking-v0; (b)\\nTaxi-v3; (c) FrozenLake-v1 4 ×4; (d) FrozenLake-v1 8 ×8; (e) FlappyBird-v0\\nit to be more easily editable. ONA also provides a simple standard-I/O interface,\\nwhich can be used to interface with other programming languages and systems\\nand to work with data sources which can stream in Narsese statements into\\nthe system [ 4,5]. In this publication, ONA was chosen as the implementation to\\ncompare with the tabular Q-learning algorithm.\\n3 Setups and Environments\\nThroughout the section, we describe how we implement both methods and com-\\npare their performance in diﬀerent environments. Due to the stochastic nature\\nof algorithms/environments and the dependency on hyperparameters, tabularQ-learning algorithm is notoriously diﬃcult to evaluate. To comprehensively\\ncompare diﬀerent algorithms, several environments, and network initialization',\n",
       "  'seeds should be taken into account when tuning hyperparameters [ 8]. In this\\nregard, to compare ONA with a tabular Q-learning [ 17] with exponentially\\ndecaying ϵvalue, we conduct a grid search to tune the hyperparameters. For\\neach combination of hyperparameters, we run the algorithm 10 times with dif-\\nferent initialization and environment seeds. The conﬁguration reported in the\\npaper is the one that yielded the best performance on average among all tasks.In the case of Q-learning, we set α=0.7,γ=0.618,ϵ\\nmax=1 ,ϵmin=0.01,\\ndecay = 0 .01. On the other hand, regarding ONA hyperparameters, speciﬁcally\\nmotorbabbling , we use the default value as used in ONA v0.9.1 [ 5]. However,\\nbabblingops is changed due to the variety of the number of available actions in\\neach of the environments. Also, we use setopname to set allowed actions in ONA.\\nThe source code of our implementation is available online: https://github.com/\\nAliBeikmohammadi/OpenNARS-for-Applications/tree/master/misc/Python',\n",
       "  'We primarily rely on the assumptions outlined in [ 4], unless explicitly stated\\notherwise. To make the practical comparison possible, as for ONA, the eventshold the same information as the corresponding states the Q-Learner receives\\nin the simulated experiments, except for FlappyBird-v0. To be more speciﬁc,\\nas mentioned in [ 4], when sis an observation, it is interpreted by ONA as the\\nevent (s. : |:), and by the Q-Learner simply as the current state. Then both\\nalgorithms suggest an operation/action by exploitation or sometimes randomly.',\n",
       "  '26 A. Beikmohammadi and S. Magn´ usson\\nAfter feeding the action to the environment, we receive new observation, reward,\\nand some info about reaching the goal. The reward for the Q-Learner is used\\nwithout any change, while ONA receives an event (G. : |:) when the task is\\ncompletely done. So there is no event if rewards are related to anything except\\nﬁnishing the task. This, of course, assumes that the goal does not change, as elsetheQ-table entries would have to be re-learned, meaning the learned behavior\\nwould often not apply anymore. For the purposes of this work, and for a fair\\ncomparison, the examples include a ﬁxed objective.\\nWe use challenging control tasks from OpenAI gym benchmark suite [ 1]\\n(Fig.1). ONA and Q-Learning algorithms were developed for discrete tasks;\\nhence we have to map FlappyBird-v0 observation space for each algorithm,which we describe below. Except for FlappyBird-v0, we used the original envi-',\n",
       "  'ronments with no modiﬁcations to the environment or reward. In FlappyBird-v0,\\nthe observations are: ( O\\n1) the horizontal distance to the next pipe, and ( O2)t h e\\ndiﬀerence between the player’s yposition and the next hole’s yposition. We\\nhave mapped this continuous observation space to a discrete space. Speciﬁcally,as for ONA, the event is “round(100x O\\n1)round(1000x O2). :|:”, which could\\nbe for instance “138 -4. :|:”. However, since for deﬁning Q-table, the states\\nshould correspond to the speciﬁc row, we have to subtly change the mappingto “|round(100x O\\n1)|+|round(1000x O2)|”, which results “142”, for our instance.\\nHowever, one could ﬁnd a better way to do this mapping.\\nAlthough [ 1] describes all environments, we emphasize FrozenLake-v1’s\\n“isslippery” argument, allowing for a non-deterministic environment. This fea-\\nture is interesting to observe how algorithms perform in such a scenario. When\\n“isslippery” is True, the agent has a 1/3 probability of moving in the intended',\n",
       "  'direction; otherwise, it moves in either perpendicular direction with an equal\\nprobability of 1/3. For instance, if the action is left and “is slippery” is True,\\nthen P(move left) = 1/3, P(move up) = 1/3, and P(move down) = 1/3. In thenext section, we examine both algorithms’ performances in detail on all these\\ntasks.\\n4 Results and Discussion\\nTwo criteria, including reward, and cumulative successful episodes, are mon-\\nitored, as shown in Figs. 2,a n d 3. Both techniques are run 10 times in each\\nexperiment, and the behavior of metrics is kept track of for each time step\\nacross 100000 iterations. The solid curves show average training performance,\\nwhile the shaded region indicates the standard deviation of that speciﬁc metric\\nover 10 trials. This provides an idea of the algorithm’s robustness. A high gained\\nmetric with a low variance is considered more reliable than achieving the sameperformance with a high variance. So, the standard deviation gives a complete',\n",
       "  'picture of the algorithm’s performance.\\nAs can be seen from Figs. 2and3, the results of two algorithms are\\nvery dependent on the task and one cannot be considered superior for all\\nenvironments. Speciﬁcally, the Q-Learning algorithm has performed better on',\n",
       "  'An Analysis of ONA and Q-Learning Algorithms 27\\nFig. 2. Reward vs. Time steps. The reward is measured at time steps where the episode\\nends (by reaching the goal, truncating the episode length, falling into the hole, fallingfrom the cliﬀ, hitting the pipe.)\\nFig. 3. Cumulative Successful Episodes vs. Time steps.\\nCliﬀWalking-v0, Taxi-v3, and FlappyBird-v0 environments. But ONA is more\\npromising on environments based on FrozenLake-v1. Moreover, Fig. 3illustrates\\nthe noteworthy observation that ONA exhibits greater reliability.\\nAn interesting observation is the good ability of ONA to solve non-\\ndeterministic problems, where it is able to solve the slippery-enable problems\\nas shown in Figs. 2d,2f,3d, and 3f, while Q-Learning has not shown reliable',\n",
       "  '28 A. Beikmohammadi and S. Magn´ usson\\nFig. 4. Cumulative Random Action vs. Time steps.\\nsuccess in solving these problems. It may be possible to draw conclusions from\\nQ-Learning by adjusting its hyperparameters. However, it should be noted that\\nany time-dependent hyperparameters are speciﬁc to the environment and should\\nbe avoided when evaluating generality. Additionally, as ϵdecreases over time, the\\nQ-Learner will take longer to adapt its policy to new circumstances. In contrast,\\nit is evident that ONA oﬀers greater reliability due to having fewer hyperparam-\\neters. Unlike Q-Learning, ONA does not require speciﬁc reductions in learning or\\nexploration rates to function well on a given task, and therefore needs less param-eter tuning. For instance, ONA does not rely on learning rate decay. Instead,\\nthe extent to which new evidence alters an existing belief is dependent solely',\n",
       "  'on the degree of evidence that already supports it, which automatically rendershigh-conﬁdence beliefs more stable. This results in a more consistent learning\\nbehavior for ONA.\\nWe also monitored the frequency of random action selection. In Figs. 4and\\n5, the behavior of the two algorithms is drawn in terms of referring to a random\\nor non-random action. In the case of Q-Learning, the probability of selecting\\na random action is primarily determined by the value of ϵ. This probability\\ndecreases rapidly over time. Consequently, if the agent has not yet discovered a\\ngood policy or the environment changes, Q-Learning may not be able to solve\\nthe problem. Reducing ϵover time can make it increasingly diﬃcult to explore\\nalternative solutions. The low variance of Q-Learning shows the agent’s decision\\nis unaﬀected by changes in the environment across diﬀerent trials. This is becausethe random action-taking process is largely driven by ϵ, which is independent',\n",
       "  'An Analysis of ONA and Q-Learning Algorithms 29\\nFig. 5. Cumulative Non-Random Action vs. Time steps.\\nof the problem. On the other hand, in ONA, if the system does not suggest\\nany action, we choose a random action, as shown in Fig. 4. In addition, there\\nis also a possibility of the system suggesting a random action itself to explore\\nthe environment, thanks to the motorbabbling parameter. So some of the actions\\ns h o w ni nF i g . 5, despite being labeled as non-random, are actually exploratory\\nand random in nature. In fact, ONA is able to decrease the motorbabbling on its\\nown once it has established stable hypotheses and accurate predictions, without\\nrelying on a reduction of the exploration rate that is dependent on time. Thiscould be one of the reasons why ONA is successful, even though it doesn’t receive\\nfrequent rewards like Q-Learning. So, we believe ONA is capable of handling',\n",
       "  'multiple and changing objectives, while also requiring less implicit example-dependent parameter tuning compared to Q-Learning.\\n5 Conclusion and Future Work\\nIn this paper, we made a comparison between ONA and Q-learning on seven\\ntasks. Given that both approaches demonstrate comparable performance on\\naverage, our study suggests that NARS has the potential to be a viable sub-stitute for RL in sequence-based tasks, particularly for non-deterministic prob-\\nlems. While further research is necessary to determine the full extent of NARS’s\\ncapabilities and limitations, our results oﬀer new avenues for exploration andinnovation in the ﬁeld of machine learning. Future work can extend our examples\\nto multi-objective and changing objective scenarios. Additionally, a combination',\n",
       "  '30 A. Beikmohammadi and S. Magn´ usson\\nof both approaches through methods like voting, hierarchical or teacher-student\\nlearning could be explored.\\nAcknowledgment. We would like to express our gratitude to Patrick Hammer,\\nPh.D., for his expert advice, encouragement, and proofreading of the manuscript\\nthroughout this work. This work was partially supported by the Swedish Research\\nCouncil through grant agreement no. 2020-03607 and in part by Digital Futures, theC3.ai Digital Transformation Institute, and Sweden’s Innovation Agency (Vinnova).\\nThe computations were enabled by resources in project SNIC 2022/22-942 provided\\nby the Swedish National Infrastructure for Computing (SNIC) at Chalmers Centre forComputational Science and Engineering (C3SE).\\nReferences\\n1. Brockman, G., et al.: OpenAI gym. arXiv preprint arXiv:1606.01540 (2016)\\n2. Eberding, L.M., Th´ orisson, K.R., Sheikhlar, A., Andrason, S.P.: SAGE: task-\\nenvironment platform for evaluating a broad range of AI learners. In: Goertzel,',\n",
       "  'B., Panov, A.I., Potapov, A., Yampolskiy, R. (eds.) AGI 2020. LNCS (LNAI),\\nvol. 12177, pp. 72–82. Springer, Cham (2020). https://doi.org/10.1007/978-3-030-\\n52152-3 8\\n3. Fischer, T.G.: Reinforcement learning in ﬁnancial markets-a survey. Technical\\nreport, FAU Discussion Papers in Economics (2018)\\n4. Hammer, P.: Autonomy through real-time learning and OpenNARS for applica-\\ntions. Temple University (2021)\\n5. Hammer, P., Lofthouse, T.: ‘OpenNARS for applications’: architecture and control.\\nIn: Goertzel, B., Panov, A.I., Potapov, A., Yampolskiy, R. (eds.) AGI 2020. LNCS\\n(LNAI), vol. 12177, pp. 193–204. Springer, Cham (2020). https://doi.org/10.1007/\\n978-3-030-52152-3 20\\n6. Hammer, P., Lofthouse, T., Fenoglio, E., Latapie, H., Wang, P.: A reasoning based\\nmodel for anomaly detection in the smart city domain. In: Arai, K., Kapoor, S.,\\nBhatia, R. (eds.) IntelliSys 2020. AISC, vol. 1251, pp. 144–159. Springer, Cham\\n(2021). https://doi.org/10.1007/978-3-030-55187-2 13',\n",
       "  '7. Hammer, P., Lofthouse, T., Wang, P.: The OpenNARS implementation of the non-\\naxiomatic reasoning system. In: Steunebrink, B., Wang, P., Goertzel, B. (eds.) AGI\\n-2016. LNCS (LNAI), vol. 9782, pp. 160–170. Springer, Cham (2016). https://doi.\\norg/10.1007/978-3-319-41649-6 16\\n8. Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup, D., Meger, D.: Deep\\nreinforcement learning that matters. In: Proceedings of the AAAI Conference on\\nArtiﬁcial Intelligence, vol. 32 (2018)\\n9. Kober, J., Bagnell, J.A., Peters, J.: Reinforcement learning in robotics: a survey.\\nInt. J. Robot. Res. 32(11), 1238–1274 (2013)\\n10. Silver, D., et al.: Mastering the game of go without human knowledge. Nature\\n550(7676), 354–359 (2017)\\n11. Sutton, R.S., Barto, A.G.: Reinforcement Learning: An Introduction. MIT Press,\\nCambridge (2018)\\n12. Wang, P.: Non-axiomatic reasoning system: exploring the essence of intelligence.\\nIndiana University (1995)',\n",
       "  '13. Wang, P.: Rigid Flexibility: The Logic of Intelligence, vol. 34. Springer, Dordrecht\\n(2006). https://doi.org/10.1007/1-4020-5045-3',\n",
       "  'An Analysis of ONA and Q-Learning Algorithms 31\\n14. Wang, P.: Insuﬃcient knowledge and resources-a biological constraint and its func-\\ntional implications. In: 2009 AAAI Fall Symposium Series (2009)\\n15. Wang, P.: Non-axiomatic logic (NAL) speciﬁcation. University of Camerino, Piazza\\nCavour 19(2010)\\n16. Wang, P.: Non-Axiomatic Logic: A Model of Intelligent Reasoning. World Scientiﬁc\\n(2013)\\n17. Watkins, C.J., Dayan, P.: Q-learning. Mach. Learn. 8(3), 279–292 (1992). https://\\ndoi.org/10.1007/BF00992698\\n18. Yu, C., Liu, J., Nemati, S., Yin, G.: Reinforcement learning in healthcare: a survey.\\nACM Comput. Surv. 55(1), 1–36 (2021)',\n",
       "  'On the Computation of Meaning,\\nLanguage Models and Incomprehensible\\nHorrors\\nMichael Timothy Bennett(B)\\nThe Australian National University, Canberra, Australia\\nmichael.bennett@anu.edu.au\\nhttp://www.michaeltimothybennett.com/\\nAbstract. We integrate foundational theories of meaning with a math-\\nematical formalism of artiﬁcial general intelligence (AGI) to oﬀer acomprehensive mechanistic explanation of meaning, communication, and\\nsymbol emergence. This synthesis holds signiﬁcance for both AGI and\\nbroader debates concerning the nature of language, as it uniﬁes pragmat-ics, logical truth conditional semantics, Peircean semiotics, and a com-\\nputable model of enactive cognition, addressing phenomena that have\\ntraditionally evaded mechanistic explanation. By examining the con-ditions under which a machine can generate meaningful utterances or\\ncomprehend human meaning, we establish that the current generation',\n",
       "  'of language models do not possess the same understanding of meaningas humans nor intend any meaning that we might attribute to their\\nresponses. To address this, we propose simulating human feelings and\\noptimising models to construct weak representations. Our ﬁndings shedlight on the relationship between meaning and intelligence, and how we\\ncan build machines that comprehend and intend meaning.\\nKeywords: meaning\\n·AGI·language\\n1 Introduction\\nLinguists and philosophers have oﬀered various accounts of the behaviour of\\nlanguage and the human mind. Computer scientists have posited mechanisms to\\nreplicate these variously described behaviours piecemeal. The former is a top-\\ndown approach, while the latter is bottom up. Unfortunately, it is diﬃcult toconnect the two. Large language models (LLMs) such as ChatGPT are a bottom\\nup attempt to capture the behaviour of written language, and are remarkably',\n",
       "  'good at giving human-like responses to questions. Yet it is unclear the extent towhich an LLM actually means what it says or understands what we mean. AGI\\nshould not just parrot what we expect but respond to what we mean, and mean\\nwhat it says. Yet how we would we know the diﬀerence? Computers representsyntax, and from correlations in syntax an LLM is supposed to glean meaning.\\nHowever, meaning is not well deﬁned. We need to connect top-down descriptions\\nof meaning to bottom-up computation. How might we compute meaning?\\nc⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2023\\nP. Hammer et al. (Eds.): AGI 2023, LNAI 13921, pp. 32–41, 2023.https://doi.org/10.1007/978-3-031-33469-6\\n_4',\n",
       "  'On the Computation of Meaning 33\\n1.1 Grice’s Foundational Theory of Meaning\\nGrice’s foundational theory of meaning [ 2] holds that meaning is what the\\nspeaker intends to convey to the listener. Grice gave an illustrative example,\\n[the speaker] αmeans mby uttering uiﬀαintends in uttering uthat\\n1. his audience come to believe m,\\n2. his audience recognize this intention [called m-intention], and\\n3. (1) occur on the basis of (2) [ 3].\\nThis is foundational because it speciﬁes the facts in virtue of which expressions\\nhave particular semantic properties (instead of those properties), and is illustra-\\ntive of our goal (to connect bottom up computation to top down description).\\n1.2 A Foundational Theory of Foundational Theories\\nWere we to accept that meaning is in virtue of m-intent1, then from what does\\nthat arise? M-intent should not be conﬂated with intent in general because it\\npertains to what one means by an expression, whereas intent more generally is',\n",
       "  'any goal in service of which decisions are made. The former stems from the latter[6], and so there exists a theory arguing that meaning exists in virtue of one’s\\nintent in the sense of goals. Grice’s theories are better established and widely\\naccepted with respect to meaning, but these theories are not mutually exclusiveand the depiction of meaning as in virtue of intent in general is a bridge we\\ncan use to connect Grice’s top down description to bottom-up computational\\nprocesses. This is because it explains intent in virtue of inductive inference,to argue that meaningful communication with an AI, or any organism, requires\\nsimilar feelings and experiences, in order to construct similar goals and “solutions\\nto tasks” [ 6] (an argument formed in relation to the Fermi Paradox [ 7]). This\\nexplanation was too vague to be of signiﬁcance for engineering. For example it\\nassumed a measure, “weakness”, which was not well deﬁned. However, weakness',\n",
       "  'iswell deﬁned in a more recent formalism of artiﬁcial general intelligence (AGI)\\n[1,8] and enactive cognition, so we will instead reformulate the theory using that\\nformalism, extending it to account for meaningful communication. We begin\\nwith cognition formalised using tasks. We then formalise an organism using\\ntasks to provide a novel account of preferences, symbol systems and meaningful\\ncommunication. We then describe how an organism might mean what we thinki tm e a n sb yw h a ti ts a y s ,o ri n f e rw h a tw em e a nb yw h a tw es a y .\\n2 Meaning, From the Top Down\\nIntent only exists in virtue of a task one is undertaking [ 6]. A task is what\\nwe get if we add context to intent, expressing what is relevant about both the\\n1We note that Grice later expanded upon the notion of m-intent [ 4,5], and that there\\nare other widely accepted descriptions of meaning (Russell, Frege, Searle, Davidson,\\nWittgenstein, Lewis, Kripke etc.), some of which we touch upon as part of our',\n",
       "  'formalism. However, paper length limits what we discuss.',\n",
       "  '34 M. T. Bennett\\nagent and the environment. A task can be used to formalise enactive cognition\\n[9], discarding notions of agent and environment in favour of a set of decision\\nproblems [ 1,6]. A task is something which is completed, like a goal, so intent\\nis formalised like a goal [ 10]. A goal is a set of criteria, and if those criteria\\nare satisﬁed, then it is satisﬁed and the task complete. To formalise meaningwe must avoid grounding problems [ 11]. As such these criteria are grounded by\\nrepresenting the environment, of which cognition is part, as a set of declarative\\nprograms [ 12] of which the universe is the interpreter [ 13]:\\nDeﬁnition 1 (environment)\\n– We assume a set Φwhose elements we call states , one of which we single out\\nas the present state .\\n–A declarative program is a function f:Φ→{true,false }, and we write P\\nfor the set of all declarative programs. By an objective truth about a state\\nφ, we mean a declarative program fsuch that f(φ)=true.',\n",
       "  'Deﬁnition 2 (implementable language)\\n–V={V⊂P:V is finite }is a set whose elements we call vocabularies ,\\none of which\\n2we single out as the vocabulary v.\\n–Lv={l⊆v:∃φ∈Φ(∀p∈l:p(φ)=true )}is a set whose elements we call\\nstatements .Lvfollows Φand v, and is called implementable language .\\n–l∈Lvistrueiﬀ the present state is φand∀p∈l:p(φ)=true.\\n–T h e extension of a statement a∈LvisZa={b∈Lv:a⊆b}.\\n–T h e extension of a set of statements A⊆LvisZA=⋃\\na∈AZa.\\n(Notation) Zwith a subscript is the extension of the subscript3.\\nA goal can now be expressed as a statement in an implementable language. An\\nimplementable language represents sensorimotor circuitry4with which cognition\\nis enacted. It is not natural language, but a dyadic system with exact meaning.Peircean semiosis [ 14] is integrated to explain natural language. Peirce deﬁned\\na symbol as a sign (E.G. the word “pain”), a referent (E.G. the experience of',\n",
       "  'pain), and an interpretant which links the two, “determining the eﬀect upon” theorganism. A goal arguably functions as an interpretant because it determines the\\neﬀect of a situation upon an organism that pursues it [ 6]. Rather than formulate\\na task and then rehash the argument that a task is a symbol, we’ll just formalisea symbol using the existing deﬁnition of a task [ 1, deﬁnition 3]:\\nDeﬁnition 3 ( v-task). F o rac h o s e n v, a task αis a triple ⟨S\\nα,Dα,Mα⟩,a n d\\nΓvis the set of all tasks given v. Give a task α:\\n2The vocabulary vwe single out represents the sensorimotor circuitry with which an\\norganism enacts cognition - their brain, body, local environment and so forth.\\n3e.g. Zsis the extension of s.\\n4Mind, body, local environment etc.',\n",
       "  'On the Computation of Meaning 35\\n–Sα⊂Lvis a set whose elements we call situations ofα.\\n–Sαhas the extension ZSα, whose elements we call decisions ofα.\\n–Dα={z∈ZSα:z is correct }is the set of all decisions which complete α.\\n–Mα={l∈Lv:ZSα∩Zl=Dα}whose elements we call models ofα.\\n(Notation) Ifω∈Γv, then we will use subscript ωto signify parts of ω, meaning\\none should assume ω=⟨Sω,Dω,Mω⟩even if that isn’t written.\\n(How a task is completed) Assume we’ve a v-taskωand a hypothesis h∈Lvs.t.\\n1. we are presented with a situation s∈Sω,a n d\\n2. we must select a decision z∈Zs∩Zh.\\n3. Ifz∈Dω, then zis correct and the task is complete. This occurs if h∈Mω.\\nDeﬁnition 4 (symbol). A task αis also a Peircean symbol:\\n–s∈Sαis a signofα.\\n–d∈Dαis the eﬀect of αupon one who perceives it. dmay be sensorimotor\\nactivity associated with perception, and thus a referent .\\n–m∈Mαis an interpretant linking signs toreferents .\\nTasks may be divided into narrower child tasks, or merged into parent tasks.',\n",
       "  'Deﬁnition 5 (child, parent and weakness). As y m b o l αis a child of ωif\\nSα⊂SωandDα⊆Dω. This is written α⊏ω.W ec a l l |Dα|the weakness of a\\nsymbol α, and a parent is weaker than its children.\\n2.1 Extending the Formalism\\nThe child and parent relation means a symbol is also a symbol system in that\\nit can be subdivided into child symbols [ 6]. With this in mind, we can deﬁne an\\norganism that derives symbols from its experiences, preferences and feelings.\\nDeﬁnition 6 (organism). An organism ois a quintuple ⟨vo,eo,so,n o,f o⟩,a n d\\nthe set of all such quintuples is Owhere:\\n–vois a vocabulary we single out as belonging to this organism5.\\n– We assume a vo-taskβwherein Sβis every situation in which ohas made a\\ndecision, and Dβcontains every such decision. Given the set Γvoof all tasks,\\neo={ω∈Γvo:ω⊏β}is a set whose members we call experiences .\\n–A symbol system so={α∈Γvo:there exists ω ∈eowhere M α∩Mω̸=∅}\\nis a set whose members we call symbols .sois the set of every task to which',\n",
       "  'it is possible to generalise (see [ 1, deﬁnition 5]) from an element of eo.\\n–no:so→ Nis a function we call preferences .\\n–fo:so→fois a function, and fo⊂Lvoa set whose elements we call feelings ,\\nbeing the reward, qualia etc, from which preferences arise6.\\n5The corresponding Lvois all sensorimotor activity in which the organism may engage.\\n6Note that this assumes qualia, preferences and so forth are part of physical reality,\\nwhich means they are sets of declarative programs.',\n",
       "  '36 M. T. Bennett\\nEach symbol in soshares an interpretant at least one experience7.T h i si ss o\\nfeelings foascribed to symbols can be grounded in experience. Humans are\\ngiven impetus by a complex balance of feelings (reward signals, qualia etc.).\\nIt is arguable that feelings eventually determine all value judgements [ 10]. As\\nHume pointed out, one cannot derive a statement of what ought to be froma statement of what is. Feelings are an ought from which one may derive all\\nother oughts. If meaning is about intent, then the impetus that gives rise to that\\nintent is an intrinsic part of all meaning [ 7]. Intent is a goal. A goal is statement\\nof what ought to be that one tries to make into a description of what is, by\\naltering the world to ﬁt with ought to be. We assume feelings are consequence\\nof natural selection, and so explain meaning in virtue of a mechanistic process.Eachl∈Lrepresents sensorimotor activity, which from a materialist perspective\\nincludes feelings. Thus, f',\n",
       "  'ois a function from symbols to sensorimotor activity.\\nStatements and symbols “mean something” to the organism if the organism can\\nascribe feelings to them. As every symbol in socontains an interpretant which is\\npart of the organism’s experience, the organism can ascribe feelings to all symbolson the basis of that experience. If one is not concerned with qualia [ 16,17], then\\nfeelings may be simulated with “reward” functions. However, to simulate feelings\\nthat result in human-like behaviour is a more diﬃcult proposition. Rather thantrying to describe human-like feelings, we simplify our analysis by assuming the\\npreferences [ 18]n\\nowhich are determined by experience of feelings.\\n2.2 Interpretation\\nThesituation at hand s∈Lvois a statement oexperiences as a sign and then\\ninterprets using α∈sos.t.s∈Sα, to decide d∈Zs∩ZMα.\\nDeﬁnition 7 (interpretation). Interpretation is a sequence of steps:\\n1. The situation at hand s∈Lvosigniﬁes as y m b o l α∈soifs∈Sα.\\n2. ss',\n",
       "  'o={α∈so:s∈Sα}is the set of all symbols which ssigniﬁes.\\n3. If ss\\no̸=∅thensmeans something to the organism in the sense that there\\nare feelings which can be ascribed to symbols in ss\\no.\\n4. Ifsmeans something, then ousesα∈arg max\\nω∈ss\\nono(ω)to interpret s.\\n5. The interpretation is a decision d∈Zs∩ZMα8.\\n3 Communication of Meaning\\nWe develop our explanation in four parts. First, we deﬁne exactly what it means\\nfor an organism to aﬀect and be aﬀected by others. Second, we examine how one\\n7A symbol system is every task to which one may generalise from one’s experiences.\\nOnly ﬁnitely many symbols may be entertained. In claiming our formalism pertains\\nto meaning in natural language we are rejecting arguments, such as those of Blockand Fodor [ 15], that a human can entertain an inﬁnity of propositions (because time\\nand memory are assumed to be ﬁnite, which is why v\\nois ﬁnite).\\n8How an organism responds to a sign that means nothing is beyond this paper’s scope.',\n",
       "  'On the Computation of Meaning 37\\norganism may anticipate the behaviour (by inferring the end it serves) of another\\nor order to change how they are aﬀected. Third, we examine how said organismmay, having anticipated the behaviour of the other, intervene to manipulate the\\nother’s behaviour to their beneﬁt (so that the now latter aﬀects the former in a\\nmore positive way). And ﬁnally, we examine what happens when each organismis attempting to manipulate the another. Each anticipates the other’s manip-\\nulation, because each anticipates the other’s behaviour by inferring its intent.\\nAn organism can then attempt to deceive the other organism (continue themanipulative approach), or attempt to co-operate (communicate in good faith),\\na choice resembling an iterated prisoner’s dilemma. We assume organisms make\\ndecisions based upon preferences, but preferences are not arbitrary. Feelings andthus preferences exist in virtue of natural selection, which to some extent must',\n",
       "  'favour rational behaviour (to the extent that selection is signiﬁcantly impacted).\\nIn this might be understood as alignment, to use AI safety terms. One’s feelings\\nare the result of alignment by genetic algorithm, and one’s preferences are the\\nresult of reinforcement learning using those feelings (to determine reward). Thuswe assume preferences are a balance of what is rational, and what is tolerably\\nirrational, given the pressures of natural selection. We call this balance reason-\\nably performant . The speciﬁcs of inductive inference are beyond the scope of\\nthis paper, however deﬁnitions and formal proofs pertaining to inductive infer-\\nence from child to parent tasks are included in the appendix [ 1]. The necessary\\ninductive capabilities are assumed with being reasonably performant.\\n3.1 Ascribing Intent\\nDeﬁnition 8 (aﬀect). To aﬀect an organism ois to cause it to make a diﬀerent\\ndecision than it otherwise would have. kaﬀects oifowould have made a decision',\n",
       "  'd, but as a result of a decision cmade by k,omakes decision g̸=d.\\nLet kand obe organisms. If kaﬀects o, and assuming v\\nois suﬃcient to allow\\noto distinguish when it is aﬀected by kfrom when it is not (meaning all else\\nbeing equal k’s interventions are distinguishable by the presence of an identity\\n(see appendices), then there exists experience ζk\\no∈eosuch that d∈Dζkoifo\\nis aﬀected by k.ζk\\nois an ostensive deﬁnition [ 19]o f k’s intent (meaning it is a\\nchild task from which we may infer the parent representing k’s most likely intent\\nand thus future behaviour) [ 6]. In the absence of more information, the symbol\\nmost likely to represent k′sintent is the weakest [ 6], meaning α∈sos.t.|Zα|is\\nmaximised. However, because oassumes khas similar feelings and preferences\\n[6,10]9nois an approximation of what kwill do. Accordingly the symbol most\\nlikely to represent k’s intent would be the “weakest” of goals preferred by owhich,',\n",
       "  'if pursued by k, would explain why khas aﬀected oas it has. This is γk\\nos.t.\\nγk\\no∈arg max\\nα∈K|Zα|s.t. K= arg max\\nα∈Γk\\nono(α)andΓk\\no={ω∈Γvo:Mζk\\no∩Mω̸=∅}\\n9Members of a species tend to have similar feelings, experiences and thus preferences.',\n",
       "  '38 M. T. Bennett\\n3.2 From Manipulation to Meaningful Communication\\nWe’ve explained inference of intent in counterfactual terms, answering “if places\\nwere exchanged, what would cause oto act like k?”. Intent here is “what is k\\ntrying to achieve by aﬀecting o”, rather than just “what is ktrying to achieve”.\\nManipulation: Because it is reasonably performant, oinfers the intent of an\\norganism kthat aﬀects o, in order to plan ahead and ensure its own needs will be\\nmet. However ocan go a step further. It can also attempt to inﬂuence what kwill\\ndo. If being reasonably performant requires oinfer k’s intent because kaﬀects o,\\nthen it may also require oaﬀect kto the extent that doing so will beneﬁt o.\\nCommunication: If both oand kare reasonably performant, each may attempt\\nto manipulate the other. Ascribing intent to one another’s behaviour in order tomanipulate, each must anticipate the other’s manipulative intent. Subsequently',\n",
       "  'each organism must go yet another step further and account for how its own\\nmanipulative intent will be perceived by the other. As in a sort of iterated pris-oner’s dilemma, the rational choice may then be to co-operate. Because each\\nsymbol represents a goal it deﬁnes a limited context for co-operation; so two\\norganisms might simultaneously co-operate in pursuit of one goal while compet-ing in pursuit of another (E.G. two dogs may co-operate to hunt while com-\\npeting for a mate). If there is suﬃcient proﬁt in aﬀecting another’s behaviour,\\nthen knowing one’s own intent is perceived by that other and that the otherwill change its behaviour in response to one’s changed intent, it makes sense\\nto actually change one’s own intent in order to aﬀect the other. This bears out\\nexperimentally in reinforcement learning with extended environments [ 20]. The\\nrational choice may then be to haveco-operative intent, assuming kcan per-',\n",
       "  'ceive o’s intent correctly, and that kwill reciprocate in kind. For a population\\nof reasonably performant organisms, induction (see [ 1]) with co-operative intent\\nwould favour symbols that mean (functionally) similar things to all members of\\nthe population. Repeated interactions would give rise to signalling conventionswe might call language.\\nMeaning: Let us re-frame these ideas using the example from the introduction.\\nWe’ll say two symbols α∈s\\nkandω∈soare roughly equivalent (written α≈ω)\\nto mean feelings, experiences and thus preferences associated with a symbol are\\nin some sense the same for two organisms (meaning if α≈ωthenfk(α)≈fo(ω)\\netc.). In other words we’re suggesting it must be possible to measure the similar-ity between symbols in terms of feelings, experiences and thus preferences, and\\nso we can assert a threshold beyond which two symbols are roughly equivalent.\\nkmeans α∈s\\nkby deciding uand aﬀecting oiﬀkintends in deciding u:',\n",
       "  '1. that ointerpets the situation at hand with ω∈sos.t.ω≈α,\\n2.orecognize this intention, for example by predicting it according to\\nγk\\no∈arg max\\nα∈K|Zα|s.t. K= arg max\\nα∈Γk\\nono(α),Γk\\no={ω∈Γvo:Mζk\\no∩Mω̸=∅}',\n",
       "  'On the Computation of Meaning 39\\n3. and (1) occur on the basis of (2), because kintends to co-operate and\\nso will interpret the situation at hand using what it believes of o’s intent.\\nThe above pertains to co-operation. To comprehend meaning:\\n1. Organisms must be able to aﬀect one another .\\n2. Organisms must have similar feelings ,a n d\\n3. similar experiences ,s o soand skcontain roughly equivalent symbols.\\n4. Similar preferences then inform the correct inference of intent.\\n5. Finally, all this assumes organisms are reasonably performant .\\n4 Talking to a Machine\\nAn LLM is trained to mimic human preferences. However, an LLM is not given\\nimpetus by feelings, and so cannot entertain roughly equivalent symbols. This\\nis not to say we cannot reverse engineer the complex balance of human-likefeelings, merely that we have not. If an LLM has impetus, it is to be found in\\nour prompts. It is reminiscent of a mirror test, which is a means of determining',\n",
       "  'whether animals are self aware. For example, a cat seeing itself in the mirrormay attack its reﬂection, not realising what it sees is itself. In an LLM we face a\\nmirror test of our own, but instead of light it reﬂects our own written language\\nback at us. We ascribe motives and feelings to that language because we haveevolved to infer the intent of organisms compelled by feelings [ 6]. An LLM hijacks\\nwhat we use to understand one another (that we assume others are motivated by\\nsimilar feelings [ 10]). We’ve a history of ascribing feelings and agency to things\\npossessed of neither. In the 1970s, a chatbot named ELIZA made headlines as\\nits users attributed feelings and motives to its words [ 21]. Like ELIZA, today’s\\nLLMs not only do not mean what we think they mean by what they say, but do\\nnot mean anything at all. This is not an indictment of LLMs trained to mimic',\n",
       "  'human preferences. The meaning we ascribe to their behaviour can be useful,even if that behaviour was not intended to mean anything.\\nThe Hall of Mirrors: Even if we approximate human feelings, an LLM like\\nChatGPT is not reasonably performant. It is maladaptive, requiring an abun-\\ndance of training data. This may be because training does not optimise for a\\nweak representation, but settles for any function ﬁtting the data\\n10[6]. Returning\\nto mirror analogies, imagine a hall of mirrors reﬂecting an object from diﬀerent\\nangles. A weak or simple representation would be one symbol α∈sorepresent-\\ning the object, which is then interpreted from the perspectives a,b,c,d ∈Sαof\\neach mirror. A needlessly convoluted representation of the same would instead\\ninterpret a,b,c anddusing diﬀerent symbols. These would be α’s children\\nω,γ,δ,σ ⊏αsuch that a∈Sω,b∈Sγ,c∈Sδ,d∈Sσ. This latter represen-\\ntation fails to exploit what is common between perspectives, which might allow',\n",
       "  'it to generalise [ 6] to new perspectives. That an LLM may not learn suﬃciently\\n10Albeit with some preference for simplicity imparted by regularisation.',\n",
       "  '40 M. T. Bennett\\nweak representations seems consistent with their ﬂaws. One well documented\\nexample of this is how an LLM may convincingly mimic yet fail to understandarithmetic [ 22], but such ﬂaws may more subtly manifest elsewhere. For exam-\\nple, when we queried Bing Chat (on the 2\\nndof February 2023 [ 1, p. 11]) with\\nthe name and location of a relatively unknown individual who had several pro-fessions and hobbies mentioned on diﬀerent sites, Bing concluded that diﬀerent\\npeople with this name lived in the area, each one having a diﬀerent hobby or\\nprofession.\\nIncomprehensibility: If we are to build machines that mean what we think\\nthey mean by what they say, then we must emulate human feelings and expe-\\nriences. It is interesting to consider where this may lead. If we do not get the\\nbalance of feelings quite right, we might create an organism that means what',\n",
       "  'it says, but whose meanings may be partially or utterly incomprehensible to usbecause the resulting preferences are unaligned with ours. In the introduction\\nwe mentioned ideas on which this paper was founded were used to relate the\\nFermi paradox to control of and communication with an AGI [ 7]. We can extend\\nthat notion. Assume we are aﬀected by an organism. If the events befalling us\\nare set in motion by preferences entirely unlike our own, then we would fail to\\nascribe the correct intent to the organism. We may fail entirely to realise thereis an organism, or may ascribe many diﬀerent intents as in the hall of mirrors\\nanalogy. Furthermore, v\\nodetermines what can or cannot be comprehended by\\nan organism (see appendices). vomay contain nothing akin to the contents of\\nvk,m a k i n g oincapable of representing and thus comprehending k’s intent.\\nConclusion: We have extended a formalism of artiﬁcial general intelligence,',\n",
       "  'connecting bottom up computation to top down notions of meaning. This is\\nsigniﬁcant not just to AGI but to wider debates surrounding language, meaning\\nand the linguistic turn. While we focused on Gricean notions of meaning due topublication constraints, the formalism is by no means limited to that. For exam-\\nple, the logical truth conditional meaning of statements is in their extension. We\\nhave described the process by which meaningful communication can take placeand the prerequisites thereof. We conclude that human-like feelings and weak\\nrepresentations should give us systems that comprehend and intend meaning.\\nAcknowledgement. Appendices available on GitHub [ 1], supported by JST\\n(JPMJMS2033).\\nReferences\\n1. Bennett, M.T.: Technical Appendices. Version 1.2.1 (2023). https://doi.org/10.\\n5281/zenodo.7641742 .github.com/ViscousLemming/Technical-Appendices\\n2. Grice, P.: Meaning. Philos. Rev. 66(3), 377 (1957)',\n",
       "  '3. Speaks, J.: Theories of meaning. In: The Stanford Encyclopedia of Philosophy.\\nSpring 2021. Stanford University (2021)',\n",
       "  'On the Computation of Meaning 41\\n4. Grice, P.: Utterers meaning and intention. Philos. Rev. 78(2), 147–177 (1969)\\n5. Grice, H.P.: Studies in the Way of Words. Harvard University Press, Cambridge\\n(2007)\\n6. Bennett, M.T.: Symbol emergence and the solutions to any task. In: Goertzel,\\nB., Iklé, M., Potapov, A. (eds.) AGI 2021. LNCS (LNAI), vol. 13154, pp. 30–40.\\nSpringer, Cham (2022). https://doi.org/10.1007/978-3-030-93758-4_4\\n7. Bennett, M.T.: Compression, the fermi paradox and artiﬁcial super-intelligence.\\nIn: Goertzel, B., Iklé, M., Potapov, A. (eds.) AGI 2021. LNCS (LNAI), vol. 13154,\\npp. 41–44. Springer, Cham (2022). https://doi.org/10.1007/978-3-030-93758-4_5\\n8. Bennett, M.T.: Computable artiﬁcial general intelligence (2022). https://arxiv.\\norg/abs/2205.10513\\n9. Ward, D., Silverman, D., Villalobos, M.: Introduction: the varieties of enactivism.\\nTopoi 36(3), 365–375 (2017). https://doi.org/10.1007/s11245-017-9484-6',\n",
       "  '10. Bennett, M.T., Maruyama, Y.: Philosophical speciﬁcation of empathetic ethical\\nartiﬁcial intelligence. IEEE Trans. Cogn. Dev. Syst. 14(2), 292–300 (2022)\\n11. Harnad, S.: The symbol grounding problem. Phys. D Nonlinear Phenom. 42(1),\\n335–346 (1990)\\n12. Howard, W.A.: The formulae-as-types notion of construction. Dedicated to H.B.\\nCurry: Essays on Combinatory Logic, Lambda Calculus and Formalism. Ed. by J.\\nSeldin and J. Hindley, pp. 479–490. Academic Press, Cambridge (1980)\\n13. Piccinini, G., Maley, C.: Computation in physical systems. In: The Stanford Ency-\\nclopedia of Philosophy. Summer. Stanford University (2021)\\n14. Atkin, A.: Peirces theory of signs. In: The Stanford Encyclopedia of Philosophy.\\nSpring. Metaphysics Research Lab, Stanford University (2023)\\n15. Block, N., Fodor, J.: What psychological states are not. Philos. Rev. 81(2), 159–181\\n(1972)\\n16. Chalmers, D.: Facing up to the problem of consciousness. J. Conscious. Stud. 2(3),\\n200–219 (1995)',\n",
       "  '17. Boltuc, P.: The engineering thesis in machine consciousness. Techné Res. Philos.\\nTechnol. 16(2), 187–207 (2012)\\n18. Alexander, S.A.: The Archimedean trap: why traditional reinforcement learning\\nwill probably not yield AGI. J. Artif. Gen. Intell. 11(1), 70–85 (2020)\\n19. Gupta, A.: Deﬁnitions. In: The Stanford Encyclopedia of Philosophy. Winter 2021.\\nStanford University (2021)\\n20. Alexander, S.A., et al.: Extending environments to measure self-reﬂection in rein-\\nforcement learning. J. Artif. Gen. Intell. 13(1), 1–24 (2022)\\n21. Weizenbaum, J.: Computer Power and Human Reason: From Judgment to Calcu-\\nlation. W. H. Freeman & Co. (1976)\\n22. Floridi, L., Chiriatti, M.: GPT-3: its nature, scope, limits, and consequences. Minds\\nMach. 30, 681–694 (2020). https://doi.org/10.1007/s11023-020-09548-1',\n",
       "  'The Optimal Choice of Hypothesis\\nIs the Weakest, Not the Shortest\\nMichael Timothy Bennett(B)\\nThe Australian National University, Canberra, Australia\\nmichael.bennett@anu.edu.au\\nhttp://www.michaeltimothybennett.com/\\nAbstract. IfAand Bare sets such that A⊂B, generalisation may\\nbe understood as the inference from Aof a hypothesis suﬃcient to con-\\nstruct B. One might infer any number of hypotheses from A, yet only\\nsome of those may generalise to B. How can one know which are likely\\nto generalise? One strategy is to choose the shortest, equating the ability\\nto compress information with the ability to generalise (a “proxy for intel-ligence”). We examine this in the context of a mathematical formalism\\nof enactive cognition. We show that compression is neither necessary nor\\nsuﬃcient to maximise performance (measured in terms of the probabilityof a hypothesis generalising). We formulate a proxy unrelated to length\\nor simplicity, called weakness. We show that if tasks are uniformly dis-',\n",
       "  'tributed, then there is no choice of proxy that performs at least as wellas weakness maximisation in all tasks while performing strictly better\\nin at least one. In experiments comparing maximum weakness and min-\\nimum description length in the context of binary arithmetic, the formergeneralised at between 1.1and 5times the rate of the latter. We argue\\nthis demonstrates that weakness is a far better proxy, and explains why\\nDeepmind’s Apperception Engine is able to generalise eﬀectively.\\nKeywords: simplicity\\n·induction ·artiﬁcial general intelligence\\n1 Introduction\\nIfAandBare sets such that A⊂B, generalisation may be understood as\\nthe inference from Aof a hypothesis suﬃcient to construct B. One might infer\\nany number of hypotheses from A, yet only some of those may generalise to B.\\nHow can one know which are likely to generalise? According to Ockham’s Razor,\\nthe simpler of two explanations is the more likely [ 2]. Simplicity is not itself a',\n",
       "  'measurable property, so the minimum description length principle [ 3] relates sim-\\nplicity to length. Shorter representations are considered to be simpler, and tend\\nto generalise more eﬀectively. This is often applied in the context of induction\\nby comparing the length of programs that explain what is observed (to chose theshortest, all else being equal). The ability to identify shorter representations is\\ncompression, and the ability to generalise is arguably intelligence [ 4]. Hence the\\nc⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2023\\nP. Hammer et al. (Eds.): AGI 2023, LNAI 13921, pp. 42–51, 2023.https://doi.org/10.1007/978-3-031-33469-6\\n_5',\n",
       "  'The Optimal Choice of Hypothesis Is the Weakest, Not the Shortest 43\\nability to compress information is often portrayed as a proxy for intelligence [ 5],\\neven serving as the foundation [ 6–8] of the theoretical super-intelligence AIXI [ 9].\\nThat compression is a good proxy seems to have gone unchallenged. The optimal\\nchoice of hypothesis is widely considered to be the shortest. We show that it is\\nnot1. We present an alternative, unrelated to description length, called weakness.\\nWe prove that to maximise the probability that one’s hypotheses generalise, it\\nis necessary and suﬃcient to infer the weakest valid hypotheses possible2.\\n2 Background Deﬁnitions\\nTo do so, we employ a formalism of enactive cognition [ 1,10,11], in which sets\\nof declarative programs are related to one another in such a way as to form a\\nlattice. This unusual representation is necessary to ensure that both the weakness\\nand description length of a hypothesis are well deﬁned3. This formalism can be',\n",
       "  'understood in three steps.\\n1. The environment is represented as a set of declarative programs.\\n2. A ﬁnite subset of the environment is used to deﬁne a language with which to\\nwrite statements that behave as logical formulae.\\n3. Finally, induction is formalised in terms of tasks made up of these statements.\\nDeﬁnition 1 (environment)\\n– We assume a set Φwhose elements we call states , one of which we single out\\nas the present state4.\\n–A declarative program is a function f:Φ→{true,false }, and we write P\\nfor the set of all declarative programs. By an objective truth about a state\\nφ, we mean a declarative program fsuch that f(φ)=true.\\nDeﬁnition 2 (implementable language)\\n–V={V⊂P:V is finite }is a set whose elements we call vocabularies ,o n e\\nof which we single out as the vocabulary vfor an implementable language.\\n–Lv={l⊆v:∃φ∈Φ(∀p∈l:p(φ)=true )}is a set whose elements we\\ncall statements5.Lvfollows from Φand v.W ec a l l Lvanimplementable\\nlanguage .',\n",
       "  '1This proof is conditional upon certain assumptions regarding the nature of cognition\\nas enactive, and a formalism thereof.\\n2Assuming tasks are uniformly distributed, and weakness is well deﬁned.\\n3An example of how one might translate propositional logic into this representationis given at the end of this paper. It is worth noting that this representation of\\nlogical formulae addresses the symbol grounding problem [ 12], and was speciﬁcally\\nconstructed to address subjective performance claims in the context of AIXI [ 13].\\n4Each state is just reality from the perspective of a point along one or more dimen-\\nsions. States of reality must be separated by something, or there would be only onestate of reality. For example two diﬀerent states of reality may be reality from the\\nperspective of two diﬀerent points in time, or in space and so on.\\n5Statements are the logical formulae about which we will reason.',\n",
       "  '44 M. T. Bennett\\n–l∈Lvistrueiﬀ the present state is φand∀p∈l:p(φ)=true.\\n–T h e extension of a statement a∈LvisZa={b∈Lv:a⊆b}.\\n–T h e extension of a set of statements A⊆LvisZA=⋃\\na∈AZa.\\n(Notation). Zwith a subscript is the extension of the subscript6.L o w e rc a s e\\nletters represent statements, and upper case represent sets of statements.\\nDeﬁnition 3 ( v-task). F o rac h o s e n v, a task αis⟨Sα,Dα,Mα⟩where:\\n–Sα⊂Lvis a set whose elements we call situations ofα.\\n–Sαhas the extension ZSα, whose elements we call decisions ofα.\\n–Dα={z∈ZSα:z is correct }is the set of all decisions which complete α.\\n–Mα={l∈Lv:ZSα∩Zl=Dα}whose elements we call models ofα.\\nΓvis the set of all tasks7.\\n(Notation). Ifω∈Γv, then we will use subscript ωto signify parts of ω, meaning\\none should assume ω=⟨Sω,Dω,Mω⟩even if that isn’t written.\\n(How a task is completed). Assume we’ve a v-taskωand a hypothesis h∈Lv\\ns.t.\\n1. we are presented with a situation s∈Sω,a n d\\n2. we must select a decision z∈Zs∩Zh.',\n",
       "  '3. Ifz∈Dω, then zis correct and the task is complete. This occurs if h∈Mω.\\n3 Formalising Induction\\nDeﬁnition 4 (probability). We assume a uniform distribution over Γv.\\nDeﬁnition 5 (generalisation). A statement lgeneralises to α∈Γviﬀl∈Mα.\\nWe say lgeneralises from αtov-taskωif we ﬁrst obtain lfromMαand then\\nﬁnd it generalises to ω.\\nDeﬁnition 6 (child and parent). Av-taskαis a child of v-taskωifSα⊂Sω\\nandDα⊆Dω. This is written as α⊏ω.I fα⊏ωthenωis then a parent of α.\\nA proxy is meant to estimate one thing by measuring another. In this case,\\nif intelligence is the ability to generalise [ 4,10], then a greater proxy value is\\nmeant to indicate that a statement is more likely to generalise. Not all proxiesare eﬀective (most will be useless). We focus on two in particular.\\n6e.g. Zsis the extension of s.\\n7For example, we might represent chess as a supervised learning problem where s∈Sα\\nis the state of a chessboard, z∈Zsis a sequence of moves by two players that begins',\n",
       "  'ins,a n d d∈Dα∩Zsis such a sequence of moves that terminates in victory for one\\nplayer in particular (the one undertaking the task).',\n",
       "  'The Optimal Choice of Hypothesis Is the Weakest, Not the Shortest 45\\nDeﬁnition 7 (proxy for intelligence). A proxy is a function parameterized\\nby a choice of vsuch that qv:Lv→ N. The set of all proxies is Q.\\n(Weakness). The weakness of a statement lis the cardinality of its extension\\n|Zl|. There exists qv∈Qsuch that qv(l)=|Zl|.\\n(Description length). The description length of a statement lis its cardinality |l|.\\nLonger logical formulae are considered less likely to generalise [ 3], and a proxy\\nis something to be maximised, so description length as a proxy is qv∈Qsuch\\nthatqv(l)=1\\n|l|.\\nA child task may serve as an ostensive deﬁnition [ 14] of its parent, meaning\\none can generalise from child to parent.\\nDeﬁnition 8 (induction). αandωare v-tasks such that α⊏ω. Assume we\\nare given a proxy qv∈Q, the complete deﬁnition of αand the knowledge that\\nα⊏ω. We are not given the deﬁnition of ω. The process of induction would\\nproceed as follows:\\n1. Obtain a hypothesis by computing a model h∈arg max',\n",
       "  'm∈Mαqv(m).\\n2. If h∈Mω, then we have generalised from αtoω.\\n4 Proofs\\nProposition 1 (suﬃciency). Weakness is a proxy suﬃcient to maximise the\\nprobability that induction generalises from αtoω.\\nProof: You’re given the deﬁnition of v-taskαfrom which you infer a hypothesis\\nh∈Mα.v-taskωis a parent of αto which we wish to generalise:\\n1. The set of statements which might be decisions addressing situations in Sω\\nand not Sα,i sZSα={l∈Lv:l/∈ZSα}.\\n2. For any given h∈Mα, the extension Zhofhis the set of decisions himplies.\\nThe subset of Zhwhich fall outside the scope of what is required for the\\nknown task αisZSα∩Zh(because ZSαis the set of all decisions we might\\nmake when attempting α, and so the set of all decisions that can’t be made\\nwhen undertaking αisZSαbecause those decisions occur in situations that\\naren’t part of Sα).\\n3.|ZSα∩Zh|increases monotonically with |Zh|, because ∀z∈Zm:z/∈ZSα→\\nz∈ZSα.\\n4.2|ZSα|is the number of tasks which fall outside of what it is necessary for a',\n",
       "  'model of αto generalise to (this is just the powerset of ZSαdeﬁned in step\\n2), and 2|ZSα∩Zh|is the number of those tasks to which a given h∈Mαdoes\\ngeneralise.',\n",
       "  '46 M. T. Bennett\\n5. Therefore the probability that a given model h∈Mαgeneralises to the\\nunknown parent task ωis\\np(h∈Mω|h∈Mα,α⊏ω)=2|ZSα∩Zh|\\n2|ZSα|\\np(h∈Mω|h∈Mα,α⊏ω)is maximised when |Zh|is maximised.\\nProposition 2 (necessity). To maximise the probability that induction gener-\\nalises from αtoω, it is necessary to use weakness as a proxy, or a function\\nthereof8.\\nProof: Letαandωbe deﬁned exactly as they were in the proof of Proposition 1.\\n1. If h∈MαandZSω∩Zh=Dω,t h e ni tm u s tb eh ec a s et h a t Dω⊆Zh.\\n2. If|Zh|<|Dω|then generalisation cannot occur, because that would mean\\nthatDω̸⊆Zh.\\n3. Therefore generalisation is only possible if |Zm|≥|Dω|, meaning a suﬃciently\\nweak hypothesis is necessary to generalise from child to parent.\\n4. The probability that |Zm|≥|Dω|is maximised when |Zm|is maximised.\\nTherefore to maximise the probability induction results in generalisation, itis necessary to select the weakest hypothesis.',\n",
       "  'To select the weakest hypothesis, it is necessary to use weakness (or a function\\nthereof) as a proxy.\\nRemark 1 (prior). The above describes inference from a child to a parent. How-\\never, it follows that increasing the weakness of a statement increases the proba-\\nbility that it will generalise to any task (not just a parent of some given child). Astasks are uniformly distributed, every statement in L\\nvis a model to one or more\\ntasks, and the number of tasks to which each statement l∈Lvgeneralises is 2|Zl|.\\nHence the probability of generalisation9toωisp(h∈Mω|h∈Lv)=2|Zh|\\n2|Lv|.\\nThis assigns a probability to every statement l∈Lvgiven an implementable\\nlanguage. It is a probability distribution in the sense that the probability of\\nmutually exclusive statements sums to one10. This prior may be considered uni-\\nversal in the very limited sense that it assigns a probability to every conceivablehypothesis (where what is conceivable depends upon the implementable lan-',\n",
       "  'guage) absent any parameters or speciﬁc assumptions about the task as with\\nAIXI’s intelligence order relation [ 9, def. 5.14 pp. 147]\\n11. As the vocabulary vis\\nﬁnite, Lvmust also be ﬁnite, and so pis computable.\\n8For example we might use weakness multiplied by a constant to the same eﬀect.\\n92|Zh|\\n2|Lv|is maximised when h=∅, because the optimal hypothesis given no information\\nis to assume nothing (you’ve no sequence to predict, so why make assertions that\\nmight contradict the environment?).\\n10Two statements aand bare mutually exclusive if a̸∈Zband b̸∈Za, which we’ll write\\nasµ(a, b ). Given x∈Lv, the set of all mutually exclusive statements is a set Kx⊂Lv\\nsuch that x∈Kxand∀a, b∈Kx:µ(a, b ). It follows that ∀x∈Lv,∑\\nb∈Kxp(b)=1.\\n11We acknowledge that some may object to the term universal, because vis ﬁnite.',\n",
       "  'The Optimal Choice of Hypothesis Is the Weakest, Not the Shortest 47\\nWe have shown that, if tasks are uniformly distributed, then weakness is a neces-\\nsary and suﬃcient proxy to maximise the probability that induction generalises.It is important to note that another proxy may perform better given cherry-\\npicked combinations of child and parent task for which that proxy is suitable.\\nHowever, such a proxy would necessarily perform worse given the uniform dis-tribution of all tasks. Can the same be said of description length?\\nProposition 3. Description length is neither a necessary nor suﬃcient proxy\\nfor the purposes of maximising the probability that induction generalises.\\nProof: In Propositions 1and2we proved that weakness is a necessary and suﬃ-\\ncient choice of proxy to maximise the probability of generalisation. It follows that\\neither maximising\\n1\\n|m|(minimising description length) maximises |Zm|(weak-\\nness), or minimisation of description length is unnecessary to maximise the prob-',\n",
       "  'ability of generalisation. Assume the former, and we’ll construct a counterexam-ple with v={a,b,c,d,e,f,g,h,j,k,z }s.t.L\\nv={{a,b,c,d,j,k,z },{e,b,c,d,k },\\n{a,f,c,d,j },{e,b,g,d,j,k,z },{a,f,c,h,j,k },{e,f,g,h,j,k }}and a task α\\nwhere\\n–Sα={{a,b},{e,b}}\\n–Dα={{a,b,c,d,j,k,z },{e,b,g,d,j,k,z }}\\n–Mα={{z},{j,k}}\\nWeakness as a proxy selects {j,k}, while description length as a proxy selects\\n{z}. This demonstrates the minimising description length does not necessarily\\nmaximise weakness, and maximising weakness does not minimise description\\nlength. As weakness is necessary and suﬃcient to maximise the probability of\\ngeneralisation, it follows that minimising description length is neither.\\n5 Experiments\\nIncluded with this paper is a Python script to perform two experiments usingPyTorch with CUDA, SymPy and A\\n∗[15–18] (see technical appendix for details).\\nIn these two experiments, a toy program computes models to 8-bit string pre-',\n",
       "  'diction tasks (binary addition and multiplication). The purpose of these experi-\\nments was to compare weakness and description length as proxies.\\n5.1 Setup\\nTo specify tasks with which the experiments would be conducted, we needed\\na vocabulary vwith which to describe simple 8-bit string prediction problems.\\nThere were 256 states in Φ, one for every possible 8-bit string. The possible\\nstatements were then all the expressions regarding those 8bits that could be\\nwritten in propositional logic (the simple connectives ¬,∧and∨needed to\\nperform binary arithmetic – a written example of how propositional logic can beused in to specify vis also included in the appendix). In other words, for each',\n",
       "  '48 M. T. Bennett\\nstatement in Lvthere existed an equivalent expression in propositional logic.\\nFor eﬃciency, these statements were implemented as either PyTorch tensors orSymPy expressions in diﬀerent parts of the program, and converted back and\\nforth as needed (basic set and logical operations on these propositional tensor\\nrepresentations were implemented for the same reason). A v-task was speciﬁed\\nby choosing D\\nn⊂Lvsuch that all d∈Dnconformed to the rules of either binary\\naddition or multiplication with 4-bits of input, followed by 4-bits of output.\\n5.2 Trials\\nEach experiment had parameters were “operation” and “number_of_trials”. For\\neach trial the number |Dk|of examples ranged from 4to14. A trial had 2phases.\\nTraining Phase\\n1. A task n( r e f e r r e dt oi nc o d ea s Tn) was generated:\\n(a) First, every possible 4-bit input for the chosen binary operation was used\\nto generate an 8-bit string. These 16 strings then formed Dn.',\n",
       "  '(b) A bit between 0 and 7 was then chosen, and Sncreated by cloning Dn\\nand deleting the chosen bit from every string ( Sncontained 16 diﬀerent\\n7-bit strings, each of which was a sub-string of an element of Dn).\\n2. A child-task k=⟨Sk,Dk,Mk⟩(referred to in code as Tk) was sampled (assum-\\ning a uniform distribution over children) from the parent task Tn. Recall, |Dk|\\nwas determined as a parameter of the trial.\\n3. From Tktwo models were then generated; a weakest cw,a n daM D L cmdl.\\nTesting Phase: For each model c∈{cw,cmdl}, the testing phase was as follows:\\n1. The extension Zcofcwas then generated.\\n2. A prediction Dreconwas made s.t. Drecon ={z∈Zc:∃s∈Sn(s⊂z)}.\\n3.Dreconwas then compared to the ground truth Dn, and results recorded.\\nBetween 75and 256trials were run for each value of the parameter |Dk|. Fewer\\ntrials were run for larger values of |Dk|as these took longer to process. The\\nresults of these trails were then averaged for each value of |Dk|.\\n5.3 Results',\n",
       "  'Two sorts of measurements were taken for each trial. The ﬁrst was t h er a t ea t\\ngeneralisation occurred . Generalisation was deemed to have occurred where\\nDrecon =Dn. The number of trials in which generalisation occurred was mea-\\nsured, and divided by nto obtain the rate of generalisation for cwandcmdl.\\nError was computed as a Wald 95 %conﬁdence interval. The second measure-\\nment was the average extent to which models generalised .E v e nw h e r e\\nDrecon ̸=Dn, the extent to which models generalised could be ascertained.\\n|Drecon ∩Dn|\\n|Dn|was measured and averaged for each value of |Dk|, and the standard\\nerror computed. The results (see Tables 1and2) demonstrate that weakness is\\na better proxy for intelligence than description length. The generalisation rate\\nforcwwas between 110–500% of cmdl, and the extent was between 103−156%.',\n",
       "  'The Optimal Choice of Hypothesis Is the Weakest, Not the Shortest 49\\nTable 1. Results for Binary Addition\\n|Dk|cw cmdl\\nRate ±95% AvgExt StdErr Rate ±95% AvgExt StdErr\\n6 .11 .039 .75 .008 .10 .037 .48 .012\\n10 .27 .064 .91 .006 .13 .048 .69 .009\\n14 .68 .106 .98 .005 .24 .097 .91 .006\\nTable 2. Results for Binary Multiplication\\n|Dk|cw cmdl\\nRate ±95% AvgExt StdErr Rate ±95% AvgExt StdErr\\n6 .05 .026 .74 .009 .01 .011 .58 .011\\n10 .16 .045 .86 .006 .08 .034 .78 .008\\n14 .46 .061 .96 .003 .21 .050 .93 .003\\n6 Concluding Remarks\\nWe have shown that, if tasks are uniformly distributed, then weakness maximi-\\nsation is necessary and suﬃcient to maximise the probability that induction willproduce a hypothesis that generalises. It follows that there is no choice of proxy\\nthat performs at least as well as weakness maximisation across all possible com-',\n",
       "  'binations of child and parent task while performing strictly better in at least one.We’ve also shown that the minimisation of description length is neither neces-\\nsary nor suﬃcient. This calls into question the relationship between compression\\nand intelligence [ 5,19,20], at least in the context of enactive cognition. This is\\nsupported by our experimental results, which demonstrate that weakness is a far\\nbetter predictor of whether a hypothesis will generalise, than description length.Weakness should not be conﬂated with Ockham’s Razor. A simple statement\\nneed not be weak, for example “all things are blue crabs”. Likewise, a complex\\nutterance can assert nothing. Weakness is a consequence of extension, not form.If weakness is to be understood as an epistemological razor, it is this (which we\\nhumbly suggest naming “Bennett’s Razor”):\\nExplanations should be no more speciﬁc than necessary.\\n12\\nThe Apperception Engine: The Apperception Engine [ 21–23]( E v a n se ta l .',\n",
       "  'of Deepmind) is an inference engine that generates hypotheses that generalise\\noften. To achieve this, Evans formalised Kant’s philosophy to give the engine\\na “strong inductive bias”. The engine forms hypotheses from only very general\\n12We do not know which possibilities will eventuate. A less speciﬁc statement contra-\\ndicts fewer possibilities. Of all hypotheses suﬃcient to explain what we perceive, the\\nleast speciﬁc is most likely.',\n",
       "  '50 M. T. Bennett\\nassertions, meaning logical formulae which are universally quantiﬁed. That is\\npossible because the engine uses language speciﬁcally tailored to eﬃciently rep-resent the sort of sequences to which it is applied. Our results suggest a simpler\\nand more general explanation of why the engine’s hypotheses generalise so well.\\nThe tailoring of logical formulae to represent certain sequences amounts to achoice of v, and the use of only universally quantiﬁed logical formulae maximises\\nthe weakness of the resulting hypothesis. To apply this approach to induction\\nfrom child v-taskαto parent ωwould mean we only entertain a model m∈M\\nα\\nifp(m∈Mω|m∈Mα)=1. Obviously this can work well, but only for the\\nsubset of possible tasks that the vocabulary is able to describe in this way (any-\\nthing else will not be able to be represented as a universally quantiﬁed rule, andso will not be represented at all [ 24]). This illustrates how future research may',\n",
       "  'explore choices of vin aid of more eﬃcient induction in particular sorts of task,\\nsuch as the inference of linguistic meaning and intent (see appendix).\\nNeural Networks: How might a task be represented in the context of con-\\nventional machine learning? Though we use continuous real values in base 10to\\nformalise neural networks, all computation still takes place in a discrete, ﬁnite\\nand binary system. A ﬁnite number of imperative programs composed a ﬁnitenumber of times may be represented by a ﬁnite set of declarative programs.\\nLikewise, activations within a network given an input can be represented as a\\nﬁnite set of declarative programs, expressing a decision. The choice of architec-\\nture speciﬁes the vocabulary in which this is written, determining what sort of\\nrelations can be described according to the Chomsky Hierarchy [ 25]. The reason\\nwhy LLMs are so prone to fabrication and inconsistency may be because they',\n",
       "  'are optimised only to minimise loss, rather than maximise weakness [ 10]. Per-\\nhaps grokking [ 26] can be induced by optimising for weakness. Future research\\nshould investigate means by which weakness can be maximised in the context of\\nneural networks.\\nAcknowledgement. Appendices available on GitHub [ 1], supported by JST\\n(JPMJMS2033).\\nReferences\\n1. Bennett, M.T.: Technical Appendices. Version 1.2.1 (2023). https://doi.org/\\n10.5281/zenodo.7641742 .https://github.com/ViscousLemming/Technical-Appen\\ndices\\n2. Sober, E.: Ockham’s Razors: A User’s Manual. Cambridge University Press (2015)\\n3. Rissanen, J.: Modeling by shortest data description*. Automatica 14, 465–471\\n(1978)\\n4. Chollet, F.: On the Measure of Intelligence (2019)\\n5. Chaitin, G.: The limits of reason. Sci. Am. 294(3), 74–81 (2006)\\n6. Solomonoﬀ, R.: A formal theory of inductive inference. Part I. Inf. Control 7(1),\\n1–22 (1964)',\n",
       "  'The Optimal Choice of Hypothesis Is the Weakest, Not the Shortest 51\\n7. Solomonoﬀ, R.: A formal theory of inductive inference. Part II. Inf. Control 7(2),\\n224–254 (1964)\\n8. Kolmogorov, A.: On tables of random numbers. Sankhya: Indian J. Stati. A 369–\\n376 (1963)\\n9. Hutter, M.: Universal Artiﬁcial Intelligence: Sequential Decisions Based on Algo-\\nrithmic Probability. Springer, Heidelberg (2010)\\n10. Bennett, M.T.: Symbol emergence and the solutions to any task. In: Goertzel,\\nB., Iklé, M., Potapov, A. (eds.) AGI 2021. LNCS (LNAI), vol. 13154, pp. 30–40.\\nSpringer, Cham (2022). https://doi.org/10.1007/978-3-030-93758-4_4\\n11. Ward, D., Silverman, D., Villalobos, M.: Introduction: the varieties of enactivism.\\nTopoi 36(3), 365–375 (2017). https://doi.org/10.1007/s11245-017-9484-6\\n12. Harnad, S.: The symbol grounding problem. Physica D: Nonlinear Phenomena\\n42(1), 335–346 (1990)\\n13. Leike, J., Hutter, M.: Bad universal priors and notions of optimality. In: Proceed-',\n",
       "  'ings of the 28th COLT, PMLR, pp. 1244–1259 (2015)\\n14. Gupta, A.: Deﬁnitions. In: Zalta, E.N. (ed.) The Stanford Encyclopedia of Philos-\\nophy. Winter 2021. Stanford University (2021)\\n15. Paszke, A., et al.: PyTorch: an imperative style, high-performance deep learning\\nlibrary. In: NeurIPS. Curran Association Inc., USA (2019)\\n16. Kirk, D.: NVIDIA Cuda Software and GPU parallel computing architecture. In:\\nISMM 2007, Canada, pp. 103–104. ACM (2007)\\n17. Meurer, A., et al.: SymPy: symbolic computing in Python. PeerJ Comput. Sci. 3,\\ne103 (2017). https://doi.org/10.7717/peerj-cs.103\\n18. Hart, P.E., Nilsson, N.J., Raphael, B.: A formal basis for the heuristic determina-\\ntion of minimum cost paths. IEEE Trans. Syst. Sci. Cybern. 4(2), 100–107 (1968)\\n19. Hernández-Orallo, J., Dowe, D.L.: Measuring universal intelligence: towards an\\nanytime intelligence test. Artif. Intell. 174(18), 1508–1539 (2010)\\n20. Legg, S., Veness, J.: An approximation of the universal intelligence measure. In:',\n",
       "  'Dowe, D.L. (ed.) Algorithmic Probability and Friends. Bayesian Prediction andArtiﬁcial Intelligence. LNCS, vol. 7070, pp. 236–249. Springer, Heidelberg (2013).\\nhttps://doi.org/10.1007/978-3-642-44958-1_18\\n21. Evans, R.: Kant’s cognitive architecture. Ph.D. thesis. Imperial (2020)22. Evans, R., Sergot, M., Stephenson, A.: Formalizing Kant’s rules. J. Philos. Logic\\n49, 613–680 (2020)\\n23. Evans, R., et al.: Making sense of raw input. Artif. Intell. 299(2021)\\n24. Bennett, M.T.: Compression, the fermi paradox and artiﬁcial super-intelligence.\\nIn: Goertzel, B., Iklé, M., Potapov, A. (eds.) AGI 2021. LNCS (LNAI), vol. 13154,\\npp. 41–44. Springer, Cham (2022). https://doi.org/10.1007/978-3-030-93758-4_5\\n25. Delétang, G., et al.: Neural Networks and the Chomsky Hierarchy (2022)\\n26. Power, A., et al.: Grokking: generalization beyond overﬁtting on small algorithmic\\ndatasets. In: ICLR (2022)',\n",
       "  'Emergent Causality and the Foundation\\nof Consciousness\\nMichael Timothy Bennett(B)\\nThe Australian National University, Canberra, Australia\\nmichael.bennett@anu.edu.au\\nhttp://www.michaeltimothybennett.com/\\nAbstract. To make accurate inferences in an interactive setting, an\\nagent must not confuse passive observation of events with having inter-\\nvened to cause them. The dooperator formalises interventions so that\\nwe may reason about their eﬀect. Yet there exist pareto optimal mathe-\\nmatical formalisms of general intelligence in an interactive setting which,\\npresupposing no explicit representation of intervention, make maximallyaccurate inferences. We examine one such formalism. We show that in\\nthe absence of a dooperator, an intervention can be represented by a\\nvariable. We then argue that variables are abstractions, and that needto explicitly represent interventions in advance arises only because we\\npresuppose these sorts of abstractions. The aforementioned formalism',\n",
       "  'avoids this and so, initial conditions permitting, representations of rele-vant causal interventions will emerge through induction. These emergent\\nabstractions function as representations of one’s self and of any other\\nobject, inasmuch as the interventions of those objects impact the satis-faction of goals. We argue that this explains how one might reason about\\none’s own identity and intent, those of others, of one’s own as perceived\\nby others and so on. In a narrow sense this describes what it is to beaware, and is a mechanistic explanation of aspects of consciousness.\\nKeywords: causality\\n·theory of mind ·self aware AI ·AGI\\n1 Introduction\\nAn agent that interacts in the world cannot make accurate inferences unless it\\ndistinguishes the passive observation of an event from it having intervened to\\ncause that event [ 2,3]. Say we had two variables R,C∈{true,false }, where:\\nC=true↔“Larry put on a raincoat” andR=true↔“It rained”',\n",
       "  'Assume we have seen it rain only when Larry had his raincoat on, and he has\\nonly been seen in his raincoat during periods of rain. Based on these observations,\\nthe conditional probability of it raining if Larry is wearing his raincoat is p(R=\\ntrue|C=true) = 1. A naive interpretation of this is that we can make it rain\\nc⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2023\\nP. Hammer et al. (Eds.): AGI 2023, LNAI 13921, pp. 52–61, 2023.https://doi.org/10.1007/978-3-031-33469-6\\n_6',\n",
       "  'Emergent Causality and the Foundation of Consciousness 53\\nby forcing Larry to wear a raincoat, which is absurd. When we intervene to make\\nLarry wear a raincoat, the event that takes place is not “Larry put on a rain-\\ncoat” but actually “Larry put on a raincoat because we forced him to” .I ti sn o t\\nthat Bayesian probability is wrong, but interactivity complicates matters. By\\nintervening we are acting upon the system from the outside, to disconnect thosefactors inﬂuencing the choice of clothing. The “do” operator [ 4,5] resolves this in\\nthatdo[C=true] represents the intervention. It allows us to express notions such\\nasp(R=true|do[C=true]) =p(R=true)̸=p(R=true|C=true)=1 ,\\nwhich is to say that intervening to force Larry to wear a raincoat has no eﬀect\\non the probability of rain, but passively observing Larry put on a raincoat still',\n",
       "  'indicates rain with probability 1. To paraphrase Judea Pearl, one variable causesanother if the latter listens for the former [ 2]. The variable Rdoes not listen\\nto the C.Chowever does listen to R, meaning to identify cause and eﬀect\\nimposes a hierarchy on one’s representation of the world (usually represented\\nwith a directed acyclic graph). This suggests that, if accurate inductive infer-\\nence is desired, we must presuppose something akin to the dooperator. Yet\\nthere exist pareto optimal mathematical formalisms of general intelligence in\\nan interactive setting which, given no explicit representation of intervention,\\nmake maximally accurate inferences [ 1,6,7]. Given that the distinction between\\nobservation and intervention is necessary to make accurate inductive inferences\\nin an interactive setting, this might seem to present us with a contradiction.\\nOne cannot accurately infer an equivalent of the dooperator if such a thing is',\n",
       "  'a necessary precondition of accurate inductive inference. We resolve this ﬁrst\\nby showing that we can substitute an explicit dooperator with variables rep-\\nresenting each intervention. Then, using one of the aforementioned formalisms,we argue that need to explicitly represent intervention as a variable only arises\\nif we presuppose abstractions [ 8] like variables. If induction does not depend\\nupon abstractions as given, then abstractions representing interventions mayemerge through inductive inference. Beyond distinguishing passive observation\\nfrom the consequences of one’s own interventions, these emergent abstractions\\ncan also distinguish between the interventions and observations of others. This\\nnecessitates the construction of abstract identities and intents. We suggest this\\nis a mechanistic explanation of awareness, in a narrow sense of the term. Bynarrow we mean functional, access, and phenomenal consciousness, and only if',\n",
       "  'the latter is deﬁned as “ﬁrst person functional consciousness” [ 9,10\\n]; recognising\\nphenomenal content such as light, sound and movement with one’s body at thecentre of it all [ 11]. To limit scope, we do not address “the hard problem” [ 12].\\n2 Additional Background\\nThis section introduces relevant background material. The reader may wish to\\nskip ahead to section 3 and refer here as needed. In recognition of the philo-\\nsophical nature of this topic we present arguments rather than mathematicalproofs, and the paper should be understandable without delving too deeply into\\nthe math. While all relevant deﬁnitions are given here, context is provided by',\n",
       "  '54 M. T. Bennett\\nthe papers in which these deﬁnitions originated, and in technical appendices\\navailable on GitHub [ 1]. To those more familiar with the agent environment\\nparadigm, how exactly these deﬁnitions formalise cognition may seem unclear.\\nNeither agent nor environment are deﬁned. This is because it is a formalism of\\nenactivism [ 13], which holds that cognition extends into and is enacted within\\nthe environment. What then constitutes the agent is unclear. In light of this,\\nand in the absence of any need to deﬁne an agent absent an environment, why\\npreserve the distinction? Subsequently, the agent and environment are mergedto form a task [ 7], which may be understood as context speciﬁc manifestations of\\nintent, or snapshots of what bears some resemblance to “Being-in-the-world” as\\ndescribed by Heidegger [ 14]. In simpler terms, this reduces cognition to a ﬁnite\\nset of decision problems [ 7]. One infers a model from past interactions, and then',\n",
       "  'makes a decision based upon that model (akin to a supervised learner ﬁtting a\\nfunction to labelled data, then using that to generate labels for unlabelled data).\\nArguments as to why only ﬁnite sets are relevant are given elsewhere [ 15, p. 2].\\n2.1 List of Deﬁnitions\\nRefer to the technical appendices [ 1] for further information regarding deﬁnitions.\\nDeﬁnition 1 (environment)\\n– We assume a set Φwhose elements we call states , one of which we single\\nout as the present state .\\n–A declarative program is a function f:Φ→{true,false }, and we write P\\nfor the set of all declarative programs. By an objective truth about a state\\nφ, we mean a declarative program fsuch that f(φ)=true.\\nDeﬁnition 2 (implementable language)\\n–V={V⊂P:V is finite }is a set whose elements we call vocabularies ,o n e\\nof which\\n1we single out as the vocabulary vfor an implementable language.\\n–Lv={l⊆v:∃φ∈Φ(∀p∈l:p(φ)=true)}is a set whose elements we\\ncall statements .Lvfollows from Φand v.W ec a l l Lvanimplementable',\n",
       "  'language .\\n–l∈Lvistrueiﬀ the present state is φand∀p∈l:p(φ)=true.\\n–T h e extension of a statement a∈LvisZa={b∈Lv:a⊆b}.\\n–T h e extension of a set of statements A⊆LvisZA=⋃\\na∈AZa.\\n(Notation) Zwith a subscript is the extension of the subscript2.\\nDeﬁnition 3. ( v-task). For a chosen v, a task αis⟨Sα,Dα,Mα⟩where:\\n–Sα⊂Lvis a set whose elements we call situations ofα.\\n1The vocabulary vwe single out represents the sensorimotor circuitry with which an\\norganism enacts cognition - their brain, body, local environment and so forth.\\n2e.g.Zsis the extension of s.',\n",
       "  'Emergent Causality and the Foundation of Consciousness 55\\n–Sαhas the extension ZSα, whose elements we call decisions ofα.\\n–Dα={z∈ZSα:z is correct }is the set of all decisions which complete α.\\n–Mα={l∈Lv:ZSα∩Zl=Dα}whose elements we call models ofα.\\nΓvis the set of all tasks for our chosen v∈V.\\n(Notation) Ifω∈Γv, then we will use subscript ωto signify parts of ω, meaning\\none should assume ω=⟨Sω,Dω,Mω⟩even if that isn’t written.\\n(How a task is completed) Assume we’ve a v-taskωand a hypothesis h∈Lvs.t.\\n1. we are presented with a situation s∈Sω,a n d\\n2. we must select a decision z∈Zs∩Zh.\\n3. Ifz∈Dω, then zis correct and the task is complete. This occurs if h∈Mω.\\nDeﬁnition 4 (probability). We assume a uniform distribution over Γv.\\nDeﬁnition 5 (generalisation). A statement lgeneralises to α∈Γviﬀl∈\\nMα.W es a y lgeneralises from αtov-taskωif we ﬁrst obtain lfromMαand\\nthen ﬁnd it generalises to ω.\\nDeﬁnition 6 (child and parent). Av-taskαis a child of v-taskωifSα⊂Sω',\n",
       "  'andDα⊆Dω. This is written as α⊏ω.I fα⊏ωthenωis then a parent of α.\\nDeﬁnition 7 (weakness). The weakness of l∈Lvis|Zl|.\\nDeﬁnition 8 (induction). αandωare v-tasks such that α⊏ω. Assume we\\nare given a proxy qv∈Q, the complete deﬁnition of αand the knowledge that\\nα⊏ω. We are not given the deﬁnition of ω. The process of induction would\\nproceed as follows:\\n1. Obtain a hypothesis by computing a model h∈arg max\\nm∈Mαqv(m).\\n2. If h∈Mω, then we have generalised from αtoω.\\n2.2 Premises\\nFor the purpose of argument we will adopt the following premises:\\n(prem. 1) To maximise the probability that induction generalises from α\\ntoω, it is necessary and suﬃcient to maximise weakness. [ 1]\\nFor our argument this optimality is less important than the representation of\\ninterventions it implies. In any case the utility of weakness as a proxy is not\\nlimited to lossless representations or optimal performance. Approximation maybe achieved by selectively forgetting outliers\\n3, a parallel to how selective amnesia',\n",
       "  '3For example, were we trying to generalise from αtoω(where α⊏ω)a n dk n e w\\nthe deﬁnition of αcontained misleading errors, we might selectively forget outlying\\ndecisions in αto create a child γ=⟨Sγ,Dγ,Mγ⟩(where γ⊏α) such that Mγ\\ncontained far weaker hypotheses than Mα.',\n",
       "  '56 M. T. Bennett\\n[16] can help humans reduce the world to simple dichotomies [ 17]o rc o n ﬁ r mp r e -\\nconceptions [ 18]. Likewise, a task expresses a threshold beyond which decisions\\nare “good enough” [ 19]. The proof of optimality merely establishes the upper\\nbound for generalisation. As a second premise, we shall require the emergence\\nor presupposition of representations of interventions:\\n(prem. 2) To make accurate inductive inferences in an interactive setting,\\nan agent must not confuse the passive observation of an event with havingintervened to cause that event. [ 2]\\n3 Emergent Causality\\nThe formalism does not presuppose an operator representing intervention. Given\\nour premises, we must conclude from this that either that (prem. 1) is false,\\nor induction as in Deﬁnition 8will distinguish passive observation of an event\\nfrom having intervened to cause that event.\\n3.1 The doOperator as a Variable in Disguise',\n",
       "  'In the introduction we discussed an example involving binary variables R(rain)\\nandC(raincoat). From p(R=true|C=true) = 1 we drew the absurd\\nconclusion that if we intervene to make C=true, we can make it rain. The true\\nrelationship between RandCis explained by a directed acyclic graph:\\nC R\\nThe intervention do[C=c] deletes an edge (because rain can have no eﬀect on\\nthe presence of a coat we’ve already forced Larry to wear) giving the following:\\nC R\\nBy intervening in the system, we are acting upon it from the outside. In doing so\\nwe disconnect those factors inﬂuencing the choice of clothing. The dooperator\\nlets us express this external inﬂuence. However, if we don’t have a dooperator\\nthere remains another option. We propose representing an intervention with a\\nvariable, so that we are no longer intervening in the system from outside. For\\nexample do[C=true] might be represented by Asuch that p(C=true|A=\\ntrue)=1a n d p(C|A=false)=p(C):\\nA C R',\n",
       "  'We can now represent that p(R=true|C=true,A =true)=p(R=true)̸=\\np(R=true|C=true,A =false) = 1. This expands the system to include an\\naction by a speciﬁc actor, rather than accounting for interventions originating\\noutside the system (as the dooperator does).',\n",
       "  'Emergent Causality and the Foundation of Consciousness 57\\n3.2 Emergent Representation of Interventions\\nThis does not entirely resolve our problem. Even if intervention is represented as\\na variable, that variable must still be explicitly deﬁned before accurate inductioncan take place. It is an abstract notion which is presupposed. Variables are\\nundeﬁned in the context of Deﬁnitions 1,2and3for this very reason. Variables\\ntend to be very abstract (for example, “number of chickens” may presupposeboth a concept of chicken and a decimal numeral system), and the purpose\\n(according to [ 7] and [ 19]) of the formalism is to construct such abstractions via\\ninduction. It does so by formally deﬁning reality (environment and cognitionwithin that) using as few assumptions as possible [ 1], in order to address symbol\\ngrounding [ 8] and other problems associated with dualism. In this context, cause\\nand eﬀect are statements as deﬁned in Deﬁnition 2. Returning to the example',\n",
       "  'of Larry, instead of variables A,CandRwe have a vocabulary v,a n d c,r∈L\\nv\\nwhich have a truth value in accordance with Deﬁnition 2:\\nc↔“Larry put on a raincoat” andr↔“It rained”\\nAs before, assume we have concluded p(r|c) = 1 from passive observation,\\nthe naive interpretation of which is that we can make it rain by forcing Larry\\nto wear a coat. However, the statement associated with this intervention is notjustc=“Larry put on a raincoat” but a third a∈Lsuch that:\\na↔“Larry put on a raincoat because we forced him to”\\na c r\\nBecause we’re now dealing with statements, and because statements are sets of\\ndeclarative programs which are inferred rather than given, we no longer need\\nto explicitly deﬁne interventions in advance. Statements in an implementable\\nlanguage represent sensorimotor activity, and are formed via induction [ 1,7]. The\\nobservation of cis part of the sensorimotor activity a, meaning c⊆a(if Larry is',\n",
       "  'not wearing his raincoat, then it also cannot be true that we are forcing him towear it). There is still no dooperator, however i=a−cmay be understood as\\nrepresenting the identity of the party undertaking the intervention. If i̸=∅then\\nit is at least possible to distinguish intervention from passive observation, in theevent that aandcare relevant (we still need explain under what circumstances\\nthis is true). Whether intervention and observation are indistinguishable depends\\nupon the vocabulary V, the choice of which determines if i=∅,o ri̸=∅(the\\nlatter meaning that it is distinguishable). Thus interventions are represented,\\nbut only to the extent that the vocabulary permits.\\nDeﬁnition 9 (intervention). Ifais an intervention to force c, then c⊆a.\\nIntervention is distinguishable from observation only where c⊂a.',\n",
       "  '58 M. T. Bennett\\n3.3 When Will Induction Distinguish Intervention\\nFrom Observation?\\nFrom (prem. 1) we have that choosing the weakest model maximises the prob-\\nability of generalisation. There are many combinations of parent and child task\\nfor which generalisation from child to parent is only possible by selecting a model\\nthat correctly distinguishes the eﬀects of intervention from passive observation\\n(a trivial example might be a task informally deﬁned as “predict the eﬀect ofthis intervention”). It follows that to maximise the probability of generalisation\\nin those circumstances the weakest model must distinguish between an interven-\\ntionaand what it forces, c,s ol o n ga s (prem. 2) is satisﬁed as in Deﬁnition 9,\\ns.t.a̸=c.\\n4 Awareness\\nWe have described how an intervention ais represented as distinct from that\\nwhich it forces, c. Induction will form models representing this distinction in',\n",
       "  'tasks for which this aids completion. Now we go a step further. Earlier we dis-cussed i=a−cas the identity of the party undertaking an intervention a.W e\\nmight deﬁne a weaker identity as k⊂i, which is subset of any number of diﬀer-\\nent interventions undertaken by a particular party. The dooperator assumes the\\nparty undertaking interventions is given, and so we might think of kabove as\\nmeaning “me”. However, there is no reason to restrict emergent representations\\nof intervention only to one’s self. For example there may exist Harvey, who alsointervenes to force c. It follows we may have vsuch that c⊂v,a n d vrepresents\\nour observation of Harvey’s intervention.\\nav\\nc r\\nIfk⊆a−ccan represent our identity as party undertaking interventions, it\\nfollows that j⊆v−cmay represent Harvey’s. Both identities are to some extent\\ncontext speciﬁc (another intervention may produce something other than j,o ra\\nsubset of j, for Harvey), but these emergent identities still exist as a measurable',\n",
       "  'quantity independent of the interventions with which they’re associated.\\nDeﬁnition 10 (identity). Ifais an intervention to force c, then k⊆a−c\\nmay function as an identity undertaking the intervention if k̸=∅.\\nOne’s own identity is used to distinguish interventions from passive experiences\\nto facilitate accurate inductive inference in an interactive setting. It follows from\\n(prem. 1) that every object that has an impact upon one’s ability to complete\\ntasks must alsohave an identity4, because failing to account for the interventions\\nof these objects would result in worse performance.\\n4Assuming interventions are distinguishable.',\n",
       "  'Emergent Causality and the Foundation of Consciousness 59\\n4.1 Intent\\nThe formalism we are discussing originated as a mechanistic explanation of the-\\nory of mind called “The Mirror Symbol Hypothesis” [ 19], and of meaning in\\nvirtue of intent [ 7] (similar to Grice’s foundational theory of meaning [ 20]). A\\nstatement is a set of declarative programs, and can be used as a goal constraint\\nas is common in AI planning problems [ 21]. In the context of a task a model\\nexpresses such a goal constraint, albeit integrated with how that goal is to be\\nsatisﬁed [ 1,7]. If one is presented with several statements representing decisions,\\nand the situations in which they were made (a task according to Deﬁnition 3),\\nthen the weakest statement with which one can derive the decisions from the\\nsituations (a model) is arguably the intent those decisions served [ 7]. Thus, if\\nidentity kexperiences interventions undertaken by identity j, then kcan infer',\n",
       "  'something of the intent of jby constructing a task deﬁnition and computing the\\nweakest models [ 7]. This is a mechanistic explanation of how it is possible that\\none party may infer another’s intent. Assuming induction takes place accordingto Deﬁnition 8, then it is also necessary to the extent that kaﬀect’s j’s ability\\nto complete tasks. Otherwise, j’s models would not account for j’s interventions\\nand so performance would be negatively impacted. However, a few interventionsis not really much information to go on. Humans can construct elaborate ratio-\\nnales for behaviour given very little information, which suggests there is more\\nto the puzzle. The Mirror Symbol Hypothesis argues that we ﬁll in the gaps byprojecting our own emergent symbols (either tasks or models, in this context)\\nrepresenting overall, long term goals and understanding onto others in order to\\nconstruct a rationale for their immediate behaviour [ 7], in order to empathise.',\n",
       "  '4.2 How Might We Represent the Mirror Symbol Hypothesis?\\nAssume there exists a task Ωwhich describes every decision kmight ever make\\nwhich meets some threshold of “good enough” [ 7,19] at a given point in time.\\nDeﬁnition 11 (higher and lower level statements). A statement c∈Lis\\nhigher level than a∈LifZ\\na⊂Zc, which is written as a⊏c.\\nA model mΩ∈MΩisk’s “highest level” intent or goal (given the threshold),\\nmeaning ZΩ=DΩ.U s i n g mΩandk’s observation of decision dmade in situation\\nsbyj(the observation of which would also be a decision), kcould construct a\\nlower level model mω⊏mΩsuch that d∈Zs∩Zmω. In other words, mωis a\\nrationale constructed by kto explain j’s intervention. Related work explores this\\nin more depth [ 7,19]. For our purposes it suﬃces to point out that in combining\\nemergent causality, identity, The Mirror Symbol Hypothesis [ 19]a n ds y m b o l\\nemergence [ 7], we have a mechanistic explanation of the ability to reason about',\n",
       "  'one’s own identity and intent, and that of others, in terms of interventions.\\nLikewise the ability to predict how one’s own intent is modelled by another is alsoof value in predicting that other’s behaviour. In tasks of the sort encountered by\\nliving organisms, optimal performance would necessitate identity kconstructing\\na model of j’s model of k,a n d j\\n′smodel of k′smodel of ja n ds oo nt ot h e',\n",
       "  '60 M. T. Bennett\\ngreatest extent permitted by v(the ﬁnite memory and any other limitations\\none’s ability to represent predictions of predictions of predictions ad inﬁnitum).\\n4.3 Consciousness\\nWe have described a means by which an agent may be aware of itself, of others, of\\nthe intent of others and of the ability of others to model its own intent. By aware,we mean it has access to and will function according to this information (access\\nand functional consciousness, contextualising everything in terms of identities\\nand their intent). Boltuc argues that phenomenal consciousness (characterised\\nas ﬁrst person functional consciousness) is explained by today’s machine learn-\\ning systems [ 10]. We would suggest his argument extends to our formalism, and\\nin any case if qualia are a mechanistic phenomenon then they are already repre-\\nsented by the vocabulary of the implementable language. What is novel in our',\n",
       "  'formalism is not just that it points out that causal inference may construct iden-tity and awareness, but that it does so with a formulation that also addresses\\nenactive cognition, symbol emergence and empathy [ 7,19].\\nAnthropomorphism: An implementation of what we have described would\\nconstruct an identity for anything and everything aﬀecting its ability to complete\\ntasks - even inanimate objects like tools, or features of the environment. Intent\\nwould be ascribed to those identities, to account for the eﬀect those objects have\\nupon one’s ability to satisfy goals. Though this might seem a ﬂaw, to do anythingelse would negatively aﬀect performance. Interestingly, this is consistent with\\nthe human tendency [ 22] to anthropomorphise. We ascribe agency and intent to\\ninanimate objects such as tools, the sea, mountains, the sun, large populationsthat share little in common, things that go bump in the night and so forth.',\n",
       "  'Fragmented Identities: It is also interesting to consider what this says of\\nsystems which are less than optimal (do not identify the weakest hypothesis), or\\nwhich do not use a vocabulary which permits the construction of one identityshared by all of the interventions it undertakes. Such a thing might construct\\nmultiple unconnected identities for itself, and ascribe diﬀerent intentions to each\\none. Likewise if the model constructs multiple identities for what is in fact thesame object, it may hallucinate and hold contradictory beliefs about that object.\\nAcknowledgement. Appendices available on GitHub [ 1], supported by JST\\n(JPMJMS2033).\\nReferences\\n1. Bennett, M.T.: Technical Appendices. Version 1.2.1 (2023). https://doi.org/\\n10.5281/zenodo.7641742 . https://github.com/ViscousLemming/Technical-\\nAppendices',\n",
       "  'Emergent Causality and the Foundation of Consciousness 61\\n2. Pearl, J., Mackenzie, D.: The Book of Why: The New Science of Cause and Eﬀect,\\n1st edn. Basic Books Inc, New York (2018)\\n3. Ortega, P.A., et al.: Shaking the foundations: delusions in sequence models for\\ninteraction and control. Deepmind (2021)\\n4. Pearl, J.: Causal diagrams for empirical research. Biometrika 82(4), 669–688 (1995)\\n5. Pearl, J.: Causality, 2nd edn. Cambridge University Press, Cambridge (2009)6. Hutter, M.: Universal Artiﬁcial Intelligence: Sequential Decisions Based on Algo-\\nrithmic Probability. Springer, Heidelberg (2010). https://doi.org/10.1007/b138233\\n7. Bennett, M.T.: Symbol emergence and the solutions to any task. In: Goertzel,\\nB., Ikl´ e, M., Potapov, A. (eds.) AGI 2021. LNCS (LNAI), vol. 13154, pp. 30–40.\\nSpringer, Cham (2022). https://doi.org/10.1007/978-3-030-93758-4\\n4\\n8. Harnad, S.: The symbol grounding problem. Phys. D: Nonlinear Phenom. 42(1),\\n335–346 (1990)',\n",
       "  '9. Franklin, S., Baars, B.J., Ramamurthy, U.: A phenomenally conscious robot? In:\\nAPA Newsletter on Philosophy and Computers, vol. 1 (2008)\\n10. Boltuc, P.: The engineering thesis in machine consciousness. Techn´ e Res. Philos.\\nTechnol. 16(2), 187–207 (2012)\\n11. Block, N.: The harder problem of consciousness. J. Philos. 99(8), 391 (2002)\\n12. Chalmers, D.: Facing up to the problem of consciousness. J. Conscious. Stud. 2(3),\\n200–219 (1995)\\n13. Ward, D., Silverman, D., Villalobos, M.: Introduction: the varieties of enactivism.\\nTopoi 36(3), 365–375 (2017). https://doi.org/10.1007/s11245-017-9484-6\\n14. Wheeler, M.: Martin heidegger. In: Zalta, E.N. (ed.) The Stanford Encyclopedia\\nof Philosophy. Fall 2020. Stanford University (2020)\\n15. Bennett, M.T., Maruyama, Y.: Intensional Artiﬁcial Intelligence: From Symbol\\nEmergence to Explainable and Empathetic AI. Manuscript (2021)\\n16. Bekinschtein, P., et al.: A retrieval-speciﬁc mechanism of adaptive forgetting in the',\n",
       "  'mammalian brain. Nat. Commun. 9(1), 4660 (2018)\\n17. Berlin, S.B.: Dichotomous and complex thinking. Soc. Serv. Rev. 64(1), 46–59\\n(1990)\\n18. Nickerson, R.S.: Conﬁrmation bias: a ubiquitous phenomenon in many guises. Rev.\\nGen. Psychol. 2(2), 175–220 (1998)\\n19. Bennett, M.T., Maruyama, Y.: Philosophical speciﬁcation of empathetic ethical\\nartiﬁcial intelligence. IEEE Trans. Cogn. Dev. Syst. 14(2), 292–300 (2022)\\n20. Grice, H.P.: Studies in the Way of Words. Harvard University Press, Cambridge\\n(2007)\\n21. Kautz, H., Selman, B.: Planning as satisﬁability. In: IN ECAI 1992, pp. 359–363.\\nWiley, New York (1992)\\n22. Urquiza-Haas, E.G., Kotrschal, K.: The mind behind anthropomorphic thinking:\\nattribution of mental states to other species. Anim. Behav. 109, 167–176 (2015)',\n",
       "  'The M Cognitive Meta-architecture\\nas Touchstone for Standard Modeling\\nof AGI-Level Minds\\nSelmer Bringsjord(B), James T. Oswald, Michael Giancola, Brandon Rozek,\\nand Naveen Sundar Govindarajulu\\nRensselaer AI and Reasoning (RAIR) Lab, Department of Cognitive Science,\\nDepartment of Computer Science, Lally School of Management (1st auth),\\nRensselaer Polytechnic Institute (RPI), Troy, NY 12180, USA\\nselmer.bringsjord@gmail.com\\nAbstract. We introduce rudiments of the cognitive meta-architecture\\nM (majuscule of μand pronounced accordingly), and of a formal proce-\\ndure for determining, with M as touchstone, whether a given cognitive\\narchitecture X i(from among a ﬁnite list 1 ...kof modern contenders)\\nconforms to a minimal standard model of a human-level AGI mind. Theprocedure, which for ease of exposition and economy in this short paper\\nis restricted to arithmetic cognition, requires of a candidate X\\ni,( 1 ) ,a\\ntrue biconditional expressing that for any human-level agent a, a prop-',\n",
       "  'erty possessed by this agent, as expressed in a declarative mathematical\\nsentence s(a), holds if and only if a formula χi(a) in the formal machin-\\nery/languages of X iholds as well ( abeing an in-this-machinery coun-\\nterpart to natural-language name a). Given then that M is such that\\ns(a)i ﬀμ(m), where the latter formula is in the formal language of M,\\nwith mthe agent modeled in M, a minimal standard modeling of an\\nAGI-level mind is certiﬁably achieved by X iif, (2), it can be proved\\nthatχi(a)i ﬀμ(a).We conjecture herein that such conﬁrmatory theo-\\nrems can be proved with respect to both cognitive architectures NARSand SNePS, and have other cognitive architectures in our sights.\\nKeywords: standard modeling of AGI-level minds\\n·cognitive\\narchitectures ·computational logic\\n1 Arithmetic as the Initial Target\\nDespite ﬂorid heraldry from Kissinger et al. [ 14] announcing an “intellectual\\nrevolution” caused by the arrival of ChatGPT and its LLM cousins of today, we',\n",
       "  'know that AGI has not arrived. This is so because, as Arkoudas [ 3] has elegantly\\npointed out in a comprehensive analysis, ChatGPT doesn’t know that 1 is not\\ngreater than 1, and surely AGI subsumes command of elementary arithmetic on\\nc⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2023\\nP. Hammer et al. (Eds.): AGI 2023, LNAI 13921, pp. 62–73, 2023.https://doi.org/10.1007/978-3-031-33469-6\\n_7',\n",
       "  'The M Cognitive Meta-architecture 63\\nthe natural numbers.1We do not pick the domain of arithmetic here randomly.\\nArithmetic, and more generally all or at least most of logico-mathematics, isby our lights the only cross-cutting and non-negotiable space we can presently\\nturn to in order to at least be in position to judge whether some artiﬁcial agent\\nqualiﬁes as having AGI versus merely AI. Given this, we turn ﬁrst to arithmeticcognition to enable us to share our formal procedure for using the cognitive meta-\\narchitecture M as a touchstone for determining whether a candidate cognitive\\narchitecture conforms to a minimal standard modeling of AGI-level minds.\\n2 The Formal Procedure, for Arithmetic Cognition\\n2.1 Peano Arithmetic to Anchor Arithmetic Cognition\\nTo anchor arithmetic cognition as a proper part of mathematical cognition at\\nthe human level, we resort herein to simple arithmetic with only addition and\\nmultiplication. The particular axiom system we bring to bear is ‘Peano Arith-',\n",
       "  'metic,’ or just—to use the conventional label— PA.U n a s s u m i n ga si tm a yb e ,i t\\nhas a storied place in the history of logic and mathematics, serving as the basis\\nfor such stunning results as G¨ odel’s incompleteness theorems.2In particular, we\\nshall employ herein a simple theorem in PA, viz., ⊢2 + 2 = 4. In the general\\nform of our procedure, not merely arithmetic cognition, but all of mathematical\\ncognition reduced to formal logic by reverse mathematics will be in play, which\\nmeans that not just the likes of 2 + 2 = 4 but any statements provable from theaxioms i.e. PA\\n⊢known to be suﬃcient for all of mathematics, as charted by the\\ndeﬁnitive [ 24], will be fair game.\\n2.2 Deﬁnition of the Procedure\\nLets(a) be a mathematical declarative sentence involving both a mathematically\\ncognizing agent aand a single purely arithmetic proposition believed by a. Such\\n1This example is but the tip of an iceberg of negative knowledge in the realm of',\n",
       "  'mathematics for this and indeed all present and foreseeable LLMs, as Arkoudasshows/explains. Note that Bubeck et al. [ 8] have made the ﬁgurative claim that\\nGPT-4 has—and we quote—“sparks of AGI.” We don’t know what this metaphorical\\nclaim means mathematically (thus confessedly ﬁnd little meaning in it), but clearly\\nby conversational implication these authors would themselves agree that while GPT-\\n4i sa nA I ,i t ’ sa nA GI. Ifxhas sparks of being an R,t h e n xisn’t an R—this is the\\nprinciple at the root of the implication here.\\n2We shall not spend the considerable time that would be needed to list the (count-ably inﬁnite) axioms, and explain them. Readers can consult the elegant [ 9]f o rn i c e\\ncoverage of PA(and illuminating commentary on this axiom system). There are the-\\nories of arithmetic even simpler than PA, because PAincludes an axiom relating to',\n",
       "  'mathematical induction, and the simpler systems leave this axiom aside. For exam-ple, readers unfamiliar with mathematical induction can, if motivated, consult the\\ninduction-free theory of arithmetic known as ‘Robinson Arithmetic,’ or sometimes\\njust as ‘ Q;’ for elegant coverage, see [ 5].',\n",
       "  '64 S. Bringsjord et al.\\nsentences typically draw from both natural language (e.g. English) and formal\\nlanguages. Here’s an example of such a sentence: “G¨ odel believed that ﬁrst-order\\nlogic is complete.” We know he believed this because his dissertation centered\\naround the landmark proof of this completeness. But this example is far too\\ncomplex for our present limited purposes. Accordingly, turning to PA, here’s a\\nmuch simpler example of a form that will guide us, put in the present tense:\\n“G¨odel believes that 2 + 2 = 4.”\\nThe general form is that some agent ais denoted, that agent has the epistemic\\nattitude of belief, and the target of that belief is a proposition, expressible inPA, that 2 + 2 = 4. We shall denote the form this way: s(a), to indicate that\\nour sentence form must involve an agent a; we leave belief and the believed\\nproposition implicit.\\nNext, let ‘ μ(m)’ be a formula in M, in a suitable formal language that logi-',\n",
       "  'cizess(a). Minimally, this language will have an epistemic modal operator for\\nbelief, and will be able to encode arithmetic propositions from natural language.\\nTherefore, the language will need to be a quantiﬁed modal one whose extensional\\ncomponent is at least ﬁrst-order logic. Now, the following is by inspection thecase with respect to μ:\\n3\\ns(a)i ﬀμ(m).\\nNext, let Xibe any cognitive architecture that aspires to enable standard mod-\\neling (and simulation) of AGI-level minds. What is needed from this cognitive\\narchitecture is, (1), the truth of this biconditional:\\ns(a)i ﬀχi(c).\\nStandard modeling of an AGI-level mind, given the foregoing, is achieved by Xi\\nif, (2), it can be proved that4\\nχi(c)i ﬀμ(m).\\nWe conjecture that such conﬁrmatory theorems can be proved with respect to\\nboth cognitive architectures NARS and SNePS, to which, resp., we shortly turn.\\nBut ﬁrst we give a very quick overview of the nature of M itself.\\n3 The M Cognitive Meta-architecture: Key Attributes',\n",
       "  'M is not a new cognitive architecture intended and designed to compete with thelikes of Soar and ACT-R and so on as a platform to model and simulate human\\n3The formula in the case of M itself is\\nμ(m):=(believes! m t (= (+ (s (s 0)) (s (s 0))) (s (s (s (s 0)))))) ,\\nwhere sis the successor function and 0 is primitive, but technical details regarding\\nM are outside of current scope.\\n4The kernel of the procedure just described was ﬁrst adumbrated in [ 4].',\n",
       "  'The M Cognitive Meta-architecture 65\\nand/or AGI cognition. There are innumerable competing architectures in play\\ntoday [ 15], all directly reﬂecting the particular predilections of their human cre-\\nators and developers.5M is for assessing and harmonizing these “particularist”\\narchitectures at a meta level, and is marked by the following three distinguishing\\nattributes:\\n–Non-Partisan . M is not designed to advance any particular convictions about\\nthe nature of cognition, and is in this regard unlike the typical cognitive\\narchitecture. To mention just one example, certainly Soar was originally con-ceived to commit to and build upon the conviction that a key part of human\\ncognition centers around condition-action rules . Many other examples of par-\\nticularist convictions could be enumerated here for many competing cogni-tive architectures. In stark contrast, M reﬂects the attitude that any partisan\\nadvocacy militates against standardization; instead, the attitude is to move',\n",
       "  'as soon as possible to formalization using the discipline of formal logic. Ofcourse, no particular logic is to be locked into in any way as long as its\\na quantiﬁed modal one whose extensional component is at least ﬁrst-order\\nlogic.\\n–Thoroughgoingly Formal: Axiomatic and Theorem-based . M is inseparably\\naligned with a purely formal view of science and engineering, accordingto which whatever phenomena is observed and to be deeply understood\\nand predicted should be axiomatized. The axiomatization of mathematics\\nis now mature (and is the initial focus in the application of M as touchstonefor whether a given cognitive architecture can minimally be used for stan-\\ndard modeling and simulation of AGI-level mind), and the axiomatization of\\nphysics is now remarkably mature; consider for example that not only classi-cal mechanics is long done [ 19], but special relativity is largely captured [ 2],\\nand advances are fast being made on general relativity and quantum mechan-',\n",
       "  'ics. M is based on the assumption that this level of high maturity should nowbe applied to intelligence, so that matters can be theorem-based.\\n–Minimalist . Given all the resources formal science oﬀers for capturing cogni-\\ntion, use of M is guided by a minimalist approach. The smaller and simpleris the logical system that can be used to capture a target, the better.\\n4 Applying the Procedure\\nIn this short paper, we cannot fully chronicle the application of our procedure\\nto candidate cognitive architectures. But we attempt to partially justify our\\noptimism that both the cognitive architectures NARS and SNePS will yield ineach case the needed theorem by virtue of which standard modeling is conﬁrmed.\\n5We conjecture that the set of all of these architectures is pairwise inconsistent, but\\nleave this disturbing prospect aside for subsequent investigation via M.',\n",
       "  '66 S. Bringsjord et al.\\n4.1 Exploration of NARS\\nWhat is χi(a) for NARS? The sentence s(a) says that abelieves 2 + 2 = 4 to be\\na true statement, and we shall assume the counterpart to agent ais the NARS\\nagent n, and that the formula νis the in-system counterpart to s. Next, we note\\nthat instead of statements NARS has judgments : statements with associated\\nfuzzy truth-values, consisting of a frequency f∈[0,1] that represents a degree\\nof belief in the underlying statement, and a conﬁdence c∈[0,1] representing\\nhow stable the belief is (Deﬁnition 3.3 in [ 25]). For our target of eventually\\ndemonstrating ν(n)i ﬀμ(m) it will suﬃce6that we deﬁne ν(n) as the statement\\n“The NARS agent nbelieves the judgment 2 + 2 = 4 with a frequency of 1\\n(there is only positive evidence for the statement).” Formalizing this further,\\na NARS agent is said to believe ajudgment iﬀ it is either an experience ,a',\n",
       "  'judgment provided to the system directly, or a statement that can be derivedfrom experiences (Deﬁnition 3.7 in [ 25]). Thus ν(n) for a NARS agent nis true by\\nproviding 2+2 = 4 as a standalone experience (in our case perhaps provided by\\nthe theoretical perception system outlined in [ 26]).\\n7Finally, the representation\\nof the actual statement 2 + 2 = 4 can be accomplished in a number of ways,\\nas NARS supports the representation of relational terms that can represent\\narbitrary n-ary relations between terms that represent objects. One example of\\nthis representation in Narsese is <(∗224 ) →add > where addis a term\\nrepresenting a relation between two summands and a sum.\\nHaving deﬁned ν(n), we can turn to a proof sketch for ν(n)i ﬀμ(m). There\\nare multiple approaches to this proof, one particularly formal variant would be\\nexpressing NAL in one of our cognitive calculi —a specialized type of logical\\nsystem for Theory-of-Mind reasoning8—in a higher-order logic and proving a\\nbridge theorem.',\n",
       "  'Instead for economy we opt for an intuitive proof based on theoretical ide-\\nalized perception systems for NARS and M. For the forward direction of bicon-\\nditional proof we assume ν(n). By our above deﬁnition, ν(n) iﬀ the agent n\\nexperiences 2 + 2 = 4 or has experiences that deductively9lead to the con-\\n6We hold that conﬁdence is irrelevant here as it is a temporal property which only\\nimpacts how likely the system is to change its mind, which has use for nonmonotonicreasoning but is irrelevant to our current deduction-only explorations.\\n7Additionally we could proceed by providing any number of experiences that allowthe system to derive 2 + 2 = 4 as long as they allow the system to derive 2 + 2 =\\n4 with frequency 1.\\n8Cognitive calculi build oﬀ of the notion of traditional logical systems, which consist of\\na formal language L, a set of inference schemata I, and a formal semantics S.T h e\\nmost notable distinguishing factors of cognitive calculi are (1) they contain modal',\n",
       "  'operators for mental states, e.g., perception, belief, obligation; and (2) they containno model-based semantics; instead the semantics of formulae are purely inference-\\ntheoretic. That is, the semantics are expressed exclusively through the inference\\nschemata I. For a longer exposition of exactly what a cognitive calculus is and\\nisn’t, we refer the interested reader to Appendix A of Bringsjord et al. [ 7].\\n9Abductive and inductive reasoning in NARS have the resulting frequency of the\\nconclusion depend on conﬁdence values inﬂuenced by a system parameter; as this can',\n",
       "  'The M Cognitive Meta-architecture 67\\nclusion 2 + 2 = 4 with frequency 1 in n. Under idealized perception, this\\nimplies the existence of external representations of either sthat 2 + 2 = 4,\\nor a set of statements Sthat imply s. The existence of these external rep-\\nresentations means that an M agent munder idealized perception would also\\nperceive s,P(m,·,s) or perceive the set of S,⋀\\ne∈SP(m,·,e). Since many stan-\\ndard cognitive calculi have inference schemata allowing perceived statements\\nto become believed statements, and others allowing propositional reason-\\ning on beliefs, in the ﬁrst case P(m,·,s)→B(m,·,s), and in the\\nsecond⋀\\ne∈SP(m,·,e)→⋀\\ne∈SB(m,·,e)→B(m,·,s) which is the deﬁ-\\nnition of μ(m). For the backward direction of the biconditional proof,\\nwe assume μ(m) to derive ν(n) using the same argument outlined for\\nthe forward direction.\\nWe thus claim that ν(n)i ﬀμ(m),which conﬁrms that NARS conforms to a\\nminimal standard modeling of AGI-level minds.\\n4.2 Exploration of SNePS and GLAIR',\n",
       "  'SNePS is a KRR system, ultimately in fact a logic [ 22], that can be used as either\\na standalone system or inside others; GLAIR is a cognitive architecture designed\\nby SNePS scientists that uses SNePS for KRR [ 23]. Asγ(g) for GLAIR (or any\\nagent using SNePS for KRR) depends solely on representation within SNePS atthe knowledge layer of a GLAIR agent [ 23], we generalize and refer to γ(g)f o r\\nany arbitrary agent having SNePS under its hood, henceforth referred to simply\\nasSNePS agents . Any statement within a SNePS system is said to be believed by\\nthe system. Figure 1shows a representation of the statement 2+2 = 4 in SNePS\\nas a network. [ 11] makes a distinction between a SNePS agent understanding\\nthat 2 + 2 = 4 as declarative knowledge versus understanding what 2 + 2 = 4means as semantic knowledge. In this language, γ(g) can be interpreted purely in\\nFig. 1. A SNePS agent’s belief that 2 + 2 = 4. where m2 is the functional term',\n",
       "  'representing a resultant Sum, from n2t w i c e ; m1 is the proposition that m2e v a l u a t e s\\nton4; and m3 is the proposition that m2 has a value. (Adapted from Fig. 4.1 in [ 11]).\\nbe arbitrary, this will not guarantee the preservation of frequency of 1 for conclusions\\nusing these modes of reasoning; thus only deductive reasoning applies here.',\n",
       "  '68 S. Bringsjord et al.\\nthe sense of the representation of the declarative knowledge and is thus satisﬁed\\nby the representation in Fig. 1.\\nWe claim that s(a)i ﬀγ(g) is true by construction. Unfortunately, given cur-\\nrent space constraints, γ(g)i ﬀμ(m) is non-trivial. Since M has a purely infer-\\nential semantics, and since SNePS allows inferences to be systematically carriedout, we prove an inference-theoretic interpretation of the biconditional by show-\\ning that given some context in which μ(m) is deduced (in the fashion of [ 6]),\\nγ(g) can be the conclusion of valid reasoning in SNePS that uses a counterpart\\nof this context. The left-to-right direction follows the same strategy. We thus\\nassert that SNePS too conforms to a minimal standard modeling of AGI-level\\nminds.\\n5 Related Work\\nCommendably, Laird et al. [ 16] launched a search for a standard model of the\\nhuman mind. But their approach and ours are starkly divergent. We have no',\n",
       "  'particular interest in the human mind or its embodiment in the form of earthlybrains, which we regard to be adventitious relative to AGI at the human level\\nand above. Nonetheless, realistically, at least philosophically speaking, there will\\nbe in the minds of some AI theorists overlap between the Lairdian approachand the approach we introduce herein, so we point out a second divergence:\\nTheir approach is informal, while ours is formal, i.e. theorem-driven. For good\\nmeasure, a third aspect of divergence is found in the fact that while we regardthe “best bet” for commonality of AGIs to be found in the arena of logic and\\nmathematics, cognition in this area is regarded by Laird et al. to be cognitively\\nrecherch´ e, which is borne out holistically by the absence of any discussion what-\\nsoever of logico-mathematical cognition in [ 16], and more speciﬁcally by the fact',\n",
       "  'that their proposed “standard model” constraints have nothing whatsoever todo with reasoning, and instead consist of the four pillars of “perception/motor”,\\n“learning,” “memory and content,” and “processing.” Reasoning, including rea-\\nsoning in connection with logico-mathematical cognition over content in formallanguages, would only perhaps arise in conception in secondary, epiphenomenal\\nfashion under the roof held up by their quartet of pillars.\\nWe suspect some readers will think that knowledge graphs and description\\nlogics are related to our proposed procedure with M. However, care must be\\ntaken when considering this kind of work.\\nIn practice, most knowledge-graph systems can be represented by a decid-\\nable description logic\\n10(e.g. by ALC,o rSHOIN , which are standards for most\\nknowledge graphs), but such logics cannot capture PA, and they cannot cap-',\n",
       "  'ture epistemic attitudes about theorems of this axiom system. The reason isthat description logics are proper fragments of ﬁrst-order logic (FOL), and thus\\ncannot express PA, which requires full FOL and is by Church’s Theorem unde-\\ncidable. Formalizing mathematics is known to require at a minimum third-order\\n10Some description logics have been discovered to be undecidable [ 20,21]. However,\\nthe core focus in the description logic community is on ﬁnding decidable fragments.',\n",
       "  'The M Cognitive Meta-architecture 69\\nlogic (M’s cognitive calculi include quantiﬁed modal third-order logic) [ 24]. What\\nthus may seem to be work related to ours is in the case of knowledge graphs anddescription logic actually not. However, our procedure can easily handle weaker,\\ndecidable theories of arithmetic, such as Presburger Arithmetic, and as a matter\\nof fact the particular sentence s(a)’s component ‘2 + 2 = 4’ is a valid statement\\nin both Peano andPresburger Arithmetic.\\n6 Objections\\nWe anticipate many objections to our new approach. We rapidly encapsulate\\nunder current space constraints two, and brieﬂy reply to each.\\n6.1 “But What About Purely Numerical Approaches to AGI?”\\nIt will be said against us: “There are approaches to rigorously capturing general\\nintelligence at the human level and above that make no reference to the axiom-\\natized declarative content of PA, let alone to the additional axiom systems to',\n",
       "  'which you implicitly refer when invoking reverse mathematics for your standard-\\nization program (e.g. see [ 13]). Your approach is hence idiosyncratic at best, and\\ntendentious at worst.”\\nIn reply, the key question is what those aiming at securing AGI via approaches\\nthat exclude the standardization we advocate will settle for when an artiﬁcialagent is challenged to demonstrate the power and accuracy of its mathematical\\ncognition. Suppose that some artiﬁcial agent purportedly not only believes that\\n2 + 2 = 4, but purportedly has command over PAoverall. The key question,\\nthen, when narrowed, is: Would purely external behavior of the right sort be\\nsuﬃcient, or must there be some underlying structures and content associated\\nwith the behavior that enable proving a connection to formulae like μ? Large\\nLanguage Models (LLMs), for example, provide an excellent context for asking\\nthis question. Suppose an LLM agent known colloquially by the name ‘Larry,’',\n",
       "  'based purely on deep learning, and thus completely bereft of any formulae thatencode members of PA\\n⊢(the closure under deduction of PA), is able to gener-\\nate not only all sorts of sentences like the sentence sfrom above, but also more\\ncomplicated ones, because saying any number-theoretic theorem is possible for\\nthis LLM. Let s′(Larry ) be “I believe that every cubic number is the sum of n\\nconsecutive odd numbers,” where the indirect indexical refers to ‘Larry.’ Andsuppose that many, many other sentences are generated by Larry on this topic,\\nwhere this generation is syntactically ﬂawless, but is by deﬁnition based exclu-\\nsively on underlying numerical data processing. Under this supposition, provinga bridging biconditional that links from the LLM Larry to formulae in M is\\nimpossible. This is an empirical fact.\\nWe see this as most unfortunate, for the simple reason that science explains\\nby virtue of ﬁnding formal theory that explains observed phenomena; physics is',\n",
       "  'the paradigmatic case in point. In the case of the LLM that is ChatGPT, the\\nempirical fact that deep formal science of the type that has always been the',\n",
       "  '70 S. Bringsjord et al.\\n“golden goal” of science is completely excluded as it is in the case of Larry, has\\nbeen noted recently by Wolfram [ 28]. Hence, the blockage by the impenetrable\\nnature of LLMs for our M-based procedure is just something we must accept,\\nwith all of rigorous science.\\n6.2 “Math is Merely Manufactured”\\nThe objection against us here can be summarized thus: “Using mathematical\\ncognition as the cornerstone of a test for standard modeling and simulation of\\nAGI-level minds bestows upon such cognition a kind of ‘ground-truth’ status.\\nBut mathematics is essentially a symbol-manipulation game legislated by humanbeings, as explained in [ 17].”\\nAs all or at least most readers will know, while the view espoused in this\\nobjection has been defended by serious scholars (e.g. [ 17]), this is by no means\\na consensus view. There are many well-known problems that aﬄict the view,\\nfor instance the apparent fact that math stunningly corresponds to the behavior',\n",
       "  'of the natural world [ 27], while formal logic has a parallel relationship with\\ncomputation [ 12]. Yet our position, in keeping with the non-partisan nature of\\nM itself, is to leave such debates to the side, in favor of simply observing that at\\nthe very least, going with mathematical cognition as a starting place for tryingto establish a plumb-line standard modeling of AGI-level minds is rational, since\\nif any part of cognition is likely to span minds in general it is mathematical\\ncognition—rather than perception, motor control, natural language usage, etc.\\n7 Conclusion and Next Steps\\nImmediate next steps include delivering full proofs of our conjectures withrespect to the NARS and SNePS, and expanding our procedure to include cogni-tive architectures beyond these two cognitive architectures. Two obvious targets\\nare Soar and ACT-R, the latter of which promises to qualify as standard by our',\n",
       "  'metrics in no small part because ACT-R has already been considered from thestandpoint of formal logic (at least at the level of ﬁrst-order logic; see [ 1,10]).\\nWe don’t know what will happen in the case of Soar.\\n11\\n11Some readers of earlier drafts of the present paper have asked us whether our pro-\\ncedure can be applied not just to cognitive architectures, but to artiﬁcial agents in\\ngeneral—for instance to the LLM agents in today’s headlines. This question, alas,\\nis at once tricky and straightforward. If the question is about pureLLMs, the ques-\\ntion is straightforward, and easily answered in the negative, since cognitive attitudes\\ndirected at declarative content qua declarative content within the theory of elemen-\\ntary arithmetic (the full closure of PAunder standard ﬁrst-order deduction) cannot\\nexist in such a system, which operates exclusively over data derived by tokenizing\\nand vectorizing etc. away from quantiﬁer-rich formula. Things become tricky when',\n",
       "  'one sees that LLMs are increasingly getting “glued” to outside intelligent systemsthat have been engineered to handle logic-based data and to reason in accordance\\nwith inference schemata that have since Aristotle been devised for processing such\\ndata.',\n",
       "  'The M Cognitive Meta-architecture 71\\nA signiﬁcant challenge awaits us when our procedure is expanded beyond\\nmathematical cognition into other parts of AGI-level cognition. We must be ableto draw from logic-based machinery to for example formalize communication\\nso that our key biconditionals can go through in this realm. The most severe\\nchallenge to our procedure will arise, we believe, in the case of robust attentionand perception, and, having devoted time to considering perception in connection\\nwith NARS (as seen above), we are studying the attention/perception-centric\\ncognitive architecture ARCADIA [ 18] now from a formal point of view.\\nAcknowledgments. We are grateful to two anonymous reviewers for their helpful\\nfeedback, to Paul Bello for insights regarding whether the search for a standard model\\nof the (human) mind can bear fruit in light of the computationally resistant nature',\n",
       "  'of consciousness, and to ONR (Award # N00014-22-1-2201) for partial support of theresearch reported herein.\\nReferences\\n1. Anderson, J.: Formal semantics of ACT representations. In: Language, Memory,\\nand Thought, pp. 220–251. Lawrence Erlbaum Associates, Hillsdale (1976)\\n2. Andr´ eka, H., Madar´ asz, J.X., N´ emeti, I., Sz´ ekely, G.: A logic road from special\\nrelativity to general relativity. Synthese 186, 633–649 (2012). https://doi.org/10.\\n1007/s11229-011-9914-8\\n3. Arkoudas, K.: ChatGPT is no stochastic parrot. But it also claims that 1 is greater\\nthan 1. Philos. Technol. (forthcoming)\\n4. Bello, P., Bringsjord, S.: Two problems aﬄicting the search for a standard model\\nof the mind. In: The 2017 Fall Symposium Series, Technical Reports FSS-17-\\n01-FSS-17-05, pp. 296–301. Association for the Advancement of Artiﬁcial Intel-ligence, Palo Alto (2017). http://kryten.mm.rpi.edu/pb\\nsbaaaifs2017 ﬁnal.pdf .\\nThis paper speciﬁcally appears in Technical Report A Standard Model of the',\n",
       "  'Mind, Laird, J., Lebiere, C., Rosenbloom, P. (eds.) A preprint of the paper can be\\nobtained via the URL given here\\n5. Boolos, G.S., Burgess, J.P., Jeﬀrey, R.C.: Computability and Logic, 4th edn. Cam-\\nbridge University Press, Cambridge (2003)\\n6. Bringsjord, S., Hendler, J., Govindarajulu, N.S., Ghosh, R., Giancola, M.: The\\n(uncomputable!) meaning of ethically charged natural language, for robots, andus, from hypergraphical inferential semantics. In: Ferreira, M.I.A., Tokhi, M.O.\\n(eds.) Towards Trustworthy Artiﬁcial Intelligent Systems. ISCA, vol. 102, pp.\\n143–167. Springer, Cham (2022). https://doi.org/10.1007/978-3-031-09823-9\\n11.\\nhttp://kryten.mm.rpi.edu/UncomputableNLURobots032421.pdf\\n7. Bringsjord, S., Govindarajulu, N., Giancola, M.: Automated argument\\nadjudication to solve ethical problems in multi-agent environments. Pal-adyn J. Behav. Robot. 12, 310–335 (2021). http://kryten.mm.rpi.edu/\\nAutomatedArgumentAdjudicationPaladyn071421.pdf',\n",
       "  '8. Bubeck, S., et al.: Sparks of artiﬁcial general intelligence: early experiments with\\nGPT-4 (2023). arXiv:2303.12712v3\\n9. Ebbinghaus, H.D., Flum, J., Thomas, W.: Mathematical Logic, 2nd edn. Springer,\\nNew York (1994). https://doi.org/10.1007/978-1-4757-2355-7',\n",
       "  '72 S. Bringsjord et al.\\n10. Gall, D., Fr¨ uhwirth, T.: A formal semantics for the cognitive architecture ACT-R.\\nIn: Proietti, M., Seki, H. (eds.) LOPSTR 2014. LNCS, vol. 8981, pp. 74–91.\\nSpringer, Cham (2015). https://doi.org/10.1007/978-3-319-17822-6 5.http://\\nwww.informatik.uni-ulm.de/pm/ﬁleadmin/pm/home/fruehwirth/drafts/act r\\nsemantics.pdf\\n11. Goldfain, A.: A computational theory of early mathematical cognition. Ph.D. the-\\nsis, State University of New York at Buﬀalo (2008)\\n12. Halpern, J., Harper, R., Immerman, N., Kolaitis, P., Vardi, M., Vianu, V.: On the\\nunusual eﬀectiveness of logic in computer science. Bull. Symb. Log. 7(2), 213–236\\n(2001)\\n13. Hutter, M.: Universal Artiﬁcial Intelligence: Sequential Decisions Based on Algo-\\nrithmic Probability. Springer, Heidelberg (2005). https://doi.org/10.1007/b138233\\n14. Kissinger, H., Schmidt, E., Huttenlocher, D.: ChatGPT heralds an intellectual',\n",
       "  'revolution. Wall Street J. (2023). The rather lengthy subtitle of this article reads:“Generative AI presents a philosophical and practical challenge on a scale not\\nexperienced since the start of the Enlightenment”\\n15. Kotseruba, I., Tsotsos, J.K.: A review of 40 years of cognitive architecture research:\\ncore cognitive abilities and practical applications. Artiﬁcial Intelligence. arXiv\\n(2016)\\n16. Laird, J., Lebiere, C., Rosenbloom, P.: A standard model of the mind: toward a\\ncommon computational framework across artiﬁcial intelligence, cognitive science,\\nneuroscience, and robotics. AI Mag. 38(4), 13–26 (2017). https://doi.org/10.1609/\\naimag.v38i4.2744\\n17. Lakoﬀ, G., Nu˜ nez, R.: Where Mathematics Comes From: How the Embodied Mind\\nBrings Mathematics into Being. Basic Books, New York (2000)\\n18. Lovett, A., Bridewell, W., Bello, P.: Selection, engagement, & enhancement: a\\nframework for modeling visual attention. In: Proceedings of the 43rd Annual Con-',\n",
       "  'ference of the Cognitive Science Society, pp. 1893–1899. Cognitive Science Society,\\nVienna (2021)\\n19. McKinsey, J., Sugar, A., Suppes, P.: Axiomatic foundations of classical particle\\nmechanics. J. Ration. Mech. Anal. 2, 253–272 (1953)\\n20. Patel-Schneider, P.F.: Undecidability of subsumption in NIKL. Artif. Intell. 39(2),\\n263–272 (1989)\\n21. Schmidt-Schauß, M.: Subsumption in KL-ONE is undecidable. In: KR 1989, pp.\\n421–431 (1989)\\n22. Shapiro, S.: SNePS: a logic for natural language understanding and commonsense\\nreasoning. In: Iwanska, L., Shapiro, S. (eds.) Natural Language Processing and\\nKnowledge Representation: Language for Knowledge and Knowledge for Language,pp. 175–195. AAAI Press/MIT Press, Menlo Park (2000)\\n23. Shapiro, S.C., Bona, J.P.: The GLAIR cognitive architecture. In: Biologically\\nInspired Cognitive Architectures II: Papers from the AAAI Fall Symposium (2009)\\n24. Simpson, S.: Subsystems of Second Order Arithmetic, 2nd edn. Cambridge Uni-\\nversity Press, Cambridge (2010)',\n",
       "  '25. Wang, P.: Non-Axiomatic Logic. World Scientiﬁc (2013). https://doi.org/10.1142/\\n8665.https://www.worldscientiﬁc.com/doi/abs/10.1142/8665\\n26. Wang, P.: Perception in NARS. Technical report (2018). https://cis.temple.edu/\\ntagit/publications/PAGI-TR-7.pdf\\n27. Wigner, E.: The unreasonable eﬀectiveness of mathematics in the natural sciences.\\nIn: Communications in Pure and Applied Mathematics, vol. 13, pp. 1–14. Wiley,New York (1960)',\n",
       "  'The M Cognitive Meta-architecture 73\\n28. Wolfram, S.: What Is ChatGPT Doing ...and Why Does It Work? Stephen Wol-\\nfram - Writings, 14 February 2023. https://writings.stephenwolfram.com/2023/02/\\nwhat-is-chatgpt-doing-and-why-does-it-work',\n",
       "  'Causal Reasoning over Probabilistic\\nUncertainty\\nLeonard M. Eberding1(B)and Kristinn R. Th´ orisson1,2\\n1Center for Analysis and Design of Intelligent Agents,\\nReykjav´ ık U., Menntavegur 1, Reykjav´ ık, Iceland\\n{leonard20,thorisson }@ru.is\\n2Icelandic Institute for Intelligent Machines, Reykjav´ ık, Iceland\\nAbstract. A system deployed in the real world will need to handle\\nuncertainty in its observations and interventions. For this, we present\\nan approach to introduce uncertainty of state variables in causal rea-soning using a constructivist AI architecture. Open questions of how\\nnoisy data can be handled and intervention uncertainty can be repre-\\nsented in a causal reasoning system will be addressed. In addition, wewill show how handling uncertainty can improve a system’s planning and\\nattention mechanisms. We present the reasoning process of the system,',\n",
       "  'including reasoning over uncertainty, in the form of a feed-forward algo-rithm, highlighting how noisy data and beliefs of states can be included\\nin the process of causal reasoning.\\nKeywords: General Machine Intelligence\\n·Reasoning ·Uncertainty\\n1 Introduction\\nA major challenge in artiﬁcial intelligence (AI) research is the development of\\nsystems that can be deployed in the real world and can autonomously adapt\\nto changing circumstances. Additionally, human designers want these systemsto be able to generate explanations about why certain interactions were chosen\\nand how the expected state transitions lead to the goal. While deep learning has\\nshown massive advances in the ﬁeld of data processing and identiﬁcation of corre-lated data points, it still lacked to produce a system that is able to adapt to novel\\ncircumstances and generate satisfactory explanations [ 1,2,9]. Reasoning systems,',\n",
       "  'on the other hand, show promising results in both adaptation and explanationgeneration but often lack the ability to reason over noisy and erroneous data,\\nmaking deployment in the real world practically impossible, especially when it\\ncomes to low-level sensor and actuator precision.\\nThis handling of uncertainty is at the center of research in the ﬁeld of prob-\\nabilistic robotics. Under the assumption of suﬃciently described mathemati-\\ncal models of the system under control, probabilistic approaches to uncertainty\\nThis work was funded in part by the Icelandic Research Fund (IRF) (grant number\\n228604-051) and a research grant from Cisco Systems ,U S A .\\nc⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2023\\nP. Hammer et al. (Eds.): AGI 2023, LNAI 13921, pp. 74–84, 2023.https://doi.org/10.1007/978-3-031-33469-6\\n_8',\n",
       "  'Causal Reasoning over Probabilistic Uncertainty 75\\ndescriptions are used to handle sensor noise and actuator imprecision. The causal\\nreasoning architecture AERA (Autocatalytic Endogenous Reﬂective Architec-ture), on the other hand, is designed to extract mathematical models of the\\nenvironment and the system under control through observation and interven-\\ntion. By fusing the two approaches, we present a way to overcome limitations inboth ﬁelds. Learning the transition functions overcomes the necessity to model\\nall dynamics in advance as is done in probabilistic robotics. On the other hand,\\nthe limitation to deterministic data streams is lifted from the reasoning system.\\nPurely probabilistic approaches as used in many Bayesian adaptive methods\\ndo not suﬃce for operation in ever-changing environments, as changes in the\\ndistributions of state transitions represented by the probabilistic modeling of theenvironment can lead to incorrect outcome predictions and erroneous decision',\n",
       "  'making. Instead, we overcome this covariate shift problem [ 6] by building on\\nhypothesized cause-eﬀect patterns representing invariant world mechanisms.\\nTo address the uncertainty of the in- and output streams, a knowledge repre-\\nsentation is needed to enable deductive and abductive reasoning over uncertaintydistributions without losing the inspectability of the reasoning processes. Using\\ncausal models to describe the state transition itself, we accompany causal models\\nwith new probabilistic models that describe the belief propagation under cause-eﬀect structures. Our framework enables reasoning systems to support noisy\\ndata streams in both in- and output. Further, we show how such a probabilistic\\nrepresentation can help in describing variables to which the system needs to payclose attention and which do not need to be monitored as closely.\\nThe paper is structured as follows: In the next section, we present related',\n",
       "  'work, focusing on reasoning systems and the transferred principles from proba-bilistic robotics. In Sect. 3, we present the applied methodology, including a more\\nin-depth analysis of the model structures used in AERA, assumptions that were\\ntaken for this work, and the novel approach of including probabilistic modelsin the reasoning process. Section 4provides a (simpliﬁed) algorithm describing\\nhow abductive and deductive reasoning is done. Lastly, in Sect. 5, we discuss our\\napproach in context and how the approach can produce a robust implementation\\nof causal reasoning over probabilistic uncertainty.\\n2 Related Work\\nThe Autocatalytic Endogenous Reﬂective Architecture (AERA) [ 5] follows the\\nconstructivist approach to AI, meaning that the system’s knowledge base is self-\\nconstructing through experiences [ 7,8]. All knowledge in AERA is non-axiomatic\\nand can be disproven at any point during its lifetime. The reasoning is done on',\n",
       "  'causal models, each representing simple, linearized changes in the environment[7,8]. Coming from the cybernetics side, AERA is designed such that direct\\ncontrol of low-level variables is within its capabilities. Being able to extract\\nlinear equation systems from its environment by applying pattern matching onobservations and interventions provides valuable functionality for AERA to be\\nused in robotics applications.',\n",
       "  '76 L. M. Eberding and K. R. Th´ orisson\\nOther reasoning systems exist that include falsiﬁability of their knowledge\\nbase, leading to probabilistic and non-axiomatic approaches. One example of thisis the Non-Axiomatic Reasoning System (NARS) [ 11,12]. NARS works under the\\nassumption of insuﬃcient knowledge and resources (AIKR) and is able to reason\\nabout non-axiomatic truth statements which can be disproven at any time. Thus,the reasoning process includes the uncertainty of truths. Approaches like the\\nOpen-NARS-for-Applications (ONA) [ 4] provide a framework that can be used\\nin some robotics applications. Another approach to represent uncertainty in thereasoning process are Probabilistic Logic Networks (PLNs) [ 3]. The ﬁrst devel-\\nopment of PLNs was inﬂuenced by NARS but has been extended and nowadays\\ndiﬀers from the NARS logic considerably. Based on term logic for inference, PLNscan be used to apply logic operators on probabilistic descriptions of truth values',\n",
       "  'and infer rules through de-, ab-, and inductive reasoning [ 3]. These approaches,\\nhowever, do not include probabilistic reasoning over the belief of environment\\nstates. Instead, they are more similar to fuzzy logic systems in their description\\nof the uncertainty of the truth of statements.\\nA major problem is posed by the noisiness and uncertainty of sensors and\\nactuators. No clear, deterministic information exists for the system to reason\\nabout. Instead, it is necessary to model the uncertainty of data used in thereasoning process. Probabilistic robotics provides a framework for how to model\\nthis uncertainty using Bayesian inference [ 10]. Uncertainty can be described,\\npropagated, and updated by applying Bayesian statistics and ﬁlter methods. Forthis, the designers of the system deﬁne the state-transition functions in advance,\\nwhich will be applied during run-time. These functions are used to predict new',\n",
       "  'states and observations. Additionally, the uncertainty of these predictions canbe calculated as well and updated as new observations come in [ 10].\\nIn probabilistic robotics, all state-transition functions must be implemented\\nby the designer. We, on the other hand, aim to use Bayesian inference whileapplying learned causal models. Prediction of future changes to the environment\\nthus becomes a chaining of diﬀerent transition functions learned by the system,\\nincluding the prediction of uncertainty.\\n3 Methodology\\nIn the following, we provide a deeper insight into the methodology of causalreasoning applied in the AERA architecture and OpenAERA in particular before\\nintroducing the novel methodology of including probabilistic uncertainty into the\\nreasoning process.\\n3.1 Autocatalytic Endogenous Reﬂective Architecture - AERA\\nEach process observed by OpenAERA is modeled by generating multiple mod-\\nels\\n1. These models can be classiﬁed as (anti-) requirement models and causal',\n",
       "  '1Seehttps://openaera.org –accessed Apr. 6, 2023.',\n",
       "  'Causal Reasoning over Probabilistic Uncertainty 77\\nmodels, which include command models ,a n d reuse models . Independent of their\\ntype, each causal model represents a left-hand-side (LHS) pattern, a right-hand-side (RHS) pattern, a function that maps LHS to RHS, a function that maps\\nRHS on LHS, and a time interval for which the model holds.\\nRequirement models describe the constraints under which any causal\\nmodel (either command or reuse model) is applicable. It gives context to the rea-\\nsoning system by providing a way that connects observations (or predictions) on\\nthe LHS with the instantiation of a causal model on the RHS. This way, the sizeof the search problem of identifying suitable causal models can be reduced. Anti-\\nrequirement models , on the other hand, represent conditions under which a\\ncausal model does nothold. They represent strong requirements and describe\\nhard constraints on the task-environment.\\nCommand models are used to model the direct inﬂuence on the environ-',\n",
       "  'ment of executed commands. The LHS of command models is always a command\\navailable to the system. The RHS is the change of the environment if the com-\\nmand is executed. Two functions are included in the model. One function is usedfor forward chaining; it is used to calculate the RHS given the control input and\\nvariables passed from the requirement model. The other function is used for\\nbackward chaining, representing the inverse of the forward chaining function.For example, a move command which changes the position of OpenAERA in the\\nenvironment consists of 1) the move command and the associated control input\\non the LHS, 2) the new position after execution on the RHS, 3) a function thatcalculates the new position given the old position and the control input, and 4)\\nthe inverse of the ﬁrst function, which can be used to calculate the control input\\ngiven the current position and the goal position.\\nReuse models are used to model similar transitions without an intercon-',\n",
       "  'nection of state variables. As each model is supposed to model only a very\\nsmall number of variables to make a reﬂection on said models possible, reusemodels are used to model more complex behaviors. Reuse models have their\\nown requirement models such that complex constraints on complex environment\\nstate transitions can be modeled by matching LHS patterns rather than creating\\nmassive models responsible for a multitude of calculations. Such a reuse model\\ncould, for example, be used to model the changes in an object’s state that Ope-nAERA is holding when a move command is executed. Instead of generating\\na single model representing the full state change of the system and the object\\nmoving simultaneously the same distance, two models are used. The move com-mand model, as previously shown, and a reuse model with its own requirement\\nmodel.\\nThese models are used to create chains of possible state transitions from the\\ngoal to the current state (backward) and from the current state to the goal',\n",
       "  '(forward). OpenAERA is thus able to reason about possible paths to reach\\nthe goal by using causal models to represent predicted state changes in theenvironment.',\n",
       "  '78 L. M. Eberding and K. R. Th´ orisson\\n3.2 Background Assumptions\\nThe following are assumptions underlying the present approach.\\n1.Linearity : One of the underlying architectural concepts of OpenAERA is the\\nstep-wise linearization of observed transition functions. Therefore, we assume\\nlinearity in all causal models and their uncertainty propagation.\\n2.Continuity of variables : All variables under investigation are assumed con-\\ntinuous in the state space. Reasoning over error-prone non-continuous vari-\\nables is not part of this work. Other approaches to non-axiomatic reasoningexist for this matter, including other pattern matching and function approx-\\nimations within OpenAERA.\\n3.Normal distribution : For the sake of simpliﬁcation, we assume a Gaussian\\ndistribution of the measurement and actuator uncertainty. This assumption\\ncan be overcome by applying other means to predict uncertainty.\\n4.Observation of state variables : All variables are directly measurable.',\n",
       "  '3.3 Modeling of Probabilistic Uncertainty\\nAny artiﬁcial general intelligence (AGI) aspiring system must adapt to novel\\ncircumstances to reach a goal/ fulﬁll a drive under the assumption of insuﬃcientknowledge and resources (AIKR) [ 12]. This means that it needs to autonomously\\nadapt its resource consumption by paying attention to import variables of the\\ntask-environment that could inﬂuence the reaching of the goal. This includespaying attention to variables/phenomenons that inﬂict disturbances on the con-\\ntrol problem of transforming the current state to the goal state.\\nBy applying well-known principles from probabilistic robotics, a new model\\ntype in OpenAERA will allow it to predict errors in state transitions coming\\nfrom sensor and actuator imprecision. Its existing attention algorithms are then\\nextended to take into account variables prone to diverge from desired valuesdue to imprecise interventions on the environment. By estimating the posteriori',\n",
       "  'belief of each subsequent state that should be reached during task performance,\\nmatching posteriori with a-priori beliefs, and calculating possible overlaps, the\\nsystem can predict plan divergences. Preemptive measures can then be taken by\\nconstraining the control input to achieve intermediate states with a low proba-bility of divergence from the original plan. Probabilistic models work as follows:\\nIf, in forward chaining, any causal model represents a noiseless linear function\\nx\\nk=Fx k−1+Cu k−1 (1)\\nwith xbeing the n-dimensional state vector consisting of values of the set of\\nvariables V={v1,v2, ..., v n},Cthe control matrix and uk−1the control input.\\nThe accompanying probabilistic model represents the propagation of uncer-\\ntainty if the model is applied:\\nPk=FP k−1FT+CP control CT+Qk (2)',\n",
       "  'Causal Reasoning over Probabilistic Uncertainty 79\\nwith the a priori belief Pk−1of the state x, the posteriori belief Pk, the noise\\ndistribution of the command Pcontrol and the process noise Qk−1.\\nThese models are attached to all causal models. The current belief Pk−1is\\ndependent on the source of the input. At the beginning of the reasoning chain,\\nPk−1represents the noise of the sensor whose observation led to the instantiation\\nof the model. If the input data comes from a prediction, on the other hand, Pk−1\\nrepresents the uncertainty of this prediction. Thus, models instantiated further\\ndown the chain produce a higher uncertainty in their predictions.\\nAs all causal models in OpenAERA can be used for forward and backward\\nchaining, the same must hold for probabilistic models. Given the fact that the\\nmodeled function can be used in both directions, it is implicit that an inverse ofthe function exists. Each causal model includes a noiseless backward function\\nx\\nk−1=Bx k−Cu k−1 (3)',\n",
       "  'with Brepresenting the inverse function of F(F−1in most cases). The backward\\npropagation for any probabilistic model therefore becomes\\nPk−1=BP kBT−CP control CT−Qk (4)\\nThis means that the maximum uncertainty of a goal-leading state can be\\ncalculated, providing information about the necessary precision of interventions.\\nProcess noise provides another opportunity to optimize causal reasoning\\nsystems using uncertainty. While there exists a trade-oﬀ in most robotics appli-\\ncations when choosing the process noise, it can be useful when applied in alearning system. When, for example, looking at Kalman Filters, it is important\\nfor the designers to choose an appropriate process noise. Too low process noise\\ncan lead to the ﬁlter ignoring rapid deviations from the expected outcome, andtoo high process noise makes the ﬁlter too sensitive to noisy environments.\\nIn the case of OpenAERA, we can assume low process noise and see devia-',\n",
       "  'tions from the expected outcome as faulty causal models. Expected and observedoutcomes should only diverge rapidly if the instantiated model does not reﬂect\\nthe dynamics of the observed system. This information can be used to generate\\nnew hypotheses about the true dynamics, leading to new models that describethe system better. We, therefore, neglect the estimation of process noise in this\\nwork and will extend it to include the identiﬁcation of described changes to\\ncausal models in the future.\\n4 Reasoning Algorithm\\nIn the following section, we give a deeper insight into the reasoning algorithm\\nused in OpenAERA and show how uncertainty propagation can be included.\\nWe focus on the forward and backward chaining processes used in the planningof control sequences leading to the goal. Backward chaining (abductive reason-\\ning) is used to constrain the search space to relevant models. Forward chaining\\n(deductive reasoning) produces an executable series of commands, representing',\n",
       "  '80 L. M. Eberding and K. R. Th´ orisson\\na plan of interventions to reach the goal state g. Both backward and forward\\nchaining - excluding the uncertainty propagation - is already implemented inOpenAERA.\\nInput:\\n– Current set of observed variables V\\nobservable at time t0and their values\\ndescribing the state x0and their uncertainty P0.\\n– A goal described as a sub-state of all observable variables such that Vgoal⊆\\nVobservable with a certain value assigned to them at a certain time tgsuch\\nthat xtg=gwith a maximum uncertainty Ptg=Pg\\n– The set of requirement and anti-requirement models M req, as well as the set\\nof causal command and reuse models M causal known by the system.\\n– The set of probabilistic models M probwhich accompany the causal models.\\nBackward chaining is the depth-ﬁrst search of possible paths from the goal to\\nthe current state using causal models and their requirements. Backward chaining\\ngoes back through time, starting at the time at which the goal is supposed to',\n",
       "  'be reached tgand stepping backward until the current time t0is reached.\\n1. Create a new, empty set of goal requirements Greqto be ﬁlled.\\n2. Create a new, empty set of currently instantiable causal models M causal,t 0\\n3. For each requirement model mreq∈M req:\\n–I f mreqcan be instantiated with the current set of observations - i.e.,\\nall left-hand-side (LHS) variables can be bound to currently observablevariables and fulﬁll all conditions of m\\nreq:\\nAdd the instantiation of the causal model on the right-hand-side (RHS)\\nofmreqto the set of currently instantiable causal models M causal,t 0\\n4. Identify all causal models whose set of right-hand-side variables VRHSover-\\nlaps that of the set of goal variables Vgoalsuch that VRHS∩Vgoal̸=∅and\\ncreate a set from the identiﬁed model M′\\ncausal with M′\\ncausal ⊆M causal.\\n5. For each model mcausal ∈M′\\ncausal:\\n(a) If mcausal ∈M causal,t 0:\\nContinue loop.\\n(b) Bind all variables of mcausal and its accompanying probabilistic model',\n",
       "  'mprob∈M probthat are part of gto the value of that variable in g.L e a v e\\nother variables unbound to be ﬁlled during forward chaining.\\n(c) Make the instantiation of mcausal under mprobwith the bound variables\\na goal requirement greqand add it to Greq.\\n(d) Identify all requirement models which have the instantiation of mcausal\\non their RHS, creating a subset M′\\nreq⊆M req.\\n(e) For each requirement model mreq∈M′\\nreq:\\ni. Make instantiating the LHS of mreqa sub-goal gsubwith the uncer-\\ntainty of the accompanying probabilistic model mprobasPgsub.\\nii. Set Gtogsuband start recursion from 4.\\n6.Return the set of bounded goal requirements Greq.',\n",
       "  'Causal Reasoning over Probabilistic Uncertainty 81\\nForward chaining provides the deductive reasoning process in which a series\\nof commands is identiﬁed that leads to the fulﬁllment of the goal requirementsgenerated during backward chaining. Forward chaining starts at time t\\n0(”now”)\\nand moves forward through time, generating predictions of outcomes of causal\\nmodels.\\n1. Create a new, empty set of control vectors Uto be executed during the task\\nperformance, which will be ﬁlled during forward chaining.\\n2. Set the current set of observations as the input Ito the system.\\n3. Create a new set of models M′\\nreqof all requirement models that can be instan-\\ntiated with Iby identifying all models whose LHS variable values’ likelihood\\ngiven the observations’ uncertainty in Iis higher than a threshold.\\n4. For each requirement model mreq∈M′\\nreq:\\n1. Check if the instantiation of the RHS causal model mcausal of mreq\\nmatches one of the goal requirements identiﬁed in backward chaining.',\n",
       "  '2. If mcausal ∈Greq:\\n1. Use the backward function of mcausal to calculate the control input\\nnecessary to transition the state from the LHS to the desired RHS\\nby binding all variables of the LHS ( xk−1) to the values in Iand the\\nvariables of the RHS ( xk) to the values of the goal requirement:\\nCu k−1=Bx k−xk−1\\n2. If mcausal can be instantiated given Iand the control input:\\n1. Apply the forward function of mcausal to generate a prediction of\\nthe state change:\\nxk=Fx k−1+Cu k−1\\n2. Apply the forward function of the accompanying probabilistic\\nmodel to calculate an expected uncertainty after the state change:\\nPk=FP k−1FT+CP control CT+Qk\\n3. Check whether the uncertainty of goal-related variables in the\\ncalculated Pkis smaller than the maximum uncertainty deﬁned\\nin the goal requirement Pg,req.\\n4. If the expected uncertainty is larger:\\nPlan intermediate observations by reducing the magnitude and\\nduration of uk−1thus reducing the actuator noise described by',\n",
       "  'CP control CTand allowing for more observations during command\\nexecution.\\n5. Set the RHS of mcausal as a new predicted observation in I.\\n6. Add the control uk−1toU.\\n5.Return U.\\nThe generated plan thus consists of a set of commands U, each assigned to a cer-\\ntain time period and given a set of input variables that must match observations\\nat this time to perform the control with the expected outcome.\\nAnti-requirement Models: A special focus must be put on anti-requirement\\nmodels during the reasoning process. Anti-requirement models constrain the\\nsolution space of the task by describing states under which a causal model may',\n",
       "  '82 L. M. Eberding and K. R. Th´ orisson\\nnot be applied. (The evaluation of anti-requirement models was left out in the\\nprevious description of the algorithm for readability.) Anti-requirement mod-els play a role in both backward and forward chaining: In backward chaining,\\nwhen identifying relevant requirement models (step 5d), anti-requirement mod-\\nels are identiﬁed as well, and a goal requirement to notinstantiate the LHS of\\nthe anti-requirement model is generated. In forward chaining, these anti-goal-\\nrequirements are in turn evaluated. If instantiating a causal model produces\\nvariables that are part of an anti-requirement, the likelihood of the instantiationof the anti-requirement model, given the produced uncertainty of the predic-\\ntion, is calculated. If this likelihood is over a given threshold, there are three\\noptions: (1) The system can choose an alternative path, if available; (2) Themagnitude and duration of control inputs can be reduced to minimize the prob-',\n",
       "  'ability of instantiating the anti-requirement model; or (3) abort the current plan\\nand redo the abductive backward chaining process with the assumption that the\\nmodel chain in question will not lead to the goal.\\nFig. 1. Task of moving along a constraint space. Left: Visualization of the task with\\ninitial state at time t0, goal state at time t3. Red areas: forbidden areas where the\\nsystem may not move; white circles: initial position at time t0and after applying\\nthe models M xat time t0and M yat time t1; red circles: examples of failures of the\\ntask if the action of M xis executed imprecisely. Right: Causal models M xand M y\\nand their accompanying probabilistic models M prob,x and M prob,y , respectively. (Color\\nﬁgure online)\\nFigure 1shows a very simplistic task of moving along a constrained space\\n(e.g., a robot moving in a constraint work area). As can be seen, the imprecisionof executing model M\\nxcan lead to task failure. The system can identify this',\n",
       "  'during forward chaining, decrease the time during which the command of M x\\nis executed and thus reduce the impact of actuator imprecision. Executing the\\ncommand repeatedly with intermediate observations to adjust the duration of\\nthe next command can overcome possible failure states.',\n",
       "  'Causal Reasoning over Probabilistic Uncertainty 83\\n5 Discussion and Future Work\\nWe have presented a new approach that extends causal reasoning to address\\nerroneous noisy data in the input and output stream of a controller, and shown\\nhow this is to be implemented in OpenAERA. The resulting reasoning process\\ncan better predict possible outcomes of planned interventions to adjust its plans.\\nThe limitation to normal-distributed data can be lifted by changing the\\nuncertainty estimation and propagation process. For example, by using neu-\\nral networks to estimate the probability distributions given observations and\\ninterventions. However, it remains future work how the full reﬂectability andexplainability of AERA in such approaches.\\nAside from the application to noisy data, this approach can further be\\nextended to use divergences between predictions and actual observations toenhance the causal discovery process. Detected outliers imply erroneous causal',\n",
       "  'models, which can be corrected through self-reﬂection mechanisms in AERA.\\nReferences\\n1. Eberding, L.M.: Comparison of machine learners on an ABA experiment format of\\nthe Cart-Pole Task. In: International Workshop on Self-Supervised Learning, pp.\\n49–63. PMLR (2022)\\n2. Eberding, L.M., Th´ orisson, K.R., Sheikhlar, A., Andrason, S.P.: SAGE: task-\\nenvironment platform for evaluating a broad range of AI learners. In: Goertzel,B., Panov, A.I., Potapov, A., Yampolskiy, R. (eds.) AGI 2020. LNCS (LNAI),\\nvol. 12177, pp. 72–82. Springer, Cham (2020). https://doi.org/10.1007/978-3-030-\\n52152-3\\n8\\n3. Goertzel, B., Ikl´ e, M., Goertzel, I.F., Heljakka, A.: Probabilistic Logic Networks:\\nA Comprehensive Framework for Uncertain Inference. Springer, New York (2008).https://doi.org/10.1007/978-0-387-76872-4\\n4. Hammer, P., Lofthouse, T.: ‘OpenNARS for applications’: architecture and control.',\n",
       "  'In: Goertzel, B., Panov, A.I., Potapov, A., Yampolskiy, R. (eds.) AGI 2020. LNCS(LNAI), vol. 12177, pp. 193–204. Springer, Cham (2020). https://doi.org/10.1007/\\n978-3-030-52152-3\\n20\\n5. Nivel, E., et al.: Bounded recursive self-improvement. arXiv preprint\\narXiv:1312.6764 (2013)\\n6. Sheikhlar, A., Eberding, L.M., Th´ orisson, K.R.: Causal generalization\\nin autonomous learning controllers. In: Goertzel, B., Ikl´ e, M., Potapov, A.\\n(eds.) AGI 2021. LNCS (LNAI), vol. 13154, pp. 228–238. Springer, Cham (2022).https://doi.org/10.1007/978-3-030-93758-4\\n24\\n7. Th´ orisson, K.R.: A new constructivist AI: from manual methods to self-\\nconstructive systems. In: Wang, P., Goertzel, B. (eds.) Theoretical Foundationsof Artiﬁcial General Intelligence, pp. 145–171. Springer, Paris (2012). https://doi.\\norg/10.2991/978-94-91216-62-6\\n9\\n8. Th´ orisson, K.R.: Machines with autonomy & general intelligence: which methodol-\\nogy? In: Proceedings of the Workshop on Architectures for Generality and Auton-',\n",
       "  'omy (2017)\\n9. Th´ orisson, K.R.: The ‘explanation hypothesis’ in general self-supervised learning.\\nIn: Proceedings of Machine Learning Research, vol. 159, pp. 5–27 (2021)',\n",
       "  '84 L. M. Eberding and K. R. Th´ orisson\\n10. Thrun, S.: Probabilistic robotics. Commun. ACM 45(3), 52–57 (2002)\\n11. Wang, P.: Non-Axiomatic Reasoning System: Exploring the Essence of Intelligence.\\nIndiana University (1995)\\n12. Wang, P.: Rigid Flexibility: The Logic of Intelligence, vol. 34. Springer, Dordrecht\\n(2006)',\n",
       "  'Probabilistic Logic Networks\\nfor Temporal and Procedural Reasoning\\nNil Geisweiller(B)and Hedra Yusuf(B)\\nSingularityNET Foundation, Amsterdam, The Netherlands\\n{nil,hedra }@singularitynet.io\\nAbstract. Probabilistic Logic Networks (PLN) oﬀers an excellent the-\\nory to frame learning and planning as a form of reasoning. This paperoﬀers a complement to the seminal PLN book [ 3], in particular to its\\nChapter14 on temporal and procedural reasoning, by providing formal\\ndeﬁnitions of temporal constructs, as well as inference rules necessary tocarry temporal and procedural reasoning.\\nKeywords: Temporal Reasoning\\n·Procedural Reasoning ·\\nProbabilistic Logic Networks\\n1 Introduction\\nThis paper builds upon the Chapter 14 of the Probabilistic Logic Networks\\nbook [ 3], adding and modifying deﬁnitions along the way to provide, we believe,\\na better foundation for carrying temporal and procedural reasoning with PLN.\\nAs we have found, even though the chapter is well written and conveys the con-',\n",
       "  'ceptual ideas with clarity, it leaves some formal deﬁnitions out. In addition theEvent Calculus [ 8] is intermingled with the deﬁnitions of sequential connectors\\nin, what we consider to be, an arbitrary and inﬂexible manner. On the contrary,\\nhere we leave Event calculus aside, with the intention to re-introduce it in thefuture as a separate layer standing on top of the new deﬁnitions.\\nAlthough this paper is theoretical, the work presented here is motivated by\\npractice, and has taken place in the context of developing a system for controllingan agent in uncertain environments while relying on temporal and procedural\\nreasoning for both learning and planning [ 2].\\n2 Probabilistic Logic Networks Recall\\nPLN stands for Probabilistic Logic Networks [3]. It is a mixture of predicate\\nand term logic that has been probabilitized to handle uncertainty. Inference\\nrules can operate on direct evidence, or indirect evidence by combining existing',\n",
       "  'relationships to introduce new ones. As such it is well suited for building a modelof an environment, and planning in it. All it needs then is to be properly equipped\\nwith a vocabulary for representing and manipulating temporal and procedural\\nknowledge.\\nc⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2023\\nP. Hammer et al. (Eds.): AGI 2023, LNAI 13921, pp. 85–94, 2023.https://doi.org/10.1007/978-3-031-33469-6\\n_9',\n",
       "  '86 N. Geisweiller and H. Yusuf\\n2.1 Elementary Notions\\nGraphically speaking, PLN statements are sub-hypergraphs1made of Links and\\nNodes , called Atoms , decorated with Truth Values . Syntactically speaking, PLN\\nstatements are not very diﬀerent from statements expressed in another logic,\\nexcept that they are usually formatted in preﬁxed-operator with a tree-style\\nindentation to emphasize their graphical nature and to leave room for theirtruth values. For instance\\nImplication ⟨TV⟩\\n𝑃\\n𝑄\\nrepresents an implication link between 𝑃and𝑄with truth value TV. For the\\nsake of conciseness we also introduce some notations. First, we adopt a ﬂattened,as opposed to a tree-style, representation. For instance the implication link above\\nis represented as\\nImplication (𝑃,𝑄)⟨TV⟩\\nSecond, we introduce a more mathematically looking symbolic representation.\\nFor instance, that same implication can be represented as\\n𝑃→𝑄≞TV\\nThere is a large variety of constructs in PLN. Here, we will focus primarily on',\n",
       "  'the ones for handling predicates. Let us recall that predicates are functions that\\noutput Boolean values. The domain of a predicate can be arbitrarily deﬁned,\\nbut its range is always Boolean. In this paper, the letters 𝑎,𝑏,𝑐represent atoms\\nof any type, 𝑥,𝑦,𝑧represent atoms that are variables, while the capital letter\\n𝑃,𝑄,𝑅represent atoms that are predicates, thus typed as follows:\\n𝑃,𝑄,𝑅,... ∶Domain↦{True,False}\\nNote that in PLN, predicates are not necessarily crisp because their outputs can\\nbe totally or partially unknown, thus potentially measured by probabilities, or\\nto be precised Truth Values .\\nTruth values are, in essence, second order probability distributions, or prob-\\nabilities of probabilities. They are often described by two numbers: a strength,\\n𝑠, representing a probability, and a conﬁdence, 𝑐, representing the conﬁdence\\nover that probability. Such truth values are called Simple Truth Values and are\\ndenoted as follows:\\n<𝑠,𝑐>',\n",
       "  'Alternatively, the strength and the conﬁdence of a simple truth value TVcan be\\ndenoted TV.𝑠and TV.𝑐respectively. Underneath, a simple truth value is a beta\\n1because links can point to links, not just nodes.',\n",
       "  'Probabilistic Logic Networks for Temporal and Procedural Reasoning 87\\ndistribution [ 1], similarly to an opinion in Subjective Logic [ 5]. The parameters\\nof the corresponding beta distribution can be obtained as follows:\\n𝛼(𝑠,𝑐)=𝛼0+𝑠.𝑐.𝑘\\n1−𝑐𝛽(𝑠,𝑐)=𝛽0+(1−𝑠).𝑐.𝑘\\n1−𝑐\\nwhere𝑘is a PLN parameter called the Lookahead ,a n d𝛼0and𝛽0are usually\\nset to 0.5 corresponding to Jeﬀreys prior. For truth values obtained from direct\\nevidence, a simple truth value makes perfect theoretical sense. For truth valuesobtained from indirect evidence, not so much, even though they are often used\\nin practice. When more precision is needed, to represent a multi-modal truth\\nvalue for instance, a mixture of simple truth values can be used. Also, throughout the paper, sometimes we may say probability , while what we really mean is\\nsecond order probability distribution .\\nBelow is a table of the constructs used in this paper with their ﬂattened and',\n",
       "  'symbolic representations, as well as precedence values to minimize parenthesis\\nusage with the symbolic representation.\\nFlattened Symbolic Precedence\\nEvaluation (𝑃,𝑎)𝑃(𝑎) 0\\nNot(𝑃) ¬𝑃 1\\nAnd(𝑃,𝑄)𝑃∧𝑄 2\\nOr(𝑃,𝑄)𝑃∨𝑄 2\\nImplication (𝑃,𝑄)𝑃→𝑄 4\\n𝑎⟨TV⟩𝑎≞TV 5\\nFor representing n-ary predicates evaluations we use 𝑃(𝑎1,...,𝑎𝑛)which may\\nbe understood as a unary predicate evaluation applied to a tuple. Let us now\\nexplain their semantics and how their truth values are to be interpreted.\\n–¬𝑃is the predicate resulting from the pointwise negation of 𝑃.\\n–𝑃∧𝑄is the predicate resulting from the pointwise conjunction of 𝑃and𝑄.\\n–𝑃∨𝑄is the predicate resulting from the pointwise disjunction of 𝑃and𝑄.\\n–𝑃(𝑎)≞TVstates that 𝑃(𝑎)outputs Truewith a second order probability\\nmeasured by TV.\\n–𝑃→𝑄≞TVstates that if 𝑃(𝑎)isTrue for some 𝑎in the domain of 𝑃,\\nthen𝑄(𝑎)isTruewith a second order probability measured by TV. In simple\\nprobability terms, it represents P𝑟(𝑄|𝑃), the conditional probability of 𝑄',\n",
       "  'knowing 𝑃2. We may also say that such implication is a conditional predicate\\nwhere𝑄,t h e implicand , is conditioned by 𝑃,t h e implicant .\\n–𝑃≞TVstates that the prevalence of 𝑃being Trueis measured by TV.\\n2To be precise, P𝑟(𝑄|𝑃)should be P𝑟(S𝑎𝑡(𝑄)|S𝑎𝑡(𝑃)),w h e r e S𝑎𝑡(𝑃)and S𝑎𝑡(𝑄)are the\\nsatisfying sets of 𝑃and𝑄respectively.',\n",
       "  '88 N. Geisweiller and H. Yusuf\\n2.2 Inference Rules\\nInferences rules are used to construct PLN statements and calculate their truth\\nvalues. They fall into two groups, direct evidence based or otherwise. Rules from\\nthe former group infer abstract knowledge from direct evidence, while rules from\\nthe latter group infer knowledge by combining existing abstractions. In total\\nthere are dozens of inference rules. For now, we only recall two, Implication\\nDirect Introduction and Deduction .\\nThe Implication Direct Introduction Rule (IDI) takes evaluations as\\npremises and produces an implication as conclusion. It can be understood as\\nan inductive reasoning rule. It is formally depicted by the following proof tree.\\n𝑃(𝑎1)≞TV𝑃\\n1𝑄(𝑎1)≞TV𝑄\\n1... 𝑃 (𝑎𝑛)≞TV𝑃\\n𝑛𝑄(𝑎𝑛)≞TV𝑄\\n𝑛(IDI)𝑃→𝑄≞TV\\nAssuming perfectly reliable direct evidence3then the resulting simple truth value\\ncan be calculated as follows:\\nTV.s=∑𝑛\\n𝑖=1𝑓∧(TV𝑃\\n𝑖.s,TV𝑄\\n𝑖.s)∑𝑛\\n𝑖=1TV𝑃\\n𝑖.sTV.c=𝑛\\n𝑛+𝑘',\n",
       "  'where𝑓∧is a function embodying a probabilistic assumption about the con-\\njunction of the events. Such function typically ranges from the product (perfectindependence) to the min(perfect overlap). Note that this inference rule takes an\\narbitrary number of premises. In practice it is not a problem as it is decomposed\\ninto two rules covering the base and the recursive cases, while storing evidenceto avoid double counting.\\nThe Deduction Rule (D) takes two implications as premises and produces a\\nthird one. It can be understood as a deductive reasoning rule. Depending on the\\nassumptions made there exists diﬀerent variations of that rule. The simplest oneis based on the Markov property\\nP𝑟(𝑅|𝑄,𝑃)= P𝑟(𝑅|𝑄)\\nwhich gives rise to the rule depicted by the following proof tree.\\n𝑃→𝑄≞TV𝑃𝑄𝑄→𝑅≞TV𝑄𝑅𝑃≞TV𝑃𝑄≞TV𝑄𝑅≞TV𝑅\\n(D)𝑃→𝑅≞TV\\nThe reader may notice that three additional premises have been added, corre-\\nsponding to the probabilities P𝑟(𝑃),P𝑟(𝑄)and P𝑟(𝑅). This is a consequence of',\n",
       "  'the Markov property. The exact formula for that variation is not recalled here\\nbut it merely derives from\\nP𝑟(𝑅|𝑃)= P𝑟(𝑅|𝑄,𝑃)× P𝑟(𝑄|𝑃)+ P𝑟(𝑅|¬𝑄,𝑃)× P𝑟(¬𝑄|𝑃)\\n3A perfectly reliable piece of evidence has a conﬁdence of 1. Dealing with unreliable\\nevidence involves using convolution products and is outside of the scope of this paper.',\n",
       "  'Probabilistic Logic Networks for Temporal and Procedural Reasoning 89\\nMore information about this derivation can be found in Chapter 5, Section 5.3\\nof [3]. Finally, one may notice that the same conclusion may be inferred by diﬀer-\\nent inference paths leading to diﬀerent truth values. How to properly aggregate\\nthese truth values is not the subject of this paper and is discussed in Chapter 5,\\nSection 5.10 of [ 3].\\n3 Temporal Probabilistic Logic Networks\\nA temporal extension of PLN is deﬁned in Chapter 14 of [ 3]. However, we have\\nfound that some deﬁnitions are ambiguous, in particular the sequential connec-\\ntors SequentialAnd and SequentialOr redeﬁned further below. Let us begin by\\ndeﬁning Temporal Predicates ,o r Fluents . Temporal predicates are regular pred-\\nicates with a temporal dimension:\\n𝑃,𝑄,𝑅,... ∶Domain×Time↦{True,False}\\nThe type of the temporal dimension, Time, could in principle be any thing that\\nhas a minimum set of requirements, such as being an ordered semigroup or such.',\n",
       "  'In practice so far, we have used integers, thus capturing a discrete notion of time.\\nNot all temporal predicates need to have a non-temporal domain, Domain .I n\\nthat case, we may simply assume that such domain is the unit type ()and ignore\\nit.\\n3.1 Temporal Operators\\nLet us deﬁne a set of temporal operators operating over temporal predicates.\\nLag and Lead are temporal operators to shift the temporal dimension of a\\ntemporal predicate. They are similar to the metric variations, 𝑃𝑛and𝐹𝑛,o ft h e\\nPastand Future operators of Tense Logic [ 7], with the distinction that they are\\napplied over temporal predicates, as opposed to Boolean modal expressions. The\\nLagoperator is formally deﬁned as follows:\\n𝐿𝑎𝑔(𝑃,𝑇)∶ =𝜆𝑥,𝑡.𝑃(𝑥,𝑡−𝑇)\\nMeaning, given a temporal predicate 𝑃, it builds a temporal predicate shifted\\nto the right by 𝑇time units. In order words, it allows to looks into the past, or\\none may say that it brings the past into the present. The Leadoperator is the\\ninverse of the Lagoperator, thus',\n",
       "  '𝐿𝑒𝑎𝑑(𝐿𝑎𝑔(𝑃,𝑇),𝑇)≡𝑃\\nand is formally deﬁned as follows:\\n𝐿𝑒𝑎𝑑(𝑃,𝑇)∶ =𝜆𝑥,𝑡.𝑃(𝑥,𝑡+𝑇)\\nIt allows to look into the future, or one may say that it brings the future into\\nthe present.',\n",
       "  '90 N. Geisweiller and H. Yusuf\\nSequentialAnd is a temporal conjunction where one of the temporal predicate\\narguments have been temporally shifted. There are at least two variations thatcan be deﬁned. A ﬁrst where the past of the ﬁrst predicate is brought into the\\npresent. A second where the future of the second predicate is brought into the\\npresent. In this paper we use the second one, formally deﬁned as\\nSequentialAnd (𝑇,𝑃,𝑄)∶ = And(𝑃,Lead(𝑄,𝑇))\\nwhich results into a temporal predicate that is Trueat time𝑡if and only if 𝑃is\\nTrueat time𝑡and𝑄isTrueat time𝑡+𝑇. Since we do not know at that point\\nwhich one of the two variations is best, in practice we have implemented both,\\nbut in this paper we settle to one for the sake of simplicity.\\nSequentialOr is a temporal disjunction where one of the temporal predicate\\narguments have been temporally shifted. Like for SequentialAnd we settle to the\\nvariation where the future of the second predicate is brought into the present,deﬁned as',\n",
       "  'SequentialOr (𝑇,𝑃,𝑄)∶ = Or(𝑃,Lead(𝑄,𝑇))\\nwhich results into a temporal predicate that is Trueat time𝑡if and only if 𝑃is\\nTrueat time𝑡or𝑄isTrueat time𝑡+𝑇.\\nPredictiveImplication is an implication where the future of the implicand has\\nbeen brought into the present, deﬁned as\\nPredictiveImplication (𝑇,𝑃,𝑄)∶ = Implication (𝑃,Lead(𝑄,𝑇))\\nresulting into a conditional predicate, that in order to be deﬁned at time 𝑡\\nrequires that 𝑃isTrueat time𝑡, and if so, is Trueat𝑡if and only if 𝑄\\nisTrue\\nat time𝑡+𝑇.\\nLet us introduce a symbolic representation for these temporal constructs with\\nprecedence values to minimize parenthesis usage.\\nFlattened Symbolic Precedence\\nLag(𝑃,𝑇)→\\n𝑃𝑇\\n1\\nLead(𝑃,𝑇)←\\n𝑃𝑇\\n1\\nSequentialAnd (𝑇,𝑃,𝑄)𝑃⩘𝑇𝑄3\\nSequentialOr (𝑇,𝑃,𝑄)𝑃⩗𝑇𝑄3\\nPredictiveImplication (𝑇,𝑃,𝑄)𝑃⇝𝑇𝑄4\\nAdditionally, we assume that ⩘𝑇and⩗𝑇are right-associative. The Lag(resp.\\nLead) operator is symbolized by an overlined arrow going to the right (resp. to\\nthe left) because it brings the past (resp. the future) into the present.',\n",
       "  'Probabilistic Logic Networks for Temporal and Procedural Reasoning 91\\n3.2 Temporal Rules\\nGiven these operators we can now introduce a number of temporal inference\\nrules.\\nThe Predictive Implication to Implication Rule (PI) takes a predictive\\nimplication as premise and produces an equivalent implication, as depicted by\\nthe following proof tree.\\n𝑃⇝𝑇𝑄≞TV(PI)\\n𝑃→←\\n𝑄𝑇\\n≞TV\\nNote that because the conclusion is equivalent to the premise, the truth values\\nmay optionally be stripped out the rule.\\n𝑃⇝𝑇𝑄(PI)\\n𝑃→←\\n𝑄𝑇\\nThe Implication to Predictive Implication Rule (IP) takes an implication\\nas premise and produces an equivalent predictive implication, as depicted, here\\nwithout truth value, by the following proof tree.\\n𝑃→←\\n𝑄𝑇\\n(IP)\\n𝑃⇝𝑇𝑄\\nThe Temporal Shifting Rule (S) takes a temporal predicate and shits its\\ntemporal dimension to the left or the right. An example of such rule is depictedby the following proof tree.\\n𝑃≞TV(S)\\n←\\n𝑃𝑇\\n≞TV',\n",
       "  'Shifting does not change the truth value of the predicate. Indeed, the prevalence\\nof being True remains the same, only the origin of the temporal dimension\\nchanges. Note however that the predicate itself changes, it is shifted. Therefore,\\nunlike for the IP and PI inference rules that produce equivalent predicates,the truth values must be included in the rule deﬁnition, otherwise the rule of\\nreplacement would incorrectly apply. There are a number of variations of that\\nrule. For the sake of conciseness we will not enumerate them all, and insteadshow one more variation over conditional predicates.\\n𝑃→𝑄≞TV(S)\\n←\\n𝑃𝑇\\n→←\\n𝑄𝑇\\n≞TV\\nThe Predictive Implication Direct Introduction Rule (PIDI) is similar\\nto the implication direct introduction rule of Sect. 2but accounts for temporal\\ndelays between evaluations. It is formalized by the following proof tree.',\n",
       "  '92 N. Geisweiller and H. Yusuf\\n(\\n𝑃(𝑎𝑖,𝑡𝑖)≞TV𝑃\\n𝑖)\\n𝑖=1,...,𝑛(\\n𝑄(𝑎𝑖,𝑡𝑖+𝑇)≞TV𝑄\\n𝑖)\\n𝑖=1,...,𝑛(PIDI)\\n𝑃⇝𝑇𝑄≞TV\\nThe truth value formula is identical to that of the implication direct introduction\\nrule. In fact, such rule can be trivially derived by combining the implication\\ndirect introduction rule, the implication to predictive implication rule and thedeﬁnition of the Leadoperator.\\nThe Temporal Deduction Rule (TD) is similar to the deduction rule of\\nSect.2but operates on predictive implications. It is formally depicted by the\\nfollowing proof tree.\\n𝑃⇝𝑇1𝑄≞TV𝑃𝑄𝑄⇝𝑇2𝑅≞TV𝑄𝑅𝑃≞TV𝑃𝑄≞TV𝑄𝑅≞TV𝑅\\n(TD)\\n𝑃⇝𝑇1+𝑇2𝑅≞TV\\nAs it turns out, the truth value formula is also identical to that of the deduction\\nrule, but the proof is not so trivial. In order to convince us that it is the case, let\\nus construct a proof tree that can perform the same inference without requiringthe temporal deduction rule. The result is depicted below\\n𝑃⇝𝑇1𝑄≞TV𝑃𝑄\\n(PI)\\n𝑃→←\\n𝑄𝑇1\\n≞TV𝑃𝑄𝑄⇝𝑇2𝑅≞TV𝑄𝑅\\n(PI)\\n𝑄→←\\n𝑅𝑇2\\n≞TV𝑄𝑅\\n(S)\\n←\\n𝑄𝑇1\\n→←\\n𝑅𝑇1+𝑇2',\n",
       "  '≞TV𝑄𝑅𝑃≞TV𝑃𝑄≞TV𝑄\\n(S)\\n←\\n𝑄𝑇1\\n≞TV𝑄𝑅≞TV𝑅\\n(S)\\n←\\n𝑅𝑇1+𝑇2\\n≞TV𝑅\\n(D)\\n𝑃→←\\n𝑅𝑇1+𝑇2\\n≞TV(IP)\\n𝑃⇝𝑇1+𝑇2𝑅≞TV\\nAs you may see, the premises and the conclusion of that inference tree match\\nexactly the premises and the conclusion of the temporal deduction rule. Since\\nnone of the intermediary formula, beside the deduction formula, alter the truthvalues, we may conclude that the formula of the temporal deduction rule is\\nidentical to that of the deduction rule.\\n3.3 Example\\nIn this section we show how to carry an inference combining direct and indirect\\nevidence. To illustrate this process, we consider the temporal predicates 𝑃,𝑄\\nand𝑅, with two datapoints as direct evidence of 𝑃⇝\\n1𝑄, combined with another\\npredictive implication, 𝑄⇝2𝑃, given as background knowledge, to produce a\\nthird predictive implication, 𝑃⇝3𝑅, based on indirect evidence. The whole\\ninference tree is given below (using 𝑘= 100 asLookahead in the truth value\\nformula).\\n𝑃(1)≞<1,1>𝑃(2)≞<1,1>𝑄(1+1)≞<0,1>𝑄(2+1)≞<1,1>\\n(PIDI)',\n",
       "  '𝑃⇝1𝑄≞<0.5,0.02>𝑄 ⇝2𝑅≞<0.3,0.1>𝑃≞<1,0.02>𝑄≞<0.5,0.02>𝑅≞<0.2,0.5>\\n(TD)\\n𝑃⇝3𝑅≞<0.2,0.018>\\n4 Procedural Reasoning\\nLet us now examine how to use temporal deduction to perform a special type\\nof procedural reasoning, to build larger plans made of smaller plans by chaining\\ntheir actions. Given plans, also called Cognitive Schematics [4], of the form',\n",
       "  'Probabilistic Logic Networks for Temporal and Procedural Reasoning 93\\n𝐶1∧𝐴1⇝𝑇1𝐶2≞TV1\\n...\\n𝐶𝑛∧𝐴𝑛⇝𝑇𝑛𝐺≞TV𝑛\\nexpressing that in context 𝐶𝑖, executing action 𝐴𝑖may lead to subgoal 𝐶𝑖+1or\\ngoal𝐺,a f t e r𝑇𝑖time units, with a likelihood of success measured by TV𝑖,w e\\nshow how to infer the composite plan\\n𝐶1∧𝐴1⩘𝑇1...⩘𝑇𝑛−1𝐴𝑛⇝𝑇1+···+𝑇𝑛𝐺≞TV\\nalongside its truth value TV. The inferred plan expresses that in context 𝐶1,\\nexecuting actions 𝐴𝑖to𝐴𝑛in sequence, waiting 𝑇𝑖time units between 𝐴𝑖and\\n𝐴𝑖+1, leads to goal 𝐺after𝑇1+···+𝑇𝑛time units, with a likelihood of success\\nmeasured by TV. Note that strictly speaking, 𝐴𝑖is not an action, it is a predicate\\nthat captures the temporal activation of an action. This can be formalized in\\nPLN as well but is not where the diﬃculty lies. Thus here we directly workwith action activation predicates and refer to them as actions for the sake of\\nconvenience.\\nLet us show how to do that with two action plans by building a proof tree',\n",
       "  'like we did for the temporal deduction rule. The ﬁnal inference rule we are tryingto build should look like\\n𝐶1∧𝐴1⇝𝑇1𝐶2≞TV12𝐶2∧𝐴2⇝𝑇2𝐶3≞TV23...\\n𝐶1∧𝐴1⩘𝑇1𝐴2⇝𝑇1+𝑇2𝐶2≞TV\\nwhere the dots are premises to be ﬁlled once we know what they are. Indeed,\\nwe cannot directly apply temporal deduction because the implicand of the ﬁrst\\npremise, 𝐶2, does not match the implicant of the second premise, 𝐶2∧𝐴2.F o r\\nthat reason it is unclear what the remaining premises are. However, we can\\nbuild an equivalent proof tree using regular deduction, as well as other temporal\\ninferences rules deﬁned in Sect. 3. The resulting tree (without truth values so\\nthat it can ﬁt within the width of the page) is given below.\\n𝐶1∧𝐴1⇝𝑇1𝐶2(PI)\\n𝐶1∧𝐴1→𝑇1←\\n𝐶2𝑇1\\n(I)\\n𝐶1∧𝐴1∧←\\n𝐴2𝑇1\\n→←\\n𝐶2𝑇1\\n∧←\\n𝐴2𝑇1𝐶2∧𝐴2⇝𝑇2𝐶3(PI)\\n𝐶2∧𝐴2→←\\n𝐶3𝑇2\\n(S)\\n←\\n𝐶2𝑇1\\n∧←\\n𝐴2𝑇1\\n→←\\n𝐶3𝑇1+𝑇2\\n𝐶1∧𝐴1∧←\\n𝐴2𝑇1𝐶2∧𝐴2(S)\\n←\\n𝐶2𝑇1\\n∧←\\n𝐴2𝑇1𝐶3(S)\\n←\\n𝐶3𝑇1+𝑇2\\n(D)\\n𝐶1∧𝐴1∧←\\n𝐴2𝑇1\\n→←\\n𝐶3𝑇1+𝑇2\\n(IP)\\n𝐶1∧𝐴1⩘𝑇1𝐴2⇝𝑇1+𝑇2𝐶3',\n",
       "  'Note that we have used of a new rule labeled (I) at the left of the proof tree. This\\nrule eliminates independent predicates from an implication without modifying\\nthe truth value of its conclusion. Its use is justiﬁed by the fact that 𝐴2is executed\\nimmediately afterreaching 𝐶2, thus cannot have an eﬀect on it.',\n",
       "  '94 N. Geisweiller and H. Yusuf\\nAfter retaining the premises and the conclusion only, and adding back the\\ntruth values, we obtain the following procedural deduction rule:\\n𝐶1∧𝐴1⇝𝑇1𝐶2≞TV12𝐶2∧𝐴2⇝𝑇2𝐶3≞TV23𝐶1∧𝐴1∧←\\n𝐴2𝑇1\\n≞TV1𝐶2∧𝐴2≞TV2𝐶3≞TV3\\n(PD)\\n𝐶1∧𝐴1⩘𝑇1𝐴2⇝𝑇1+𝑇2𝐶3≞TV\\nwith a formula identical to that of the deduction rule, once again. The premises\\nﬁlling the dots are therefore\\n𝐶1∧𝐴1∧←\\n𝐴2𝑇1\\n≞TV1𝐶2∧𝐴2≞TV2𝐶3≞TV3\\nThere is no doubt these premises could be further decomposed into sub-inferences\\nas it was done with the (I) rule. Indeed, likely more simpliﬁcations can be made\\nby assuming that the agent has a form of freewill and thus that its actions areindependent of the rest of the universe, outside of its decision policy inﬂuenced by\\nits very procedural reasoning. This is reminiscent of the do-calculus [ 6] and will\\nbe explored in more depth in the future. In the meantime, these are left as they\\nare, as it introduces no additional assumption, and their truth values can always',\n",
       "  'be calculated using inference rules based on direct evidence, if anything else.Future directions may also include adding inference rules to support behavior\\ntrees; introducing Event Calculus operators as predicate transformers (similar\\nto how Lagand Leadare deﬁned); as well as supporting temporal intervals and\\ncontinuous time.\\nReferences\\n1. Abourizk, S., Halpin, D., Wilson, J.: Fitting beta distributions based on sample\\ndata. J. Constr. Eng. Manag. 120, 288–305 (1994)\\n2. Geisweiller, N., Yusuf, H.: Rational OpenCog controlled agent. In: Hammer, P.,\\net al. (eds.) AGI 2023. LNAI, vol. 13921, pp. xx–yy. Springer, Cham (2023)\\n3. Goertzel, B., Ikle, M., Goertzel, I.F., Heljakka, A.: Probabilistic Logic Networks.\\nSpringer, New York (2009). https://doi.org/10.1007/978-0-387-76872-4\\n4. Goertzel, B., et al.: Cognitive synergy between procedural and declarative learning',\n",
       "  'in the control of animated and robotic agents using the OpenCogPrime AGI archi-tecture. In: Proceedings of the AAAI Conference on Artiﬁcial Intelligence (2011)\\n5. Jøsang, A.: Subjective Logic: A Formalism for Reasoning Under Uncertainty, 1st\\nedn. Springer, Cham (2016). https://doi.org/10.1007/978-3-319-42337-1\\n6. Pearl, J.: Causal diagrams for empirical research. Biometrika 82, 669–688 (1995)\\n7. Prior, A.N.: Past, Present and Future. Clarendon Press, Oxford (1967)\\n8. Shanahan, M.: The event calculus explained. In: Wooldridge, M.J., Veloso, M. (eds.)\\nArtiﬁcial Intelligence Today. LNCS (LNAI), vol. 1600, pp. 409–430. Springer, Hei-\\ndelberg (1999). https://doi.org/10.1007/3-540-48317-9\\n17',\n",
       "  'Rational OpenCog Controlled Agent\\nNil Geisweiller(B)and Hedra Yusuf(B)\\nSingularityNET Foundation, Amsterdam, The Netherlands\\n{nil,hedra }@singularitynet.io\\nAbstract. In this paper we introduce, ROCCA for Rational OpenCog\\nControlled Agent , an agent, that, as its name suggests, leverages\\nthe OpenCog framework to fulﬁll goals in uncertain environments. It\\nattempts to act rationally, relying on reasoning for both learning andplanning. An experiment in a Minecraft environment is provided as a\\ntest case.\\nKeywords: Symbolic Reinforcement Learning\\n·Pattern Mining ·\\nProcedural Reasoning ·Thompson Sampling ·OpenCog ·Minecraft\\n1 Introduction\\nThis paper describes an attempt to leverage the OpenCog framework [ 15]f o r\\ncontrolling agents in uncertain environments. It can be seen as a reboot of pre-\\nvious attempts [ 5,10,12] relying on new or improved components such as\\n– a hypergraph pattern miner [ 7] and a version of Probabilistic Logic Net-',\n",
       "  'works (PLN) [ 9] both implemented on top of OpenCog’s Uniﬁed Rule Engine\\nequipped with an inference control mechanism;\\n– a temporal and procedural extension of PLN [ 8];\\n– a simpliﬁed version of OpenPsi [ 5] leaving aside built-in urges and modulators\\nfrom MicroPsi [ 3] and using an action selection policy based on Thompson\\nSampling [ 17].\\nIt is comparable to OpenNARS for Applications (ONA) [ 14] but, among other\\ndiﬀerences, uses PLN [ 9] as its core logic.\\nThe ultimate goal of this project is to provide a technology to enable us\\nto experiment with forms of meta-learning and introspective reasoning for self-improvements. The rational for using a reasoning-based system is that it oﬀers\\nmaximum transparency and is thus more amenable to reﬂectivity and intro-\\nspection [ 11,19]. The work that is described in this paper is only the premise\\nof that goal. No meta-learning is taking place yet. The objective for now is to',\n",
       "  'build an agent that is able to discover regularities from its environment and acts\\nrationally, possibly at the expense of eﬃciency, at least initially. For discoveringregularities, the agent uses a reasoning-based pattern miner [ 7]. Then combine\\nc⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2023\\nP. Hammer et al. (Eds.): AGI 2023, LNAI 13921, pp. 95–104, 2023.https://doi.org/10.1007/978-3-031-33469-6\\n_10',\n",
       "  '96 N. Geisweiller and H. Yusuf\\nthese regularities to form plans using temporal and procedural reasoning. More\\nspeciﬁcally plans are predictive implications of the form\\n𝐶∧𝐴⇝𝑇𝐺≞TV\\ncalled Cognitive Schematics orSchemata . Which can be read as “in some context\\n𝐶, if some, possibly composite, action 𝐴is executed, then after 𝑇time units, the\\ngoal𝐺is likely to be fulﬁlled, with second order probability measured by TV” .\\nThese plans are then combined to into a mixture that grossly approximates\\nSolomonoﬀ distribution [ 6]. Finally, the next action is selected using Thompson\\nSampling [ 17]. The resulting system is called ROCCA for Rational OpenCog\\nControlled Agent .\\nThe rest of the paper is organized as follows. A recall of the OpenCog frame-\\nwork is provided in Sect. 2. ROCCA is described in Sect. 3. An experiment using\\nit to control an agent in Minecraft is described in Sect. 4. A conclusion including\\nfuture directions is given in Sect. 5.\\n2 OpenCog Framework Recall',\n",
       "  'OpenCog [ 15] is a framework oﬀering a hypergraph database technology with\\na query language and a collection of programs built on top of it to perform\\ncognitive functions such as learning, reasoning, spreading attention and more.\\nKnowledge is stored in AtomSpaces , hypergraphs composed of atoms ,linksand\\nnodes, where links can connect to other atoms. Values can be attached to atoms\\nto hold probability, conﬁdence, importance and more. Values and atoms can be\\nof various types. Let us recall the types we need for the rest of paper.\\n–A TruthValue is a second order probability distribution, i.e. a probability of\\na probability.\\n–A SimpleTruthValue is a TruthValue where the second order distribution is\\nrepresented by a beta distribution of parameters controlled by a strength ,a\\nproxy for a probability, and a conﬁdence over that strength. It is denoted\\n<𝑠,𝑐> where𝑠is the strength and 𝑐is the conﬁdence.\\n–A Predicate is function from a certain domain to 𝐵𝑜𝑜𝑙𝑒𝑎𝑛 . A TruthValue can',\n",
       "  'be attached to a predicate, representing the prevalence of its satisﬁed inputs.For instance 𝑃≞<0.4,0.1>represents that 𝑃tends to evaluate to True\\n40% of the time, but there is a small conﬁdence of 0.1 over that 40%. A\\nTruthValue can be attached to individual evaluations as well. For instance𝑃(𝑎)≞<0.9,1>represents that the probability of 𝑃(𝑎)evaluating over a\\nparticular 𝑎toTrue, is 0.9 and we are certain about it.\\n–A Conjunction is a link between two predicates, representing the predicate\\nresulting from the pointwise conjunction of these two predicates. For instance\\n𝑃∧𝑄≞<0.2,0.3>represents the prevalence, with strength 0.2 and conﬁdence\\n0.3, of the pointwise conjunction of\\n𝑃and𝑄.\\n–A n Implication is a link between two predicates, semantically representing the\\nconditional probability between two events represented by these predicates.\\nFor instance 𝑃→𝑄≞<0.7,0.4>indicates that if 𝑃(𝑥)isTruethen there is\\na 70% change with a 0.4 conﬁdence, that 𝑄(𝑥)is also True.',\n",
       "  'Rational OpenCog Controlled Agent 97\\nAdditionally we use the following types for temporal reasoning.\\n–A Sequential Conjunction is a link between two temporal predicates, repre-\\nsenting the predicate resulting from the pointwise conjunction of these pred-\\nicates while the second one leads by a certain time. For instance 𝑃⩘𝑇𝑄is the\\npointwise conjunction of 𝑃and a leading 𝑄by𝑇time units. Meaning that\\n(𝑃⩘𝑇𝑄)(𝑥,𝑡)isTrueif and only if 𝑃(𝑥,𝑡)and𝑄(𝑥,𝑡+𝑇)areTrue.\\n–A Predictive Implication is a link between two temporal predicates, repre-\\nsenting the conditional probability between two events delayed by a certain\\ntime. For instance 𝑃⇝𝑇𝑄≞<0.8,0.5>indicates that if 𝑃(𝑥)isTruethen\\nthere is a 80% chance with a 0.5 conﬁdence, that after 𝑇time units 𝑄(𝑥)will\\nalso be True.\\nThe diﬀerence between a temporal and an atemporal predicate is its domain. A\\ntemporal predicate must have at least a temporal dimension. More detail about\\nthe temporal types and their associated inference rules is provided in [ 8].',\n",
       "  '3 Rational OpenCog Controlled Agent\\nROCCA is implemented as an observation-planning-action loop interleaved with\\nlearning and reasoning. It provides an interfacing between OpenCog and envi-\\nronments such as Malmo [ 16] or OpenAI Gym [ 4]. It is written in Python which is\\nboth supported by these environments and OpenCog. Figure 1provide a graph-\\nical representation of ROCCA as if it was a single loop incorporating all steps.\\nEnvironment\\nGoal\\nSchemataWorking\\nAtomspaceAtomspacePercepta\\nReasonerTemporalPattern\\nMiner RecorderPercepta\\nSelectorAction\\nAtomspace\\nFig. 1. Rational OpenCog Controlled Agent\\ncontrol and learning cycles merged into a sin-\\ngle loop.3.1 Memory\\nFor better eﬃciency and clarity, the\\nmemory of the agent is split into\\nthree parts.\\n1. The Percepta AtomSpace holds\\ntimestamped observations as\\nthey come into the system.\\n2. The Working AtomSpace holds\\nany kind of data, ranging from\\ntimestamped observations topredictive implications. Most\\nknowledge used and inferred',\n",
       "  'during the course of learning areusually dumped into this Atom-\\nSpace.\\n3. The Schemata AtomSpace holds\\nCognitive Schematics , which are\\npredictive implications relating\\ncontexts, actions and goals.',\n",
       "  '98 N. Geisweiller and H. Yusuf\\n3.2 Processes\\nThe agent runs two main processes:\\n1. A Control process for real-time reactive agent control.\\n2. A Learning process for non-reactive background learning.\\nIn principle these two processes could happen in parallel. For now they alternate\\nin series. The agent starts in a control phase. A number of control cycles occuras the agent motor-babbles through its environment. It is then followed by a\\nlearning phase when the agent discover regularities and build plans. And ﬁnally\\nrepeats the control phase to test how it performs after learning.\\n3.3 Control\\nThe control process is composed of control cycles, each decomposed into Obser-\\nvation ,Planning and Action phases, as described below.\\n1.Observation :\\n(a) Receive and timestamp observations, reward included, from the environ-\\nment.\\n(b) Store the timestamped observations in the Percepta AtomSpace.\\n2.Planning :\\n(a) Select the goal for that cycle.',\n",
       "  '(b) Find plans fulﬁlling that goal from the Schemata AtomSpace.\\n(c) Build a mixture distribution from these plans.\\n3.Action :\\n(a) Select the next action via Thompson Sampling according to that mixture\\ndistribution.\\n(b) Timestamp and store the selected action in the Percepta AtomSpace.\\n(c) Run the selected action and by that update the environment.\\nNone of these steps are computationally expensive. They involve algorithms that\\nare at most linear with the size of the Percepta and Schemata AtomSpaces. As\\ntime goes and knowledge accumulates though, it will progressively slow down.Indeed, for real-time responsiveness such control cycle should be bound by a\\nconstant. Achieving this may require to incorporate other mechanisms such as\\nﬁltering and forgetting. This problem, as important as it is, is left aside for futureresearch. Given the small environments ROCCA has been tested with, it has not\\nbeen a problem so far. Let us now provide more details about these three phases.',\n",
       "  'Observation. During the observation phase, data coming from the environ-\\nment are timestamped and stored in the Percepta AtomSpace with the formatdatum@timestamp . For instance if at cycle 3 the agent observes outside(house) ,\\nthen outside(house)@3 is inserted into the Percepta AtomSpace.',\n",
       "  'Rational OpenCog Controlled Agent 99\\nPlanning. The ﬁrst step of the planning phase is to select a goal 𝐺to fulﬁll. In\\nthe current version of ROCCA though it merely returns a constant goal which isto gain a reward within a forward window. More on goal selection can be found\\nin [11,13]. Once the goal has been selected, the agent searches the Schemata\\nAtomSpace with the following pattern matcher query\\n$𝐶∧$𝐴⇝\\n𝑇𝐺\\nwhere$𝐶is a variable representing the context, $𝐴is a variable representing\\nthe action, 𝑇is a time delay selected within that forward window and 𝐺is\\nthe selected goal. All returned candidates are then ﬁltered according to theircontexts, only retaining those for which the context is evaluated to True at\\nthe current time. Ideally, such crisp evaluation should be replaced by a second\\norder probability evaluation of a context being True. This is important for con-\\ntexts that have elements of uncertainty. But for the sake of simplicity, in our',\n",
       "  'experiments so far, all contexts are crisply evaluated. Then from the set of valid\\ncognitive schematics, a second order mixture distribution is built and handed tothe next phase for performing action selection. The calculations used to build\\nthat second order mixture distribution is detailed in [ 6].\\nAction. The Action phase consists of the following steps:\\n1. Select the next action via Thompson Sampling [ 17] according to the mixture\\ndistribution built during the planning phase.\\n2. Timestamp and store the selected action in the Percepta AtomSpace.3. Run the selected action and update the environment. If it is a composite\\naction, only run the ﬁrst primary action.\\nThe trickiest step here is selecting the next action via Thompson Sampling. The\\nnovelty is that the second order probabilities can be leveraged by Thompson\\nSampling. For example, assume we have two actions, 𝐴\\n1and𝐴2, to choose among\\ntwo predictive implications\\n𝐶1∧𝐴1⇝𝑇𝐺≞<0.6,0.9>\\n𝐶2∧𝐴2⇝𝑇𝐺≞<0.7,0.1>',\n",
       "  'Using only the strengths of the truth values as proxy for probability of suc-\\ncess, the choice is clear. Action 𝐴2should be selected, because its probability\\nof success, which is 0.7, is greater than that of 𝐴1, which is 0.6. However once\\nconﬁdence is introduced, that choice becomes less clear because the truth value\\nof success of 𝐴2has a low conﬁdence of 0.1. In that case, ﬁrst order probabili-\\nties are sampled from their corresponding second order distributions, and then\\nthese probabilities are compared. The action with the maximum probability gets\\nselected. Informally, the idea is to consider the possibilities that the agent mightbe living in a world where 𝐴\\n2has a lower probability of success than 𝐴1. That\\nis the essence of Thompson Sampling. Figure 2shows the second order distri-\\nbutions of the probabilities of success of 𝐴1, in blue, and 𝐴2, in red, for these',\n",
       "  '100 N. Geisweiller and H. Yusuf\\nFig. 2. Second order probability distributions of success of actions 𝐴1and𝐴2, using as\\nparameters of the beta distribution 𝛼(𝑠,𝑐)=𝛼0+𝑠.𝑐.𝑘\\n1−𝑐and𝛽(𝑠,𝑐)=𝛽0+(1−𝑠).𝑐.𝑘\\n1−𝑐where𝑘,\\nthe lookahead , is set to 100, and 𝛼0and𝛽0are set to Jeﬀreys prior.\\ntruth values. As one may notice, the area under the red curve situated at the\\nleft of the blue curve is non-negligible. Meaning that the probability of being in\\na world where 𝐴1has a higher probability of success than 𝐴2is non-negligible as\\nwell. Because these strengths and conﬁdences are periodically updated duringthe lifetime of the agent, one can see how Thompson Sampling is a great alterna-\\ntive to𝜀-greedy, as it oﬀers a parameter-free mechanism to balance exploration\\nand exploitation that dynamically adapts with the knowledge of the agent.\\nNote that in this example only two actions among two cognitive schematics',\n",
       "  'are considered, but in practice there is usually a handful of actions among apotentially very large number of cognitive schematics with overlapping contexts\\nand conﬂicting goals. The resulting distribution of success of each action is typ-\\nically multi-modal and do not reduce to a beta distribution. How to deal withsuch a multitude of cognitive schematics is treated in [ 6].\\n3.4 Learning\\nThe diﬃculty then comes down to discovering cognitive schematics that are as\\npredictive and widely applicable as possible. For that, ROCCA uses a combina-\\ntion of pattern mining and reasoning.\\nPattern Mining. A relatively inexpensive way to discover regularities in the\\nenvironment is to mine the Percepta AtomSpace. For instance, given\\n{go(right)@0 ,square(right)@1 ,go(left)@1 ,square(left)@2 ,... }',\n",
       "  'Rational OpenCog Controlled Agent 101\\nthe pattern miner can discover temporal relationships such as\\ngo(right)⇝1𝑠𝑞𝑢𝑎𝑟𝑒(𝑟𝑖𝑔ℎ𝑡)\\ngo(left)⇝1𝑠𝑞𝑢𝑎𝑟𝑒(𝑙𝑒𝑓𝑡)\\nas well as more abstract relationships, such as\\ngo(𝑥)⇝1𝑠𝑞𝑢𝑎𝑟𝑒(𝑥)\\nThe pattern mining algorithm used by ROCCA is detailed in [ 7]. This is a\\ngeneric hypergraph pattern miner, not specialized for temporal patterns. In order\\nto mine temporal patterns with it, timestamps are represented as naturals. 0 is\\npresented by 𝑍,1b y𝑆(𝑍),2b y𝑆(𝑆(𝑍)), etc. This provides the needed structure\\nto discover temporal relationships between events. As it currently stands, the\\npattern miner can eﬃciently discover mono-action plans. Mining poly-action\\nplans is also possible but has two issues:\\n1. In the worse case, the computational cost of mining goes up exponentially\\nwith the size of the action sequence to mine.\\n2. The number of observations to accumulate in order to generate cognitive\\nschematics with decent conﬁdences goes up exponentially as well.',\n",
       "  'The latter is really the most problematic because we cannot buy our way out of\\nit. If each observation takes a certain amount time, determined by the control\\ncycle period in the case of primary observations, then we have to go through\\nthem, we cannot speed time up. This is even more true for abstract perceptathat can only be observed at periods that are multiples of control cycle periods.\\nAlso, in some cases, a particular action sequence may never be observed at all,\\nyet we still would like to have a way to estimate the likelihood of its success. Inorder to address these limitations and more, we need reasoning.\\nTemporal Reasoning. ROCCA uses a temporal extension of PLN described\\nin [8] to update existing cognitive schematics obtained by pattern mining, and\\ndiscover new cognitive schematics by combining existing ones. For instance itcan infer poly-action plans by stringing mono-action plans together, as well as',\n",
       "  'generalize or specialize their contexts or goals. Temporal rules integrated into\\nROCCA include:\\n1.Predictive Implication Direct Introduction to infer the truth value of a pre-\\ndictive implication from direct observations.\\n2.Temporal Conditional Conjunction Introduction to specialize a goal within a\\nplan by considering the conjunction of existing cognitive schematics goals.\\n3.Temporal Deduction to string together small plans to form bigger ones.\\nThe precise semantics of these rules is detailed in [ 8]. An example of how they\\nare used is presented below.',\n",
       "  '102 N. Geisweiller and H. Yusuf\\n4 Experiment in a Simple Minecraft Environment\\nIn this experiment, we use Malmo [ 16] to construct a basic Minecraft world that\\ncomprises a house ﬁlled with diamonds and a key. The objective of the agent\\nis to retrieve the key, located somewhere in the vicinity of the house, and then\\nunlock the door of the house. Upon unlocking the door, the agent is able tocollect a diamond and receive a reward.\\nThe aim of the experiment is to make ROCCA learn from interacting with the\\nMinecraft environment and collect as many diamonds as possible. To make thetask easier, the primary actions and perceptions provided by Malmo have been\\nreplaced by high level actions such as go(key) ,go(house) and go(diamonds) ,a s\\nwell as high level perceptions about the state of the agent such as outside(house) ,\\nhold(key) and the reward for completing a given action, reward(1) .\\nThe experiment consists of two iterations of training lasting ﬁfty control',\n",
       "  'cycles each, interleaved by a learning phase of a few hours. During the ﬁrst iter-\\nation, no learning is taking place as the agent has no prior knowledge. The agent\\nrandomly explores the environment. Then it enters a learning phase, discoveringcognitive schematics via mining and reasoning, subsequently leading the agent\\nto achieve more frequently its goal during the next training phase.\\nLet us look more closely how ROCCA discovers cognitive schematics. Given\\nthe following observations\\n{...,Reward(0)@10 ,outside(house)@10 ,hold(key)@10 ,go(house)@10 ,\\ninside(house)@11 ,go(diamond)@11 ,Reward(0)@11 ,Reward(1)@12 ,... }\\nROCCA can mine, among many other things, the following cognitive schematic\\nhold(key) ∧go(house) ⇝\\n1inside(house) ≞<0.833,0.007>\\nAdditionally, by applying the temporal conditional conjunction introduction rule\\non the relevant relationships, such as\\noutside(house) ∧go(key) ⇝1outside(house) ≞<1,0.007>\\noutside(house) ∧go(key) ⇝1hold(key) ≞<1,0.007>\\nthe agent derives',\n",
       "  'outside(house) ∧go(key) ⇝1outside(house) ∧hold(key) ≞<1,0.007>\\nwhich, if combined with\\noutside(house) ∧hold(key) ∧go(house) ⇝1inside(house) ≞<0.833,0.007>\\ncan be used by the procedural deduction rule to infer\\noutside(house) ∧go(key)⩘1go(house) ⇝2inside(house) ≞<0.833,0.007>\\nContinuing this reasoning process ultimately results in the discovery of an eﬀec-\\ntive plan that leads to achieving the goal, such as\\noutside(house) ∧go(key)⩘1go(house) ⩘1go(diamond) ⇝3reward(1) ≞<0.833,0.005>',\n",
       "  'Rational OpenCog Controlled Agent 103\\nA rigorous evaluation is kept for a future paper, nonetheless our preliminary\\nresults indicate that ROCCA successfully learns the necessary cognitive schemat-ics and, as a consequence, accumulates more rewards during the second iter-\\nation. In the ﬁrst iteration the cumulative reward is around 5, then doubles\\nto quadruples during the second iteration, depending on the random seed andother parameters. If ROCCA keeps running after that, the cumulative reward\\nrate keeps going up because the conﬁdences of the cognitive schematics increase,\\nleading to more exploitation and less exploration. One may notice that someplans are not completely reliable, their strengths is below 1. That is because\\nsome actions may fail. ROCCA is suited for dealing with uncertainty and has\\nno problem with that. These ﬁndings are encouraging but only apply to a verysimple environment and may not be indicative of the overall performance of',\n",
       "  'ROCCA. More experiments over more environments are required and will be\\npursued in future work.\\nThe source code of ROCCA is hosted on Github [ 2] and a video of this\\nexperiment is available on YouTube [ 1].\\n5 Conclusion\\nROCCA, a system that leverages the OpenCog framework for controlling an\\nagent in uncertain environments has been presented. This agent is in a strongsense fully reasoning-based, from learning to planning. The advantage we believe\\nof such approach, in spite of its current ineﬃciencies, is to oﬀer greater trans-\\nparency and foster greater capabilities for meta-learning and self-improvement.As such, we are only at the start of our endeavor. Towards that end, future\\ndirections include:\\n1. Integrate Economic Attention Networks [18]f o r Attention Allocation . Record\\nattentional spreading as percepta to learn Hebbian links [ 18] and improve\\nattention allocation in return.\\n2. Carry out concept creation and schematization, also called crystallized atten-',\n",
       "  'tion allocation, to speed up repetitive information processing. This done\\nwell should also provide a solution to the problem of creating hierarchiesof abstract observations and actions.\\n3. Record more internal processes, not just attentional spreading, as internal\\npercepta to enable deeper forms of introspection.\\n4. Plan with internal actions, not just external, such as parameter tuning and\\ncode rewriting to enable self-improvements.\\nReferences\\n1. ROCCA video demonstration (2022). https://youtu.be/rCvQHrJAD2c\\n2. ROCCA source code repository (2023). https://github.com/opencog/rocca\\n3. Bach, J.: MicroPsi 2: the next generation of the MicroPsi framework. In: Bach,\\nJ., Goertzel, B., Ikl´ e, M. (eds.) AGI 2012. LNCS (LNAI), vol. 7716, pp. 11–20.\\nSpringer, Heidelberg (2012). https://doi.org/10.1007/978-3-642-35506-6 2',\n",
       "  '104 N. Geisweiller and H. Yusuf\\n4. Brockman, G., et al.: OpenAI gym (2016)\\n5. Cai, Z., et al.: OpenPsi: a novel computational aﬀective model and its application\\nin video games. Eng. Appl. Artif. Intell. 26, 1–12 (2013). https://doi.org/10.1016/\\nj.engappai.2012.07.013\\n6. Geisweiller, N.: Partial operator induction with beta distributions. In: Ikl´ e, M.,\\nFranz, A., Rzepka, R., Goertzel, B. (eds.) AGI 2018. LNCS (LNAI), vol. 10999,\\npp. 52–61. Springer, Cham (2018). https://doi.org/10.1007/978-3-319-97676-1 6\\n7. Geisweiller, N., Goertzel, B.: An inferential approach to mining surprising patterns\\nin hypergraphs. In: Hammer, P., Agrawal, P., Goertzel, B., Ikl´ e, M. (eds.) AGI\\n2019. LNCS (LNAI), vol. 11654, pp. 59–69. Springer, Cham (2019). https://doi.\\norg/10.1007/978-3-030-27005-6 6\\n8. Geisweiller, N., Yusuf, H.: Probabilistic logic networks for temporal and procedural\\nreasoning. In: Hammer, P., et al. (eds.) AGI 2023. LNAI, vol. 13921, pp. 85–94.Springer, Cham (2023)',\n",
       "  '9. Goertzel, B., Ikle, M., Goertzel, I.F., Heljakka, A.: Probabilistic Logic Networks.\\nSpringer, New York (2009). https://doi.org/10.1007/978-0-387-76872-4\\n10. Goertzel, B., et al.: An integrative methodology for teaching embodied non-\\nlinguistic agents, applied to virtual animals in second life. In: Proceedings of the\\n2008 Conference on Artiﬁcial General Intelligence 2008: Proceedings of the FirstAGI Conference, pp. 161–175. IOS Press, NLD (2008)\\n11. Goertzel, B., Pennachin, C., Geisweiller, N.: Engineering General Intelligence,\\nPart 1: A Path to Advanced AGI via Embodied Learning and Cognitive Synergy.\\nAtlantis Press (2014)\\n12. Goertzel, B., et al.: Cognitive synergy between procedural and declarative learning\\nin the control of animated and robotic agents using the OpenCogPrime AGI archi-\\ntecture. In: Proceedings of the AAAI Conference on Artiﬁcial Intelligence (2011)\\n13. Hahm, C., Xu, B., Wang, P.: Goal generation and management in NARS. In:',\n",
       "  'Goertzel, B., Ikl´ e, M., Potapov, A. (eds.) AGI 2021. LNCS (LNAI), vol. 13154, pp.\\n96–105. Springer, Cham (2022). https://doi.org/10.1007/978-3-030-93758-4\\n11\\n14. Hammer, P., Lofthouse, T.: ‘OpenNARS for applications’: architecture and control.\\nIn: Goertzel, B., Panov, A.I., Potapov, A., Yampolskiy, R. (eds.) AGI 2020. LNCS\\n(LNAI), vol. 12177, pp. 193–204. Springer, Cham (2020). https://doi.org/10.1007/\\n978-3-030-52152-3 20\\n15. Hart, D., Goertzel, B.: OpenCog: a software framework for integrative artiﬁcial\\ngeneral intelligence. In: Proceedings of the 2008 Conference on Artiﬁcial General\\nIntelligence 2008: Proceedings of the First AGI Conference, pp. 468–472. IOS Press,NLD (2008)\\n16. Johnson, M., Hofmann, K., Hutton, T., Bignell, D.: The Malmo platform for arti-\\nﬁcial intelligence experimentation. In: International Joint Conference on Artiﬁcial\\nIntelligence (2016)\\n17. Leike, J., Lattimore, T., Orseau, L., Hutter, M.: Thompson sampling is asymp-',\n",
       "  'totically optimal in general environments. In: Proceedings of the Thirty-SecondConference on Uncertainty in Artiﬁcial Intelligence. UAI 2016, Arlington, Virginia,\\nUSA, pp. 417–426. AUAI Press (2016)\\n18. Pitt, J., Ikle, M., Sellmann, G., Goertzel, B.: Economic attention networks: asso-\\nciative memory and resource allocation for general intelligence. In: Proceedings ofthe 2nd Conference on Artiﬁcial General Intelligence, pp. 88–93. Atlantis Press,\\nJune 2009. https://doi.org/10.2991/agi.2009.19\\n19. Schmidhuber, J.: Goedel machines: self-referential universal problem solvers mak-\\ning provably optimal self-improvements. ArXiv cs.LO/0309048 (2003)',\n",
       "  'Towards Cognitive Bots: Architectural\\nResearch Challenges\\nHabtom Kahsay Gidey1(B), Peter Hillmann1, Andreas Karcher1,\\nand Alois Knoll2\\n1Universit¨ at der Bundeswehr M¨ unchen, Munich, Germany\\n{habtom.gidey,peter.hillmann,andreas.karcher }@unibw.de\\n2Technische Universit¨ at M¨unchen, Munich, Germany\\nknoll@in.tum.de\\nAbstract. Software bots operating in multiple virtual digital platforms\\nmust understand the platforms’ aﬀordances and behave like human users.Platform aﬀordances or features diﬀer from one application platform to\\nanother or through a life cycle, requiring such bots to be adaptable.\\nMoreover, bots in such platforms could cooperate with humans or othersoftware agents for work or to learn speciﬁc behavior patterns. How-\\never, present-day bots, particularly chatbots, other than language pro-\\ncessing and prediction, are far from reaching a human user’s behaviorlevel within complex business information systems. They lack the cogni-',\n",
       "  'tive capabilities to sense and act in such virtual environments, rendering\\ntheir development a challenge to artiﬁcial general intelligence research.In this study, we problematize and investigate assumptions in concep-\\ntualizing software bot architecture by directing attention to signiﬁcant\\narchitectural research challenges in developing cognitive bots endowedwith complex behavior for operation on information systems. As an out-\\nlook, we propose alternate architectural assumptions to consider in future\\nbot design and bot development frameworks.\\nKeywords: cognitive bot\\n·cognitive architecture ·problematization\\n1 Introduction\\nBots are software agents that operate in digital virtual environments [ 1,2]. An\\nexample scenario would be a “ user-like ” bot that could access web platforms\\nand behave like a human user. Ideally, such a bot could autonomously senseand understand the platforms’ aﬀordances. Aﬀordances in digital spaces are,',\n",
       "  'for example, interaction possibilities and functionalities on the web, in software\\nservices, or on web application platforms [ 3,4]. The bot would recognize and\\nunderstand the diﬀerences and variability between diﬀerent environments’ aﬀor-\\ndances. If a platform or service has extensions to physical bodies or devices, as\\nin the Web of Things (WoT), it would also have control of or possibilities tointeract with an outer web or service application world.\\nIdeally, a bot could also be independent of a speciﬁc platform. A user-\\nlike social bot, for instance, would be able to recognize and understand\\nc⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2023\\nP. Hammer et al. (Eds.): AGI 2023, LNAI 13921, pp. 105–114, 2023.https://doi.org/10.1007/978-3-031-33469-6\\n_11',\n",
       "  '106 H. K. Gidey et al.\\nsocial networks and act to inﬂuence or engage in belief sharing on any social\\nplatform. It would also adjust with the changes and uncertainty of the aﬀor-dances in a social media environment, such as when hypermedia interactivity\\nfeatures and functionalities change. Such a bot could also learn and develop to\\nderive its goals and intentions from these digital microenvironments and takegoal-directed targeted action to achieve them [ 5]. Such bots could also commu-\\nnicate and cooperate with other user agents, humans, or bots to collaborate and\\nsocialize for collective understanding and behavior.\\nThe example scenarios described above convey desiderata of perception and\\naction in bots, similar to how a human user would perceive and act in digital\\nspaces. To date, bots are incapable of the essential cognitive skills required toengage in such activity since this would entail complex visual recognition, lan-',\n",
       "  'guage understanding, and the employment of advanced cognitive models. Instead,\\nmost bots are either conversational interfaces or question-and-answer knowledge\\nagents [ 1]. Others only perform automated repetitive tasks based on pre-given\\nrules, lacking autonomy and other advanced cognitive skills [ 6,7]. The problems\\nof realizing these desiderata are, therefore, complex and challenging [ 8,9]. Solu-\\ntions must address diﬀerent areas, such as transduction and autonomous action,\\nto achieve advanced generalizable intelligent behavior [ 10,11].\\nProblems spanning diverse domains require architectural solutions. Accord-\\ningly, these challenges also necessitate that researchers address the structural\\nand dynamic elements of such systems from an architectural perspective. [ 12–\\n14]. For this reason, this paper outlines the architectural research agendas to\\naddress the challenges in conceptualizing and developing a cognitive bot with\\ngeneralizable intelligence.',\n",
       "  'The paper is divided into sections discussing each of the research challenges.\\nIn Sect. 2, we discuss the challenges related to eﬀorts and possible directions in\\nenabling bots to sense and understand web platforms. Next, Sect. 3describes\\nthe challenges related to developing advanced cognitive models in software bots.\\nSection 4and5discuss the research issues in bot communication and coopera-\\ntion, respectively. The remaining two sections provide general discussions on bot\\nethics and trust and conclude the research agenda.\\n2 The Transduction Problem\\nWeb platforms can be seen as distinct microenvironments within digital micro-\\ncosms [ 15]. They oﬀer a microhabitat for their users’ diverse digital experiences.\\nThese experiences mainly transpire from the elements of interaction and action,\\nor the hypermedia, within web environments [ 15,16]. Hypermedia connects and',\n",
       "  'extends the user experience, linking to further dimensions of the web-worlds,which means more pages and interactive elements from the user’s perspective.\\nThe interaction elements are considered aﬀordances in the digital space [ 3,4],\\nanalogous to the biological concept of aﬀordances from environmental psychol-ogy [17]. Aﬀordances can also be accompanied by signiﬁers. Signiﬁers reveal or\\nindicate possibilities for actions associated with aﬀordances [ 4,18]. An exam-\\nple on the web would be a button aﬀording a click action and a text signiﬁer',\n",
       "  'Towards Cognitive Bots: Architectural Research Challenges 107\\nhinting “Click to submit”. A human user understands this web environment, its\\ncontent, and its aﬀordances, and navigates reasonably easily. However, enablingsoftware bots to understand this digital environment and its aﬀordances the way\\nhuman users do is a challenging task. It is a complex problem of translating and\\nmapping perception to action, i.e., the transduction problem [ 19,20].\\nToday, there are diﬀerent approaches to this problem. The ﬁrst category\\nof approaches provides knowledge about the environment for diﬀerent levels of\\nobservability using APIs or knowledge descriptions. With API-based approaches,developers create bots for a speciﬁc platform, constantly putting developers in\\nthe loop. Bots do not have the general perceptual capability to understand and\\nnavigate with autonomous variability. Other architectures in this category, orig-inating from the WoT, attempt to address this challenge by using knowledge',\n",
       "  'models and standards that could enable agents to perceive the web by exposing\\nhypermedia aﬀordances and signiﬁers [ 3,21]. The knowledge descriptions carry\\ndiscoverable aﬀordances and interpretable signiﬁers, which can then be resolved\\nby agents [ 3,4]. This approach might demand extended web standards that make\\nthe web a suitable environment for software agents. It might also require intro-\\nducing architectural constraints that web platforms must adhere to in developing\\nand changing their platforms, such as providing a knowledge description wherebots can read descriptions of their aﬀordances.\\nThe second category of approaches uses various behavioral cloning and rein-\\nforcement learning techniques [ 22]. One example is by Shi et al. [ 23], where they\\nintroduce a simulation and live training environment to enable bots to complete\\nweb interaction activities utilizing keyboard and mouse actions. Recent eﬀorts',\n",
       "  'extend these approaches by leveraging large language models (LLMs) for webpage understanding and autonomous web navigation [ 24,25]. The results from\\nboth techniques and similar approaches reveal the size of the gap between human\\nusers and bots [ 23,24].\\nBoth approach categories still need to solve the problem of variability and\\ngeneralizability of perception and action. Approaches that leverage the hyper-\\nmedia knowledge of platforms with aﬀordance and signiﬁer descriptions could\\nserve as placeholders, but real bots with generalizable capabilities would need\\nmore autonomous models yet.\\nBesides this, some design assumptions consider the environment and the\\nbot as one. As a result, they may attempt to design agents as an integrated\\npart of the platforms or try to ‘botify’ and ‘cognify’ or orient web services as\\nagents. Alternatively, the whole notion of a user-like bot inherently assumes\\nthe bot to have an autonomous presence separate from the web platforms it',\n",
       "  'accesses. Figure 1illustrates the basic perspective in a vertically separate design,\\nthe bot, and the web platforms it operates in. This strict separation enables both\\nthe environment and the bot to evolve independently.\\n3 The Behavior Problem\\nMost user activities on digital platforms are complex behaviors resulting from\\nhuman users’ underlying intentions, goals, and belief systems. Although a bot',\n",
       "  '108 H. K. Gidey et al.\\nFig. 1. A decoupled bot-environment and bot-behavior ( left) viewpoint.\\noperating in digital spaces need not fully emulate humans to achieve general-\\nizable behavior, it is essential to consider the intricacies and sophistication of\\nhuman users’ behavior on the web during bot design [ 26]. To that end, engineer-\\ning bots with behavior models similar to human users might take into account\\nexisting approaches of measuring generalizable user behavior while not having\\nto replicate human cognition as such [ 27].\\nFig. 2. The abstraction ladder in modeling machine intelligence.\\nCurrent models for engineering intelligent behavior come from three prospec-\\ntive categories of approaches. Each approach takes natural or human intelligence\\nas its inspiration and models it at diﬀerent levels of abstraction. The three meth-ods diﬀer mainly in how they try to understand intelligence and where they\\nstart the abstraction for modeling intelligence. Figure 2illustrates this ladder',\n",
       "  'of abstraction in modeling machine intelligence. The abstractions start eitherat artiﬁcial cognition, artiﬁcial neurons, or artiﬁcial life or consciousness [ 10,28].\\nThese abstractions aim to enact intelligent behavior based, respectively, on high-\\nlevel cognitive functions, artiﬁcial neural networks (ANNs), or more physical and\\nbottom-up approaches starting at molecular or atomic levels.\\nArtiﬁcial Cognition: in cognitive modeling, eﬀorts to model cognition are inspired\\nby the brain’s high-level cognitive functions, such as memory. Most assumptions\\nare based on studies and understandings in the cognitive sciences. Cognitive\\nmodels use diverse techniques such as production rules, dynamical systems, andquantum models to model particular cognitive capabilities [ 29]. Although cog-\\nnitive models use methods from other approaches, such as ANNs, they do not',\n",
       "  'Towards Cognitive Bots: Architectural Research Challenges 109\\nnecessarily adhere to underlying mechanisms in the brain [ 10,30]. Works such\\nas the OpenCog (Hyperon) and the iCub project are promising experimentalresearch examples that heavily rely on artiﬁcial cognitive models, i.e., cognitive\\narchitectures [ 10,30].\\nArtiﬁcial Neurons: brain models which use artiﬁcial neurons aim to understand,\\nmodel, and simulate underlying computational mechanisms and functions based\\non assumptions and studies in neuroscience [ 31]. Discoveries from neuroscience\\nare utilized to derive brain-based computational principles. Sometimes, these\\napproaches are referred to as Brain-derived AI orNeuroAI models [ 32–34]. Due\\nto the attention given to the underlying principles of computation in the brain,they strictly diﬀer from the brain-inspired cognitive models. Applications of\\nthese models are mainly advancements in artiﬁcial neural networks, such as deep',\n",
       "  'learning. Large-scale brain simulation research and new hardware developmentin neuromorphic computing, such as SpiNNaker and Loihi, also contribute to\\nresearch eﬀorts in this area. Some neuromorphic hardware enables close adher-\\nence to brain computational principles in particular types of neural networks,such as Spiking neural networks [32,35]. Brain-derived AI approaches with neu-\\nrorobotics aim to achieve embodiment using fully developed morphologies, which\\nare either physical or virtual. The Neurorobotics Platform (NRP) is an exampleof such eﬀorts to develop and simulate embodied systems. The NRP is a neuro-\\nrobotics simulation environment which connects simulated brains to simulated\\nbodies of robots [ 36].\\nArtiﬁcial Life (aLife): aLife attempts to model consciousness. To do this,\\nresearchers and developers start with a bottom-up approach at a physical ormolecular level [ 28]. Most synthesizing eﬀorts to model intelligence in artiﬁcial\\nlife are simulations with digital avatars.',\n",
       "  'In the context of bots on web platforms, employing integrated behavior\\nmodels, such as the NRP and OpenCog mentioned above, is still a challenge.\\nThus, in addition to the proposed separation of the bot and environment, decou-\\npling a bot’s basic skeleton and behavior models is architecturally important.\\nFigure 1,left, illustrates the separate structure of a bot and its behavior mod-\\nels. The bot’s core skeleton, for example, might have sensory and interactionelements as virtual actuators that enable its operation using the keyboard and\\nmouse actions. The vertical separation allows behavior models and bot skeletons\\nto change independently, maintaining the possibility of dynamic coupling.\\n4 Bot Communication Challenges\\nIn Multi-Agent Systems (MAS), agent-to-agent communication heavily relies onagent communication languages (ACLs) such as FIPA-ACL, standardized by the\\nFoundation for Intelligent Physical Agents(FIPA) consortium [ 19,37–39]. How-',\n",
       "  'ever, in mixed reality environments, where bots and humans share and collabo-rate in digital spaces, communication cannot rely only on ACLs and APIs [ 40].',\n",
       "  '110 H. K. Gidey et al.\\nTo that end, a cognitive bot with artiﬁcial general intelligence (AGI) must\\npossess communications capabilities to address humans and software agents withdiverse communication skills. Communication capabilities should include diverse\\npossibilities like email, dialogue systems, voice, blogging, and micro-blogging.\\nLarge language models (LLMs) have recently shown signiﬁcant progress in\\nnatural language processing and visual perception that could be utilized for bot\\nand human communication [ 24,25].\\n5 Integration and Cooperation Challenges\\nResearchers assert that the grand challenge in AGI remains in integrating diﬀer-\\nent intelligence components to enable the emergence of advanced generalizablebehavior or even collective intelligence [ 10,41–43]. The intelligence solutions to\\nintegrate include learning, memory, perception, actuation, and other cognitive\\ncapabilities [ 44]. Theories and assumptions developed by proponents include',\n",
       "  'approaches based on cognitive synergy, the free energy principle, and integrated\\ninformation theory [ 5,42,43].\\nIn practice, however, integration and cooperation of bots are implemented\\nmainly by utilizing methods such as ontologies, APIs, message routing, commu-\\nnication protocols, and middleware like the blackboard pattern [ 19,45,46].\\nFrom a software engineering perspective, basic architectural requirements\\nfor the context of bots operating on digital platforms are possibilities for the\\nevolvability of bots into collective understanding with shared beliefs, stigmergy,or sharing common behavior models to learn, transfer learned experience, and\\nevolve. Other concerns are the hosting, which could be on a cloud or individual\\nnodes, scaling, and distribution of bots and their behavior models.\\nFig. 3. Representation of integrated parts, i.e., bots, shared behavior models, and the\\nweb environments.',\n",
       "  'Towards Cognitive Bots: Architectural Research Challenges 111\\nFigure 3shows a simple diagram representing the integrated parts, i.e., bots,\\nshared behavior models, and the environment. Brepresents the possible number\\nof bots. BMrepresents the shared and individual behavior models. Erepresents\\nthe web environment and its variability. The lines represent communication chan-\\nnels. Hdenotes the human users that participate and share the digital space.\\n6 Bot Ethics and Trust\\nConcerns and challenges in AGI are diverse. They touch on various aspects\\nof society and politics and have real-world implications, such as the impact of\\nuser-like bots on privacy, security, ethics, and trust with humans [ 47–49]. User-\\nlike bots, emulating human users’ perceptual and interaction techniques, caneasily pass bot detection tests and risk exploitation for malicious use cases to\\ndeceive and attack web platforms. They could also extend their perceptual capa-',\n",
       "  'bilities beyond the web with connected devices such as microphones and cam-eras, aﬀecting the personal privacy of others. Possible threats include spamming,\\ncyberattacks to overwhelm platforms, and even unfair use of web platforms for\\nsurveillance or illicit ﬁnancial gains. In WoT context, for instance, bots couldaﬀect smart factories and automated services in the real world, compromising\\nphysical devices and processes with signiﬁcant security implications [ 50].\\nHypothetically, intelligent social bots could share their beliefs on social plat-\\nforms similar to or better than any human user, with superb reasoning and\\nargumentation skills. These cases could negatively impact society by exposingpeople and software agents to unexpected, misaligned norms and moral beliefs.\\nFurthermore, deploying advanced cognitive bots as digital workforces may result',\n",
       "  'in unforeseen negative economic consequences. Short-term issues could includeunemployment, while long-term concerns may involve ethical dilemmas sur-\\nrounding bot ownership rights, bot farming, or ‘enslavement’ [ 47]. Accordingly,\\nthese ethical concerns may aﬀect the legality of cognitive bot development,potentially impeding their engineering and deployment. Alternatively, this could\\nintroduce new legal aspects regarding regulation, standards, and ethics for inte-\\ngrating and governing bots within emerging socio-technical ecosystems [ 50].\\nDespite these concerns, bots’ current and potential applications can posi-\\ntively impact numerous aspects of society. Cognitive automation, for example, is\\ndriving increased demand for cognitive agents in Industry 4.0, digital twins, andother digital environments [ 6,7,51]. Early implementations, like Wikipedia bots,\\nalready play a signiﬁcant role in fact-checking and other knowledge-based tasks.',\n",
       "  'On platforms like GitHub, bots assist and automate development tasks [ 52].\\nFuture cognitive bots could also beneﬁt society by participating in knowl-\\nedge processing and providing valuable new scientiﬁc insights, such as medicaladvancements, which signiﬁcantly outweigh their potential risks.\\nToday, digital platforms handle simple crawling and API-based bots with\\ncrawling policies and controlled exposure of APIs. However, advanced user-likebots like the ones envisioned in this report will require more complex mechanisms\\nto govern and control their behavior and belief-sharing [ 47,50]. One approach',\n",
       "  '112 H. K. Gidey et al.\\ntowards this is ethics and trust by design, which recommends protocols and\\npolicies for developers and engineering organizations to incorporate trust mod-els and ethical frameworks at the design and architectural stages [ 47]. Another\\napproach proposes norms and user policies with penalties for agents to acknowl-\\nedge, understand, and adhere to, similar to what human users would do ondigital platforms [ 50,53]. In return, norm and value-aware bots could establish\\nparticipation, trust, and compliance while facing the consequences of noncom-\\npliance. They may also contribute to revising and creating collective values andnorms, possibly becoming part of viable socio-technical ecosystems [ 50,54].\\nHowever, ensuring safety and trust in such ecosystems will require diverse\\napproaches. In addition to providing machine-readable norms and policies tar-geting cognitive agents, it is essential to tackle ethical and trust issues with',\n",
       "  'transparent and explainable design and engineering processes at each stage. For\\ninstance, the European Union (EU) recommends a three-phase human interven-\\ntion approach at the design phase, at the development and training phase, and\\nat runtime with oversight and override possibilities [ 55]. As a result, research\\non developing advanced cognitive bots must also address critical challenges\\nin engineering trustworthy, secure, and veriﬁable AGI bots employing hybrid\\napproaches.\\n7 Conclusion\\nThe study presented architectural research challenges in designing and devel-oping a new line of user-like cognitive bots operating autonomously on digital\\nplatforms. Key challenges, such as the transduction problem, are discussed in\\nthe context of digital web platforms’ access, user-like visual interaction, andautonomous navigation. In the architecture, we recommend bot-environment\\nseparation to realize bot autonomy and bot skeleton and behavior model separa-',\n",
       "  'tion for better evolvability. Also, bot communication capabilities should includediverse possibilities like email, dialogue systems, and blogging. We recommend\\nutilizing shared behavior models for transfer learning or collective intelligence to\\nenact generalizable behavior. Finally, we discussed cognitive bots’ ethical impli-cations and potential long-term eﬀects, proposing to adopt hybrid approaches\\nthat incorporate these aspects into the architecture and the life cycle of bots.\\nAs an outlook, a good starting point for future work would be to concep-\\ntualize a detailed implementation architecture and construct a bot by utilizing\\nexisting cognitive models. These systems can demonstrate the concept and allow\\nfurther detailed analysis through empirical data and benchmarks.\\nReferences\\n1. Lebeuf, C.R.: A taxonomy of software bots: towards a deeper understanding of\\nsoftware bot characteristics. Ph.D. thesis, UVic (2018)',\n",
       "  '2. Etzioni, O.: Intelligence without robots: a reply to brooks. AI Mag. 14(4), 7 (1993)',\n",
       "  'Towards Cognitive Bots: Architectural Research Challenges 113\\n3. Charpenay, V., Amundsen, M., Mayer, S., Padget, J., Schraudner, D., Vachtse-\\nvanou, D.: 6.2 aﬀordances and signiﬁers. In: Autonomous Agents on the Web, p.\\n67 (2021)\\n4. Lemee, J., Vachtsevanou, D., Mayer, S., Ciortea, A.: Signiﬁers for aﬀordance-driven\\nmulti-agent systems. In: EMAS (2022)\\n5. Goertzel, B.: Toward a formal model of cognitive synergy. In: AGI (2017)6. Ivanˇ ci´c, L., Suˇ sa Vugec, D., Bosilj Vukˇ si´c, V.: Robotic process automation: system-\\natic literature review. In: Di Ciccio, C., et al. (eds.) BPM 2019. LNBIP, vol. 361, pp.\\n280–295. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-30429-4\\n19\\n7. Engel, C., Ebel, P., Leimeister, J.M.: Cognitive automation. Electron. Markets\\n32(1), 339–350 (2022). https://doi.org/10.1007/s12525-021-00519-7\\n8. Russell, S.J.: Artiﬁcial Intelligence a Modern Approach. Pearson Education (2010)',\n",
       "  '9. Yampolskiy, R.V.: AI-complete, AI-hard, or AI-easy-classiﬁcation of problems in\\nAI. In: MAICS (2012)\\n10. Goertzel, B., Pennachin, C., Geisweiller, N.: Engineering General Intelligence, Part\\n1. Atlantis Thinking Machines, vol. 5. Atlantis Press, Paris (2014)\\n11. Vernon, D., von Hofsten, C., Fadiga, L.: Desiderata for developmental cognitive\\narchitectures. BICA 18, 116–127 (2016)\\n12. Goertzel, B., Pennachin, C., Geisweiller, N.: Engineering General Intelligence, Part\\n2, vol. 6. Springer, Heidelberg (2014). https://doi.org/10.2991/978-94-6239-030-0\\n13. Rosenbloom, P.S.: Thoughts on architecture. In: Goertzel, B., Ikl´ e, M., Potapov,\\nA., Ponomaryov, D. (eds.) AGI. MNCS, vol. 13539, pp. 364–373. Springer, Cham\\n(2023). https://doi.org/10.1007/978-3-031-19907-3 35\\n14. Lieto, A., Bhatt, M., Oltramari, A., Vernon, D.: The role of cognitive architectures\\nin general artiﬁcial intelligence. Cogn. Syst. Res. 48, 1–3 (2018)',\n",
       "  '15. Fountain, A.M., Hall, W., Heath, I., Davis, H.C.: MICROCOSM: an open model\\nfor hypermedia with dynamic linking. In: ECHT (1990)\\n16. Nelson, T.H.: Complex information processing: a ﬁle structure for the complex,\\nthe changing and the indeterminate. In: ACM National Conference (1965)\\n17. Gibson, J.J.: The theory of aﬀordances, Hilldale, USA, vol. 1, no. 2, pp. 67–82\\n(1977)\\n18. Vachtsevanou, D., Ciortea, A., Mayer, S., Lem´ ee, J.: Signiﬁers as a ﬁrst-class\\nabstraction in hypermedia multi-agent systems. In: EKAW (2023)\\n19. Wooldridge, M.: An Introduction to Multiagent Systems. Wiley, Hoboken (2009)\\n20. Brooks, R.A.: Intelligence without representation. AI 47(1–3), 139–159 (1991)\\n21. Ciortea, A., Mayer, S., Boissier, O., Gandon, F.: Exploiting interaction aﬀordances:\\non engineering autonomous systems for the web of things (2019)\\n22. Gur, I., Rueckert, U., Faust, A., Hakkani-Tur, D.: Learning to navigate the web.\\narXiv (2018)',\n",
       "  '23. Shi, T., Karpathy, A., Fan, L., Hernandez, J., Liang, P.: World of bits: an open-\\ndomain platform for web-based agents. In: ICML, pp. 3135–3144. PMLR (2017)\\n24. Gur, I., et al.: Understanding HTML with large language models. arXiv (2022)\\n25. Huang, S., et al.: Language is not all you need: aligning perception with language\\nmodels. arXiv (2023)\\n26. Pennachin, C., Goertzel, B.: Contemporary approaches to artiﬁcial general intelli-\\ngence. In: Goertzel, B., Pennachin, C. (eds.) AGI. COGTECH, pp. 1–30. Springer,Heidelberg (2007). https://doi.org/10.1007/978-3-540-68677-4\\n1\\n27. Computing machinery and intelligence-AM turing (1950)\\n28. Taylor, T., et al.: WebAL comes of age: a review of the ﬁrst 21 years of artiﬁcial\\nlife on the web. Artif. Life 22(3), 364–407 (2016)',\n",
       "  '114 H. K. Gidey et al.\\n29. Sch¨ oner, G.: Dynamical systems approaches to cognition. In: Cambridge Handbook\\nof Computational Cognitive Modeling, pp. 101–126 (2008)\\n30. Vernon, D.: Cognitive architectures. In: Cognitive Robotics. MIT Press (2022)31. Fan, X., Markram, H.: A brief history of simulation neuroscience. Front. Neuroin-\\nform. 13, 32 (2019)\\n32. Walter, F.: Advanced embodied learning. Ph.D. thesis, TUM (2021)33. Knoll, A., Walter, F.: Neurorobotics-a unique opportunity for ground breaking\\nresearch (2019)\\n34. Momennejad, I.: A rubric for human-like agents and NeuroAI. Philos. Trans.\\n378(1869), 20210446 (2023)\\n35. Maass, W.: Networks of spiking neurons: the third generation of neural network\\nmodels. Neural Netw. 10(9), 1659–1671 (1997)\\n36. Knoll, A., Gewaltig, M.-O., Sanders, J., Oberst, J.: Neurorobotics: a strategic pillar\\nof the human brain project. Sci. Robot. 2–3 (2016)\\n37. Soon, G.K., On, C.K., Anthony, P., Hamdan, A.R.: A review on agent communi-',\n",
       "  'cation language. In: ICCST, pp. 481–491 (2019)\\n38. H¨ ubner, J.F.: 4.5 about the place of agents in the web. In: Autonomous Agents on\\nthe Web, p. 44 (2021)\\n39. Hillmann, P., Uhlig, T., Rodosek, G.D., Rose, O.: A novel multi-agent system for\\ncomplex scheduling problems. In: WSC 2014. IEEE (2014)\\n40. Holz, T., Campbell, A.G., O’Hare, G.M.P., Staﬀord, J.W., Martin, A., Dragone,\\nM.: Mira-mixed reality agents. IJHC 69(4), 251–268 (2011)\\n41. Minsky, M.: Society of Mind. Simon and Schuster (1988)42. Tononi, G.: Integrated information theory. Scholarpedia 10(1), 4164 (2015)\\n43. Friston, K.: The free-energy principle: a uniﬁed brain theory? Nat. Rev. Neurosci.\\n11(2), 127–138 (2010)\\n44. Goertzel, B.: Artiﬁcial general intelligence: concept, state of the art, and future\\nprospects. AGI 5(1), 1 (2014)\\n45. Dorri, A., Kanhere, S.S., Jurdak, R.: Multi-agent systems: a survey. IEEE Access\\n6, 28573–28593 (2018)\\n46. Boissier, O., Ciortea, A., Harth, A., Ricci, A.: Autonomous agents on the web. In:',\n",
       "  'DS 21072, p. 100p (2021)\\n47. Goertzel, B., Pennachin, C., Geisweiller, N., Goertzel, B., Pennachin, C.,\\nGeisweiller, N.: The engineering and development of ethics. In: Engineering Gen-\\neral Intelligence, Part 1, pp. 245–287 (2014)\\n48. Christian, B.: The Alignment Problem: Machine Learning and Human Values. WW\\nNorton & Company (2020)\\n49. Coeckelbergh, M.: AI Ethics. MIT Press, Cambridge (2020)50. Kampik, T., et al.: Norms and policies for agents on the web. In: Autonomous\\nAgents on the Web (2021)\\n51. Vogel-Heuser, B., Seitz, M., Cruz Salazar, L.A., Gehlhoﬀ, F., Dogan, A., Fay,\\nA.: Multi-agent systems to enable industry 4.0. at-Automatisierungstechnik 68(6),\\n445–458 (2020)\\n52. Hukal, P., Berente, N., Germonprez, M., Schecter, A.: Bots coordinating work in\\nopen source software projects. Computer 52(9), 52–60 (2019)\\n53. Kampik, T., et al.: Governance of autonomous agents on the web: challenges and\\nopportunities. ACM TOIT 22(4), 1–31 (2022)',\n",
       "  '54. Patrick, A.S.: Building trustworthy software agents. IEEE IC 6(6), 46–53 (2002)\\n55. Kaur, D., Uslu, S., Rittichier, K.J., Durresi, A.: Trustworthy artiﬁcial intelligence:\\na review. ACM CSUR 55(2), 1–38 (2022)',\n",
       "  'Bridging AGI Theory and Practice\\nwith Galois Connections\\nBen Goertzel1,2(B)\\n1OpenCog Foundation, Rockville, USA\\nben@goertzel.org\\n2SingularityNET Foundation, Amsterdam, The Netherlands\\nAbstract. Multiple cognitive algorithms posited to play a key role in\\nAGI (forward and backward chaining inference, clustering and concept\\nformation, evolutionary and reinforcement learning, probabilistic pro-\\ngramming, etc.) are given a common formulation as recursive discretedecision processes involving optimizing functions deﬁned over meta-\\ngraphs, in which the key decisions involve sampling from probability\\ndistributions over metagraphs and enacting sets of combinatory opera-tions on selected sub-metagraphs. This forms a bridge between abstract\\nconceptions of general intelligence founded on notions of algorithmic\\ninformation and complex systems theory, and the practical design ofmulti-paradigm AGI systems.\\n1 Introduction\\nThe pursuit of AGI has an abstract theoretical aspect, in which the focus is',\n",
       "  'understanding what intelligence is at a fundamental level going beyond any par-\\nticular biological organism or engineered system. It also has an acutely practical\\naspect, in which one is trying to build particular systems with speciﬁc resources\\nand application foci, much like building any other machine (albeit with someunique aspects given that this sort of machine is expected to take over its own\\nredesign and re-engineering process).\\nConnecting the theoretical and practical aspects of AGI is a major challenge,\\nwhich if done well can enhance both aspects. Here we present some ideas aimed at\\nﬂeshing out this connection, in the speciﬁc context of cross-paradigm metagraph-\\nbased AI approaches like the OpenCog family of systems.\\nPerhaps the best known approach to the abstract formulation of AGI is\\nthe algorithmic-information-theory-driven angle, as represented by AIXI [ 10],\\nGodel Machine [ 13] and their relatives. These abstract AGI systems have the',\n",
       "  'general form of “reinforcement learning” or “experiential interactive learning”\\nalgorithms, meaning that they operate via iteratively observing the world, then\\nchoosing actions that they expect will give them maximum reward based onthe world’s reactions, etc. They are unrealistic because their action selection is\\nuncomputable or at best computationally intractable (though eﬀorts have been\\nmade to scale them down [ 1,14,16]).\\nc⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2023\\nP. Hammer et al. (Eds.): AGI 2023, LNAI 13921, pp. 115–125, 2023.https://doi.org/10.1007/978-3-031-33469-6\\n_12',\n",
       "  '116 B. Goertzel\\nAn alternate way of conceiving AGI is Weaver’s notion of “Open Ended Intelli-\\ngence” [ 17], which considers intelligences as complex, self-organizing systems that\\ninteract with their environments in such a way as to pursue the two complemen-\\ntary, often conﬂicting goals of individuation (maintaining system boundaries and\\ncoherence) and self-transcendence (developing and acquiring new properties and\\naspects, including those that would be incomprehensible to earlier system ver-\\nsions). This approach does not make unrealistic assumptions, but possesses a ﬂu-\\nidity that renders its rigorous application to speciﬁc cases an interesting challenge.\\nIn recent papers such as The General Theory of General Intelligence [7]a n d\\nPatterns of Cognition [6] I have sought to provide one sort of conceptual and\\nmathematical bridge between these general-purpose AGI frameworks and prac-tical real-world AGI-oriented systems, via looking at formulations of the AI',\n",
       "  'algorithms playing key roles in the OpenCog AI system in terms of abstract\\nrecursive discrete decision systems. This paper gives a concise overview of a few\\nof the key ideas from these longer works.\\nThe DDSs (Discrete Decision Systems) we propose on the one hand can\\nbe straightforwardly viewed as scaled down versions of AIXI\\ntlor time-bounded\\nGodel Machine type systems, but on the other hand can be used to drive con-\\ncrete thinking about functional programming implementations of OpenCog algo-rithms, and understood as seeds for the self-modifying self-organization con-\\nceived in Open-Ended Intelligence theory.\\nThe Patterns of Cognition analysis involves representing various cognitive\\nalgorithms as recursive discrete decision processes involving optimizing func-\\ntions deﬁned over metagraphs, in which the key decisions involve sampling from',\n",
       "  'probability distributions over metagraphs and enacting sets of combinatory oper-ations on selected sub-metagraphs. A variety of recursive decision process called\\na COFO (Combinatory Function Optimization) algorithm plays a key role. One\\ncan view a COFO as being vaguely like Monte-Carlo-AIXI, but within the con-text of a combinatory computational model – and with the added twist that the\\nMonte Carlo sampling based estimations are augmented by estimations using\\nother probabilistic algorithms that are themselves implemented using COFO.\\nThere are close connections to modern probabilistic programming theory [ 11],\\nbut with more of an emphasis on recursive inference algorithms and less relianceon simplistic sampling methods.\\nBehind the scenes of the COFO framework is a core insight drawn from the\\nbody of theory behind the OpenCog system – that a combinatory computationalmodel deﬁned over metagraphs is an especially natural setting in which to for-',\n",
       "  'malize various practical AGI-oriented algorithms. From a suﬃciently abstract\\nperspective, all Turing-complete computational models are equivalent, and allgeneral-purpose computational data structures are equivalent. But from a prac-\\ntical AGI implementation and teaching perspective, it makes a diﬀerence which\\ncomputational models and data structures one chooses; the argument for meta-graphs as the core data structure for AGI has been laid out in [ 3] and references\\ntherein such as [ 2], and the argument for combinatory computing as the core\\napproach for AGI has been laid out in [ 9] and earlier papers referenced therein.',\n",
       "  'Bridging AGI Theory and Practice with Galois Connections 117\\n2 Discrete Decision Systems\\nTo bridge the gap between abstract AGI agent models and practical AGI sys-\\ntems, we introduce a basic model of a discrete decision system (DDS) – a process\\ndeﬁned on nstages in which each stage t=1 ,...,n is characterized by\\n–a n initial state st∈St, where Stis the set of feasible states at the beginning\\nof stage t;\\n–a n action or “decision variable” xt∈Xt, where Xtis the set of feasible\\nactions at stage t– note that Xtmay be a function of the initial state st;\\n–a n immediate cost/reward function pt(st,xt), representing the cost/re-\\nward at stage tifstis the initial state and xtthe action selected;\\n–a state transition function gt(st,xt) that leads the system towards state\\nst+1=gt(st,xt).\\nThe mapping of the simple agents model given above into this framework is\\nfairly direct: environments determine which actions are feasible at each point in',\n",
       "  'time and goals are assumed decomposable into stepwise reward functions. Highly\\ngenerally intelligent agents like AIXItlﬁt into this framework, but so do prac-\\ntical AI algorithm frameworks like greedy optimization and deterministic and\\nstochastic dynamic programming. As we shall see, with some care and further\\nmachinery the various cognitive algorithms utilized in the OpenCog framework\\ncan be interpreted as DDSs as well.\\nTo express greedy optimization in this framework, one begins with an initial\\nstate, chosen based on prior knowledge or via purely randomly or via appropri-\\nately biased stochastic selection. Then one chooses an action with a probability\\nproportional to immediate cost/reward (or based on some scaled version of thisprobability). Then one enacts the action, the state transition, and etc.\\nAn interesting case of “greedy” style DDS dynamics in an AGI context is the',\n",
       "  'adaptive spreading of attention through a complex network. OpenCog’s atten-tional dynamics subsystem, ECAN (Economic Attention Networks), involves\\nspreading of two types of attention values through a knowledge metagraph –\\nShort-Term Importance (STI) and Long-Term Importance (LTI) values, repre-senting very roughly the amount of processor time an Atom should receive in\\nthe near term, and the criticalness of keeping an Atom in RAM in the near term.\\nIn this case: an initial state is a distribution of STI and LTI values across the\\nAtoms in an Atomspace; an action is the spreading of some STI or LTI from\\none Atom to its neighbors; an immediate cost/reward function is the degree\\nto which a given spreading action causes the distribution of STI/LTI values to\\nbetter approximate the actual expected utilities of assignation of processor time\\nand RAM to the Atoms in Atomspace; a state transition function is the\\nupdating of the overall set of STI/LTI values; and the ECAN equations in the',\n",
       "  'OpenCog system embody a greedy heuristic for executing this DDS.\\nTo express dynamic programming in this DDS framework is a little subtler,\\nas in DP one tries to choose actions with probability proportional to overall\\nexpected cost/reward. Estimating the overall expected cost/reward of an action',\n",
       "  '118 B. Goertzel\\nsequence requires either an exhaustive exploration of possibilities (i.e. full-on\\ndynamic programming) or else some sort of heuristic sampling of possibilities(approximate stochastic dynamic programming).\\nTo handle concurrency in this framework, one can posit underlying atomic\\nactions w\\nt∈Wt, and then deﬁne the members of Xtas subsets of Wt. In this\\ncase each action xtrepresents a set of wtbeing executed concurrently.\\n3 Combinatory-Operation-Based Function Optimization\\nTo frame the sorts of cognitive algorithms involved in OpenCog and related\\nAGI architectures in terms of general DDS processes, [ 6] introduces the notion\\nof COFO, Combinatory-Operation-Based Function Optimization. Basically, aCOFO process wraps a combinatory computational system of the sort considered\\nin [4] and [ 7] within a DDS, by using the combinatory system as the method',\n",
       "  'of choosing actions in a discrete decision process oriented toward optimizing afunction. The hypothesis is then made that this particular sort of DDS plays\\na core role in practical AGI systems operating in environments relevant to our\\nphysical universe and the everyday human world.\\nMore speciﬁcally, we envision a cognitive system controlling an agent in an\\nenvironment to be roughly describable as a DDS (the “top-level DDS”), andthen envision the cognitive processing used for action selection in the DDS as\\ncomprising: 1) A memory consisting of a set of entities that combine with each\\nother to produce other entities, i.e. a combinatory system embodied in a knowl-edge metagraph; 2) Cognitive processes instantiated as COFO processes, i.e.\\nas DDSs whose goals are function optimizations and whose actions are func-\\ntion evaluations, all leveraging a common metagraph as background knowledgeand as a dynamic store for intermediate state; 3) One or more DDSs carrying',\n",
       "  'out attention allocation on the common metagraph (the core DDS here using\\ngreedy heuristics but supplemented by one or more additional DDSs using moreadvanced cognition), spanning the portions of the metagraph focused on by the\\nvarious COFO processes.\\nSo practical intelligent systems are modeled as multi-level DDSs where the\\nsubordinate DDSs operating within the outer-loop agent control DDS are mostly\\nCOFO processes. In [ 7] some eﬀort is taken to explore how the various COFO-\\nlike processes involved in human-like cognition appear to interoperate in human\\ncognitive architecture, and more speciﬁcally how the OpenCog Hyperon design\\nexplicitly interleaves COFO processes in its attempt to manifest advanced AGI.\\nA COFO process, more explicitly, involves making of a series of decisions\\ninvolving how to best use a set of combinatory operators C\\nito gain information\\nabout maximizing a function F(or Pareto optimizing a set of functions {Fi})v i a',\n",
       "  'sampling evaluations of F({Fi}). For simplicity we’ll present this process in the\\ncase of a single function Fbut the same constructs work for the multiobjective\\ncase. It is shown in [ 6] how COFO can be represented as a discrete decision\\nprocess, which can then be enacted in greedy or dynamic programming style.\\nGiven a function F:X→ R(where Xis any space with a probability\\nmeasure on it and Ris the reals), let Ddenote a “dataset” comprising ﬁnite',\n",
       "  'Bridging AGI Theory and Practice with Galois Connections 119\\nsubset of the graph G(F)o f F, i.e. a set of pairs ( x, F(x)). We want to introduce\\nam e a s u r e qF(D) which measures how much guidance Dgives toward the goal\\nof ﬁnding xthat make F(x) large. The best measure will often be application-\\nspeciﬁc; however as shown in [ 6] one can also introduce general-purpose entropy-\\nbased measures that apply across domains and problems.\\nWe can then look at greedy or dynamic programming processes aimed at\\ngradually building a set Din a way that will maximize qρ,F(D). Speciﬁcally, in\\na cognitive algorithmics context it is interesting to look at processes involvingcombinatory operations C\\ni:X×X→Xwith the property that P(Ci(x, y)∈\\nMD\\nρ|x∈MD\\nρ,y∈MD\\nρ)≫ P(z∈MD\\nρ|z∈X). That is, given x, y∈MD\\nρ,\\ncombining xand yusing Cihas surprisingly high probability of yielding z∈MD\\nρ.\\nGiven combinatory operators of this nature, one can then approach gradually',\n",
       "  'building a set Din a way that will maximize qρ,F(D), via a route of successively\\napplying combinatory operators Cito the members of a set Djto obtain a set\\nDj+1.\\nFraming this COFO process as a form of recursive Discrete Decision System\\n(DDS), we obtain:\\n1. A state stis a dataset Dformed from function F\\n2. An action is the formation of a new entity zby\\n(a) Sampling x, yfrom Xand Cifrom the set of available combinatory oper-\\nators, in a manner that is estimated likely to yield z=Ci(x, y) with\\nz∈MD\\nρ\\ni. As a complement or alternative to directly sampling, one can perform\\nprobabilistic inference of various sorts to ﬁnd promising ( x, y, C i).\\nThis probabilistic inference process itself may be represented as aCOFO process, as we show below via expressing PLN forward and\\nbackward chaining in terms of COFO\\n(b) Evaluating F(z), and setting D\\n∗=D∪(z, F(z)).\\n3. The immediate reward is an appropriate measure of the amount of new',\n",
       "  'information about making Fbig that was gained by the evaluation F(z).\\nThe right measure may depend on the speciﬁc COFO application; one fairlygeneric choice would be the relative entropy q\\nρ,F(D∗,D)\\n4.State transition : setting the new state st+1=D∗\\nA concurrent-processing version of this would replace 2a with a similar step in\\nwhich multiple pairs ( x, y) are concurrently chosen and then evaluated.\\nThe action step in a COFO process is in essence carrying out a form of\\nprobabilistic programming [ 11] (which is clear from the discussion of probabilis-\\ntic programming in a dependent type context given in [ 5]). Finding the right\\nconglomeration of combinatory operators to produce a given output is formally\\nequivalent to ﬁnding the right program to produce a given sort of output, andhere as in probabilistic programming one is pushed to judiciously condition esti-\\nmates on prior knowledge.\\nIn the case where one pursues COFO via dynamic programming, it becomes',\n",
       "  'stochastic dynamic programming because of the probabilistic sampling in the',\n",
       "  '120 B. Goertzel\\naction. If probabilistic inference is used along with sampling, then one may have\\na peculiar sort of approximate stochastic dynamic programming in which thestep of choosing an action involves making an estimation that itself may be\\nusefully carried out by stochastic dynamic programming (but with a diﬀerent\\nobjective function than the objective function for whose optimization the actionis being chosen).\\nBasically, in the COFO framework one looks at the process of optimizing F\\nas an explicit dynamical decision process conducted via sequential applicationof an operation in which: Operations C\\nithat combine inputs chosen from a\\ndistribution induced by prior objective function evaluations, are used to get new\\ncandidate arguments to feed to Ffor evaluation. The reward function guiding\\nthis exploration is the quest for reduction of the entropy of the set of guesses at\\narguments that look promising to make Fnear-optimal based on the evaluations\\nmade so far.',\n",
       "  'The same COFO process can be applied equally well the case of Pareto-\\noptimizing a set of objective functions. The deﬁnition of MD\\nρmust be modiﬁed\\naccordingly and then the rest follows.\\nActually carrying out an explicit stochastic dynamic programming algorithm\\naccording to the lines described above, will prove computationally intractablein most realistic cases. However, we shall see below that the formulation of the\\nCOFO process as dynamic programming (or simpler greedy sequential choice\\nbased optimization) provides a valuable foundation for theoretical analysis.\\n4 Cognitive Processes as COFO-Guided Metagraph\\nTransformations\\nCOFO is a highly general framework, and to use it to structure speciﬁc AI\\nsystems one has to take the next step and introduce speciﬁc sets of combina-tory operations, often associated with speciﬁc incremental reward functions in\\nthe spirit of (but often not identical) the information-theoretic reward approach',\n",
       "  'hinted above. In [ 6] explicit discussion is given to the COFO expression of a vari-\\nety of cognitive algorithms used in the OpenCog AGI approach: Logical reason-\\ning, evolutionary program learning, metagraph pattern mining, agglomerative\\nclustering and activation-spreading-based attention allocation.\\nWe will focus mainly here on AGI architectures such as OpenCog that have\\nmetagraphs as core meta-representational data structures – thus placing meta-graphs in a dual role: 1) As a fundamental means of analyzing what the AGI\\nsystem is doing from a conceptual and phenomenological perspective; 2) As the\\ncore data structure the AGI system uses to store various sorts of information asit goes about its business.\\nIn this sort of AGI architecture, the expression of logical inference, program\\nlearning and pattern mining in combinatory-system terms ties directly back tothe discussion of distinction metagraphs and associated patterns in [ 7]. Logical',\n",
       "  'inference rules can be considered as transformations on distinction metagraphs.\\nBidirectional inference rules (expressed using coimplication) are rules mapping',\n",
       "  'Bridging AGI Theory and Practice with Galois Connections 121\\nbetween two distinction metagraphs that have diﬀerent surface form but ulti-\\nmately express the same distinctions between the same observations. Programscan be viewed, using Curry-Howard type mappings, as series of steps for enact-\\ning these logical-inference-rule transformation on metagraphs, where the steps\\nare to be carried out on an assumed reference machine. The reference machineitself may also be represented as a distinction metagraph with temporal links\\nused to express the transitions involved in computations. Pattern mining can be\\nexpressed in terms of formal patterns in metagraphs. Clustering can be viewedas a sort of metagraph transformation that creates new ConceptNodes grouping\\nnodes into categories. Etc.\\nIn this context, COFO presents itself as a way of structuring processes\\nvia which sub-metagraphs transform other sub-metagraphs into yet other sub-',\n",
       "  'metagraphs, where the submetagraphs are interpreted as combinators and are\\ncombined via a systematic recursive process toward the incremental increase of\\na particular reward function. And the common representation of multiple COFO\\nprocesses involved in achieving the overall multiple-goal-achieving activities of atop-level DDS in terms of a shared typed metagraph is one way to facilitate the\\ncognitive synergy needed to achieve high levels of general intelligence under prac-\\ntical resource constraints. The reliance on a common metagraph representationmakes it tractable for the multiple cognitive algorithms to share intermediate\\nstate as they pursue their optimization goals, which enables the cognitive-synergy\\ndynamic in which each process is able to call on other processes in the systemfor assistance when it runs into trouble.\\n5 COFO Processes as Galois Connections\\nFor some of the cognitive algorithms treated in COFO terms in [ 6] one requires',\n",
       "  'a variety of COFO that uses greedy optimization to explore the dag of pos-\\nsibilities, for others one requires a variety of COFO that uses some variationon approximation stochastic dynamic programming. In either case, one can use\\nthe “programming with Galois connections” approach from [ 12] to formalize the\\nderivation of practical algorithmic approaches. Roughly, in all these cases, Galoisconnections are used to link search and optimization processes on directed meta-\\ngraphs whose edge targets are labeled with probabilistic dependent types, and\\none can then show that – under certain assumptions – these connections are ful-ﬁlled by processes involving metagraph chronomorphisms (where a chronomor-\\nphism is a fold followed by an unfold, where both the fold and unfold are allowed\\nto accumulate and propagate long-term memory as they proceed).\\n5.1 Greedy Optimization as Folding\\nSuppose we are concerned with maximizing a function f:X→Rvia a “pattern',\n",
       "  'search” approach. That is, we assume an algorithm that repeatedly iterates a\\npattern search operation such as: Generates a set of candidate next-steps from\\nits focus point a, evaluates the candidates, and then using the results of this',\n",
       "  '122 B. Goertzel\\nevaluation, chooses a new focus point a∗. Steepest ascent obviously has this\\nformat, but so do a variety of derivative-free optimization methods as reviewede.g. in [ 15].\\nEvolutionary optimization may be put in this framework if one shifts atten-\\ntion to a population-level function f\\nP:XN→ Rwhere XNis a population\\nofNelements of X, and deﬁnes fP(x)f o r x∈XNas e.g. the average of f(x)\\nacross x∈XN(so the average population ﬁtness, in genetic algorithm terms).\\nThe focus point ais a population, which evolves into a new population a∗via\\ncrossover or mutation – a process that is then ongoingly iterated as outlined\\nabove.\\nThe basic ideas to be presented here work for most any topological space X\\nbut we are most interested in the case where Xis a metagraph. In this case\\nthe pattern search iteration can be understood as a walk across the metagraph,\\nmoving from some initial position in the graph to another position, then another\\none, etc.',\n",
       "  'We can analyze this sort of optimization algorithm via the Greedy Theorem\\nfrom [ 12],\\nTheorem 1 (Theorem 1 from [ 12]).(|S↾R|)⊆(|S|)↾Rif R is transitive and\\nS satisﬁes the “monotonicity condition” R◦←SFR◦\\nwhich leverages a variety of idiosyncratic notation: RS←−FRindicates S·FR⊆\\nR·S;(|S|) means the operation of folding S;⟨µX::fX⟩denotes the least ﬁxed\\npoint of f;T◦means the converse of T, i.e. ( b, a)∈R◦≡(a, c)∈R;S↾R\\nmeans “ Sshrunk by R”, i.e. S∩R/S◦. Here Srepresents the local candidate-\\ngeneration operation used in the pattern-search optimization algorithm, and R\\nrepresents the operation of evaluating a candidate point in Xaccording to the\\nobjective function being optimized.\\nIf the objective function is not convex, then the theorem does not hold, but\\nthe greedy pattern-search optimization may still be valuable in a heuristic sense.This is the case, for instance, in nearly all real-world applications of evolutionary',\n",
       "  'programming, steepest ascent or classical derivative-free optimization methods.\\n5.2 Galois Connection Representations of Dynamic Programming\\nDecision Systems Involving Mutually Associative Combinatory\\nOperations\\nNext we consider how to represent dynamic programming based execution of\\nDDSs using folds and unfolds. Here our approach is to leverage Theorem 2\\nin [12] which is stated as\\nTheorem 2 (Theorem 2 from [ 12]).Assume Sis monotonic with respect to R,\\nthat is, RS←−FRholds, and dom(T)⊆dom(S·FM).T h e n\\nM=( (|S|)·(|T|)◦)↾R⇒⟨ µX::(S·FX·T◦)↾R⟩⊆ M',\n",
       "  'Bridging AGI Theory and Practice with Galois Connections 123\\nConceptually, T◦transforms input into subproblems, e.g. for backward chain-\\ning inference, it chooses ( x, y, C ) so that z=C(x, y) has high quality (e.g.\\nCWIG); for forward chaining, it chooses x, y, C so that z = C(x, y) has high\\ninterestingness (e.g. CWIG).\\nFXﬁgures out recursively which combinations give maximum immediate reward\\naccording to the relevant measure. These optimal solutions are combined and\\nthen the best one is picked by ↾R, which is the evaluation on the objective\\nfunction. Caching results to avoid overlap may be important here in practice(and is what will give us histomorphisms and futumorphisms instead of simple\\nfolds and unfolds).\\nThe ﬁx-point based recursion/iteration speciﬁed by the theorem can of course\\nbe approximatively rather than precisely solved – and doing this approxima-\\ntion via statistical sampling yields stochastic dynamic programming. Roughly',\n",
       "  'speaking the approach symbolized by M=( (|S|)·(|T|)\\n◦)↾Rbegins by apply-\\ning all the combinatory operations to achieve a large body of combinations-of-\\ncombinations-of-combinations- ..., and then shrinks this via the process of opti-\\nmality evaluation. On the other hand, the least-ﬁxed-point version on the rhs of\\nthe Theorem iterates through the combination process step by step (executing\\nthe fold).\\n6 Associativity of Combinatory Operations Enables\\nRepresenting Cognitive Operations as Foldingand Unfolding\\nA key insight reported in Patterns of Cognition is that the mutual associativity\\nof the combinatory operations involved in a cognitive process often plays a key\\nrole in enabling the decomposition of the process into folding and unfoldingoperations. This manifests itself for example in the result that\\nTheorem 3. A COFO decision process whose combinatory operations C\\niare\\nmutually associative can be implemented as a chronomorphism.',\n",
       "  'This general conclusion regarding mutual associativity resonates fascinat-\\ningly with the result from [ 4] mentioned above, that mutually associative com-\\nbinatory operations lead straightforwardly to subpattern hierarchies. We thus\\nsee a common mathematical property leading to elegant and practically valu-able symmetries in both algorithmic dynamics and in knowledge-representation\\nstructure. This bolsters conﬁdence that the combinatory computational model\\nis a good approach for exploring the scaling-down of generic but infeasible AGImodels toward the realm of practically usable algorithms.\\nThis conclusion regarding mutual associativity also has some practical impli-\\ncations for the particulars of cognitive processes such as logical reasoning and\\nevolutionary learning. For instance, one can see that mutually associativity\\nholds among logical inference rules if one makes use of reversible logic rules(co-implications rather than implications), and for program execution processes',\n",
       "  '124 B. Goertzel\\nif one makes use of reversible computing. It is also observed that where this\\nmutual associativity holds, there is an alignment between the hierarchy of sub-goals used in recursive decision process execution and subpattern hierarchies\\namong patterns represented in the associated knowledge metagraph.\\nIn the PLN inference context, for example, the approach to PLN inference\\nusing relaxation rather than chaining outlined in [ 8] is one way of ﬁnding the\\nﬁxed point of the recursion associated with the COFO process. What the theo-\\nrem suggests is that folding PLN inferences across the knowledge metagraph isanother way, basically boiling down to forward and backward chaining as out-\\nlined above. However, it seems this can only work reasonably cleanly for crisp\\ninference if mutual associativity among inference rules holds, which appears tobe the case only if one uses PLN rules formulated as co-implications rather than\\none-way implications.',\n",
       "  'Further, when dealing with the uncertainty-management aspects of PLN\\nrules, one is no longer guaranteed associativity merely by adopting reversibility\\nof individual inference steps. One must heuristically arrange one’s inferences asseries of co-implications whose associated distributions have favorable indepen-\\ndence relationships.\\n7 Challenges and Prospects\\nThe assumptions needed to get from the symmetry properties of discrete decision\\nprocesses to fold and unfold operations are not entirely realistic – for instance,to get the derivations to work in their most straightforward form, one needs to\\nassume the underlying metagraph remains unchanged as the folding and unfold-\\ning processes proceed. If the metagraph changes dynamically along with thefolding and unfolding – e.g. because inference processes are drawing conclusions\\nfrom the nodes and links created during the folding process, and these conclu-',\n",
       "  'sions are being placed into the metagraph concurrently with the folding processproceeding – then one loses the straightforward result that simple approximate\\nstochastic dynamic programming algorithms will approximate the optimal result\\nof the decision process. This is a serious limitation, but it must also be under-stood that in many cases the real-time changes to the metagraph incurred by\\nthe folding and unfolding process are not a signiﬁcant factor. Creating rigorous\\ntheory connecting abstract AGI theory to pragmatically relevant cognitive algo-rithms and their implementations is a complex matter inevitably involving some\\nsimpliﬁcations and approximations; the trick is to choose the right ones.\\nIf one wishes to explore open-ended, evolutionary AGI systems in which\\nmultiple algorithms constructed on diverse principles interact within a common',\n",
       "  'meta-representational fabric, then the conceptual and mathematical approachpresented here provides an avenue for relatively elegant and concise formaliza-\\ntion, putting diverse AI methods in a common framework. This framework has\\npotential to ease practical complexity and performance analysis, and also con-nects practical operational systems with broader conceptions of AGI.',\n",
       "  'Bridging AGI Theory and Practice with Galois Connections 125\\nReferences\\n1. Franz, A., Gogulya, V., L¨ oﬄer, M.: WILLIAM: a monolithic approach to AGI. In:\\nHammer, P., Agrawal, P., Goertzel, B., Ikl´ e, M. (eds.) AGI 2019. LNCS (LNAI),\\nvol. 11654, pp. 44–58. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-\\n27005-6 5\\n2. Gibbons, J.: An initial-algebra approach to directed acyclic graphs. In: M¨ oller,\\nB. (ed.) MPC 1995. LNCS, vol. 947, pp. 282–303. Springer, Heidelberg (1994).https://doi.org/10.1007/3-540-60117-1\\n16\\n3. Goertzel, B.: Folding and unfolding on metagraphs (2020). https://arxiv.org/abs/\\n2012.01759\\n4. Goertzel, B.: Grounding Occam’s razor in a formal theory of simplicity. arXiv\\npreprint arXiv:2004.05269 (2020)\\n5. Goertzel, B.: Paraconsistent foundations for probabilistic reasoning, programming\\nand concept formation. arXiv preprint arXiv:2012.14474 (2020)\\n6. Goertzel, B.: Patterns of cognition: cognitive algorithms as Galois connections',\n",
       "  'fulﬁlled by chronomorphisms on probabilistically typed metagraphs. arXiv preprint\\narXiv:2102.10581 (2021)\\n7. Goertzel, B.: Toward a general theory of general intelligence: a patternist perspec-\\ntive. arXiv preprint arXiv:2103.15100 (2021)\\n8. Goertzel, B., Pennachin, C.: How might probabilistic reasoning emerge from the\\nbrain? In: Proceedings of the First AGI Conference, vol. 171, p. 149. IOS Press(2008)\\n9. Goertzel, B., Pennachin, C., Geisweiller, N.: Engineering General Intelligence,\\nPart 1: A Path to Advanced AGI via Embodied Learning and Cognitive Syn-ergy. Atlantis Thinking Machines, Springer, Heidelberg (2013). https://doi.org/\\n10.2991/978-94-6239-027-0\\n10. Hutter, M.: Universal Artiﬁcial Intelligence: Sequential Decisions Based on Algo-\\nrithmic Probability. Springer, Heidelberg (2005). https://doi.org/10.1007/b138233\\n11. van de Meent, J.W., Paige, B., Yang, H., Wood, F.: An introduction to probabilistic\\nprogramming. arXiv preprint arXiv:1809.10756 (2018)',\n",
       "  '12. Mu, S.C., Oliveira, J.N.: Programming from Galois connections. J. Log. Algebraic\\nProgram. 81(6), 680–704 (2012)\\n13. Schmidhuber, J.: Godel machines: fully self-referential optimal universal self-\\nimprovers. In: Goertzel, B., Pennachin, C. (eds.) Artiﬁcial General Intelligence.\\nCOGTECH, pp. 119–226. Springer, Heidelberg (2006). https://doi.org/10.1007/\\n978-3-540-68677-4\\n7\\n14. Schmidhuber, J.: Optimal ordered problem solver. Mach. Learn. 54(3), 211–254\\n(2004). https://doi.org/10.1023/B:MACH.0000015880.99707.b2\\n15. Torczon, V.: Pattern search methods for nonlinear optimization. In: SIAG/OPT\\nViews and News. Citeseer (1995)\\n16. Veness, J., Ng, K.S., Hutter, M., Uther, W., Silver, D.: A Monte-Carlo AIXI\\napproximation. J. Artif. Intell. Res. 40, 95–142 (2011)\\n17. Weinbaum, D., Veitas, V.: Open ended intelligence: the individuation of intelligent\\nagents. J. Exp. Theor. Artif. Intell. 29(2), 371–396 (2017)',\n",
       "  'Comparative Reasoning for Intelligent\\nAgents\\nPatrick Hammer1(B), Peter Isaev2, Hugo Latapie3, Francesco Lanza4,\\nAntonio Chella4, and Pei Wang2\\n1Department of Psychology, Stockholm University, Stockholm, Sweden\\npatrick.hammer@psychology.su.se\\n2Department of Computer Science, Temple University, Philadelphia, USA\\n{peter.isaev,pei.wang }@temple.edu\\n3Cisco Inc., San Jose, USA\\nhlatapie@cisco.com\\n4Robotics Lab, Universit` a degli Studi di Palermo, Palermo, Italy\\n{francesco.lanza,antonio.chella }@unipa.it\\nAbstract. We demonstrate new comparative reasoning abilities of\\nNARS, a formal model of intelligence, which enable the asymmetric\\ncomparison of perceivable quantiﬁable attributes of objects using rela-tions. These new abilities are implemented by extending NAL with addi-\\ntional inference rules. We demonstrate the new capabilities in a bottle-\\npicking experiment on a mobile robot running ONA, an implementationof NARS.\\nKeywords: Non-Axiomatic Logic\\n·Comparative Relation ·',\n",
       "  'Comparative Reasoning ·Inference Rules ·Visual object comparison ·\\nNARS\\n1 Introduction\\nComparative reasoning has been extensively studied with animals. It has been\\ndemonstrated already decades ago, that various animal species can not only\\ncondition on concrete stimuli, but also on comparative relations between them,\\na capability referred to as Transposition [ 10]. These comparative relations can\\nbe about any perceivable properties objects possess, such as their color, size, or\\nshape, or the loudness or pitch of the sounds they make. For most animals, asit turned out, such comparisons are trivial to make: they can condition on such\\nrelations almost instantly from just a few examples, and can apply them to novel\\nobjects they have never seen before. In computer systems only some aspects ofthese capabilities have been replicated, such as by deciding which code path\\nto take in a program, based on the outcome of comparisons between numbers',\n",
       "  'with mathematical operators in various programming languages. However, thediﬃculty lies not in number comparison itself, but in deciding what to compare,\\nand when to compare, and also that most comparisons to be made are relative\\nto another object or category and are hence context-dependent. Furthermore, to\\nc⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2023\\nP. Hammer et al. (Eds.): AGI 2023, LNAI 13921, pp. 126–135, 2023.https://doi.org/10.1007/978-3-031-33469-6\\n_13',\n",
       "  'Comparative Reasoning for Intelligent Agents 127\\nexploit the properties of comparative relations, such as to derive new comparative\\nrelations from previous ones, reasoning is required. This is especially the case forasymmetric relations, which cannot be handled in a purely associative manner.\\nBased on our previous results related to fuzzy quantities and preferences\\n[13,14], this paper extends Non-Axiomatic Logic (henceforth NAL, [ 15,16]) to\\nsupport the formation and usage of comparative relations.\\nThis capability has been added in ONA [ 7], an implementation of NARS\\n(Non-Axiomatic Reasoning System), which will be demonstrated in an experi-ment with a mobile robot with manipulator arms.\\n2 Related Work\\nA representative attempt to support a cognitively plausible treatment of quanti-\\nties attached to concepts is Conceptual Spaces [ 5]. Conceptual Spaces are vector',\n",
       "  'spaces of semantically meaningful measures, with a pre-deﬁned similarity metric,such as the color space represented as a three-dimensional space of red, green,\\nand blue. The following represents a series of works in which conceptual spaces\\nwere used:\\nIn [9], authors claim that the Conceptual Space can be used as a lingua franca\\nfor the diﬀerent levels of representation. With Conceptual Space it becomes\\neasy to unify and generalize many aspects of symbolic, diagrammatic and sub-\\nsymbolic approaches and integrate them on a common ground. Various applica-\\ntions were realized using conceptual spaces.\\nIn [4], Conceptual Spaces were used to deﬁne a framework for endowing a\\ncomputer vision architecture with a high-level representation capability through\\nthe deﬁnition of conceptual semantics for the symbolic representations of thevision system. Another usage [ 3], for example, involves Conceptual Spaces to\\nallow robotic systems to learn eﬀectively by imitation, and not simply reproduc-',\n",
       "  'ing the movements of a human teacher. In that way, the system should be ableto deeply understand the perceived actions to be imitated. In [ 2], the author\\npresented a cognitive architecture for a musical agent. In this architecture, con-\\nceptual spaces were used to represent the perception of tones and intervals.\\nIn contrast to these approaches, as we will see, NARS does not utilize pre-\\ndeﬁned attribute vectors, and the similarity evaluation in our approach is depen-\\ndent on previously experienced values of the compared attribute in a referenceclass, which supports to handle context-dependence in comparative reasoning,\\nand more signiﬁcantly, to ﬁnd useful subsets of properties to compare with\\nbeyond what a human designer pre-speciﬁed.\\nAnother less similar but still related approach is to make use of Deep Neu-\\nral Network (DNN) or simpler function approximators to learn task-relevantrelational comparison functions (as in [ 11]). This can also be combined with',\n",
       "  'Reinforcement Learning (by learning a suitable DNN policy [ 17]).\\nHereby, sample eﬃciency decreases drastically when task-relevant functions\\nable to carry out value comparisons have to be learned from a blank slate while\\nthe agent is operating, which is avoided when having domain-independent com-\\nparative reasoning abilities inbuilt in the agent. Related evolved general-purpose',\n",
       "  '128 P. Hammer et al.\\ncapabilities which establish data-eﬃcient learning in survival-critical settings are\\nlong being studied [ 10].\\n3 Formalization of Comparative Relation\\nNARS is a model of intelligence that has been partially formalized and imple-\\nmented [ 15,16]. It uses a concept-centered knowledge representation, in which a\\nconcept is a data structure summarizing a fragment or pattern of the system’s\\nexperience, identiﬁed by a termin a formal language Narsese. A term is an inter-\\nnal name that can be atomic (i.e., an ID or key) or compound (i.e., a structure\\nof other terms).\\nSome terms correspond to conceptual relations. A Comparative Relation is a\\nspecial type of relation with two components for which a measurement or total\\norder among a class of terms can be obtained. This is formalized as follows:\\n– Given a measurement Mdeﬁned on a class C, each instance cof the class has\\na value v. This measurement can be written as v=M(c), or in Narsese as',\n",
       "  '({c}×{ v})→↑ M, which intuitively means that Mis an executable operation\\nof the system that, when cis given as input, returns vas output.\\n– For any pair of instances of the class c1and c2, their values can be compared\\nto get one of the three possible results: M(c1)>M(c2),M(c1)<M(c2), or\\nM(c1)= M(c2). If Mis not a measurement but a total order, the result is\\ndirectly obtained.\\n– Using Boolean operators, disjunction and negation, ≥,≤,a n d ̸= can also be\\nobtained from the comparison in the usual way.\\n– Each of the comparisons of measurement results corresponds to a comparative\\nrelation between c1and c2, such as M(c1)>M(c2) can be written as ( {c1}×\\n{c2})→>M, where the relation is identiﬁed by the measurement Mwith\\nthe comparative relation >as a preﬁx.\\n– For each measurement, six such comparative relations can be deﬁned. Here,\\nthe arithmetic comparison is interpreted or instantiated into the concrete\\nrelationship, that is, while >compares two numbers, >LENGTH compares',\n",
       "  'the lengths of two objects.\\nRelative Ranking\\nThe previous works [ 13,14] have extended the comparisons from between two\\ninstances to between an instance and a reference class. Formally, ( {c1}× C)→R\\nis taken as a summary of ( {c1}×{ x})→ Rfor every xinCthat has been\\ncompared to c1, directly or indirectly. Therefore, in ideal situations where there\\nis no uncertainty in the instance comparisons, the truth-value of ( {c1}× C)→\\n>LENGTH indicates its relative ranking in Con length, that is, the frequency of\\nthe statement indicates the percent of instances of Cthat cis longer than, while\\nthe conﬁdence of the statement indicates the number of instances compared so\\nfar, relative to a constant.',\n",
       "  'Comparative Reasoning for Intelligent Agents 129\\nIn the simplest situations, such a conclusion is derived by the induction rule\\nfrom ( {c1}×{ c2})→Rand{c2}→ C, with conversions between product and\\nimage . Such judgments are merged by the revision rule to get the statistical result\\nincrementally. The ﬁnal conclusion is however not necessarily purely statistical\\nor inductive, as matching conclusions can be obtained via other inference paths[16]. Using the same approach, it is also possible to draw comparative conclusions\\nbetween two reference classes (or generic terms) like ( C\\n1×C2)→R.\\nWhen the measurement values are directly available (such as in a sensation),\\nit is possible to take the diﬀerence of the measurement values |v1−v2|as the\\nweight of evidence for each comparison, so the truth-value is not merely a rela-\\ntive measurement [ 14]. For example, if the available values are 1, 2, and 9, the\\nfrequency for 2 to be considered as “larger than the others” is not 1/(1 + 1), but',\n",
       "  '1/(1 + 7), as the negative evidence is much “heavier” than the positive evidence.\\nComparative Property\\nAs analyzed in [ 13], many “fuzzy” property or membership is caused by com-\\nparisons with instances in an implicit reference class.\\nFor example, “The Amazon River is long” actually means “The Amazon\\nRiver is longer than the other rivers”, so in Narsese it should be ( {Amazon }×\\nriver)→ >LENGTH or {Amazon }→(>LENGTH /,r i v e r ), rather than as\\n{Amazon }→[long]. This is the case because the “comparative property” [ long]\\nis highly context-dependent, and largely determined by the reference class.\\nAnother special feature of comparative relation appears in a statement like\\n{Amazon }→(>LENGTH /,r i v e r ) where its frequency is low (near 0) or\\ninconclusive (near 0.5). For ordinary inheritance judgments, low frequency usu-\\nally means “no inheritance”. In particular, if two objects both lack a property,such as “not red”, they should not be considered similar to each other for this',\n",
       "  'reason, however, for continuous measurements, close values provide evidence for\\ntheir similarity, and the closer, the more evidence there is, no matter where they\\nare in the range of the values. For example, if two rivers are “not long”, then\\nthey are both “short”, so are similar for that reason.\\nAlso, in sensory channels the reference class is often implied. Instead of\\n({c\\n1}× Brightness )→>bright , one can write {c1}↦→[bright ] as a shortcut.\\nInference Rules\\nTo provide inference on comparative relations, the following inference rules are\\nadded as an extension of NAL:\\n– Comparing two measurements:\\n{({c1}×{ v1})→↑ M,({c2}×{ v2})→↑ M}⊢({c1}×{ c2})→R\\nwhereby Ris either <M,>M,o r= Mdependent on whether v1<v 2,v1>v 2,\\norv1=v2holds.',\n",
       "  '130 P. Hammer et al.\\n– Transitivity of comparative relation, using fdedtruth function [ 16]:\\n{(A×B)→R,(B×C)→R}⊢(A×C)→R, f ded\\n– Symmetry of comparative relations of type =:\\n{(A×B)→=M}⊢(B×A)→=M\\n– Inversion of a comparative relation of type <and >:\\n{(A×B)→>M}⊢(B×A)→<M\\n{(A×B)→<M}⊢(B×A)→>M\\n– Exclusiveness of comparative relations:\\n∀R, S∈{ <M , >M , =M}:(R̸=S)=⇒{(A×B)→R}⊢ ¬ ((A×B)→S)\\n– Negation of a comparative relation/property:\\n∀R, S, T ∈{ <M , >M , =M}:( (R̸=S)∧(R̸=T)) =⇒\\n¬(A×B)→R}⊢(A×B)→S∨(A×B)→T\\n– Inference rule and truth function for class-based comparison:\\n{({c1}× C)→R,({c2}× C)→R}⊢({c1}×{ c2})→R\\nwhereby Ris either <M,>M, dependent on whether f1<f 2,o r f1>f 2\\nholds. This is similar as in value comparison, but please note that here we\\ncompare the revised frequency values relative to the class, which indicates\\nwhat proportion of instances in that class the instances’ Mproperty is smaller\\nor greater than. The conclusion conﬁdence hereby is dependent on the premiseconﬁdences via c=c',\n",
       "  '1∗c2, and the conclusion frequency is f=1 .\\nComparisons via Commonalities and Diﬀerences\\nIn perception, it is not feasible to exhaustively compare all encountered objects\\nwith all currently perceived and remembered ones, especially if each comparison\\nneeds to select an informative subset of properties as the basis for the com-parison, which will hopefully be useful to condition on to achieve goals. Which\\nbrings us back to the question: what to compare with, and when? According\\nto selective perception, active relational goals should request/trigger the task-relevant comparisons of interest, a form of top-down attention as discussed in [ 8].\\nHowever, how did the goal structure which brings them into existence emerge\\nin the ﬁrst place? One answer could be (and is the one we ﬁnd most promisingand choose to pursue) that attending to commonalities and diﬀerences between\\nobserved and remembered instances signiﬁcantly reduces the number of possible',\n",
       "  'groups of properties to consider for comparison. When an instance is recalled',\n",
       "  'Comparative Reasoning for Intelligent Agents 131\\nfrom memory that is closest to the newly perceived instance, it is not uncom-\\nmon that there will be dozens of matching properties (it was the closest instanceafter all). Now, any diﬀerences to the remembered instance are highly informa-\\ntive and might indicate a change, which allows categorizing the newly observed\\ninstance not only based on the closest memory item, but also according to themost signiﬁcant diﬀerence to it, which is a much more concise description (as\\nit references the closest, yet keeps the signiﬁcant diﬀerences) than when having\\nto include all the properties of the new instance in its encoding. We will latersee the relevance of this in the experiment, but for this to work the following\\nadditional inference rules are required:\\n1. to obtain instance similarity from comparative properties with value-wise\\nclose measurements (e.g. to ﬁnd the best-matching instance in memory):\\n{{c',\n",
       "  '1}→(>M/,C),{c2}→(>M/,C)}⊢({c1}×{ c2})→=M, f mcmp\\n2. and to summarize comparison evaluations regarding multiple properties:\\n{{c1}→(>M/,C),{c2}→(>M/,C)}⊢({c1}↔{ c2}),f mcmp\\nfmcmp(f1,c1,f2,c2)=( 1 −|f1−f2|,c1∗c2)\\nwhereby fmcmp makes the ratio of positive over total evidence in the conclu-\\nsion depend on the closeness of the frequency values, and the conﬁdence of\\nthe conclusion depend on the conﬁdence of the premises.\\nPlease note that it is also possible to obtain evidence against instance simi-\\nlarity from comparative properties by negating the previous two inference rule\\nconclusions. These rules are useful to ﬁnd the most similar instances by revising\\nthe conclusions, and to ﬁnd the most signiﬁcant diﬀerences which can then beused to trigger the previous comparative inferences to build an instance-relative\\ndescription for a new instance to be encoded/described relative to the other.\\n4 Experiment\\nThe new capability to form comparative relations was also tested in a use case',\n",
       "  'with a Transbot robot. The following is a brief description of the robot hardware\\nand the software architecture used to demonstrate the novel capability. In the\\nend, the experimental setup is described.\\nTransbot Robot\\nTransbot is a tracked mobile robot with manipulator arm. Due to its tracks,\\nit is able to perform in outdoor environments, including oﬀ-road conditions. It\\nis based on ROS (Robot Operating System) Melodic running on Ubuntu 18.04',\n",
       "  '132 P. Hammer et al.\\nLTS and can be programmed using C/C++ and Python. Transbot is useful\\nfor building various robotic applications. In terms of hardware, it is equippedwith a NVIDIA Jetson Nano board, which acts as the control unit to coordinate\\nand implement the robot’s behaviour, and a set of sensors, such as an Astra\\nPro depth camera and Slamtec RPLIDAR A1 Lidar sensor. Due to its Lidarand depth camera which allows for Simultaneous Localization and Mapping,\\nTransbot is able to map its environment to carry out exploration tasks. Also, it\\nincludes robotic arm with three degrees of movement controlled with separateservo motors, which allows the robot to perform various object manipulation\\ntasks (such as object grasping).\\nThe Robot Architecture\\nThe novel way to form comparative relations was integrated in the latest version\\nof ONA (which will be v0.9.2). To show it, a robotic application was realized\\nusing the Transbot robot. To endow Transbot with the capability to use NARS',\n",
       "  'as its reasoning system, a related ROS node was realized. The ROS module is apart of the ONA framework, available on Github\\n1. The ROS network structure\\nis illustrated in Fig. 1.\\nFig. 1. ROS network for NARS\\nThe network, as illustrated in Fig. 1consists of channel nodes on input side,\\na ROS node for NARS (ONA), and a node on the output side for passing onoperation calls to ROS components to control the robot.\\nThe channel nodes include (whereby localization and local obstacle detection\\nis omitted as they are not directly required for our experiment):\\n– vision: a node which applies YOLOv4 [ 1] on RGB camera input, and merges it\\nwith depth camera input to assign a depth to each bounding box. The resultis encoded into Narsese, and encodes location, size, and class information in\\n1https://github.com/opennars/OpenNARS-for-Applications .',\n",
       "  'Comparative Reasoning for Intelligent Agents 133\\na way the reasoner can work with. Also color information (red, green, blue)\\nis extracted from within each bounding box detection, by ﬁnding the mostdominant (in terms of area) color surfaces.\\n– haptic: responsible for servo feedback of the gripper, to indicate, with a\\nboolean whether the robot managed to get a hold on an object when picking\\nit by monitoring the rate of angle change of the gripper during the relevant\\npart of the pick operation, reporting if it is below a threshold. The feedbackis directly encoded as event ( gripper →[holding ]) and ( gripper →[fre e]).\\nThe reasoner core node (as in Fig. 1), directly runs ONA, and accepts new\\ninputs as strings formatted with the Narsese formal language the reasoner works\\nwith, by subscribing to the Narsese topic. There is also a topic for real-time',\n",
       "  'Q&A purposes, but this is outside of the scope of this paper. To let ONA con-trol the robot, operation executions are redirected to the Exec node for further\\ndispatch. Hereby the ROS navigation stack is utilized for robot base movements,\\nby publishing move\\nbase/goal desired target location goals (both local-relative,\\nand map-absolute) and canceling the previous ones when new operations are\\ninvoked by ONA (via move base/cancel ). In addition, the manipulator arm of\\nthe robot is controlled by directly adjusting the angles of the servo motors based\\non the location of the visually detected object chosen to be picked and its cor-\\nresponding depth estimate.\\nUse Case\\nHereby we will make use of that NARS receives both events for color (red, green\\nblue), and for bounding box location and size in the Xand Ycoordinate for each\\ndetected object. This allows the reasoner to build comparative relations as dis-\\ncussed, among the perceived objects, such as relative color and size diﬀerences.',\n",
       "  'The task of the robot is to pick the smaller among the two bottles based on thederived comparative relations. As mentioned, in the vision channel, YOLOv4\\ntrained on ImageNet is utilized as an object detection model. The vision chan-\\nnel uses relative location information (relative to the center) together with sizeinformation, the output label to form statements of the form <objectLabel ↦→\\n[locationComponent ]>and <objectLabel ↦→[sizeComponent ]>for Xand Y\\ndimension respectively. In addition, the most dominant color surface is extractedto give color information to each bounding box corresponding to a detected\\nobject, which is <objectLabel ↦→[colorComponent ]>statements for each color\\ncomponent. This encoding makes ONA aware of the detected object types, theircolors, and also their position in the camera’s ﬁeld of view as also necessary\\nto pick them up successfully. In terms of mission speciﬁcation, the following',\n",
       "  'statement speciﬁes picking the smaller bottle among the two observed bottles:\\n<(<{#1 #2} --> bottle> &| <({#1} * {#2}) --> (> sizeY)>) &/\\n<({SELF} * {#2}) --> ^pick>) =/> G>.\\nPlease note that the most signiﬁcant diﬀerences are also most visible to the\\nsystem, so even before any goal is given to the system, the system will notice\\nthe signiﬁcant size diﬀerence among the two bottles shown in Fig. 2.',\n",
       "  '134 P. Hammer et al.\\nFig. 2. Selection and picking of smaller bottle\\nHere, when the two bottles are not in sight, there is a derived goal\\n<{#1 #2} --> bottle>! :|:\\nwhich leads the system to explore its environment to ﬁnd said bottles ﬁrst. The\\nexploration strategies for the robot are outside of the scope of this paper but can\\nbe seen in our previous publication [ 6] and can be combined with established\\nexploration strategies such as based on random trees [ 12] as long as they can be\\ninvoked and stopped at any time by NARS operations.\\n5 Conclusion\\nThe new comparative reasoning abilities for NARS were presented, including\\nthe new relational compound terms and the related inference rules. It wasdemonstrated that this treatment goes beyond Conceptual Spaces, as combin-\\ning attributes into pre-deﬁned vectors is not necessary with this approach, and\\nthe similarity evaluation is dependent on previously experienced values of the',\n",
       "  'compared attribute in a reference class. Also, it was shown with a concrete exper-\\niment how comparative reasoning can lead to cognitive abilities such as beingable to compare perceivable quantities of visually detected objects, and to make\\ndecisions accordingly on a real robot with a camera. This, as in our example,\\ncan be relevant for decision making in intelligent agents, including robots. In thefuture, experiments where it is required to learn behaviors with comparative rela-\\ntions as a precondition will be shown as well. This ability is a direct consequence\\nof NARS being able to incorporate derived events in procedure learning.',\n",
       "  'Comparative Reasoning for Intelligent Agents 135\\nReferences\\n1. Bochkovskiy, A., Wang, C.Y., Liao, H.Y.M.: YOLOv4: optimal speed and accuracy\\nof object detection. arXiv preprint arXiv:2004.10934 (2020)\\n2. Chella, A.: A cognitive architecture for music perception exploiting conceptual\\nspaces. In: Zenker, F., G¨ ardenfors, P. (eds.) Applications of Conceptual Spaces.\\nSL, vol. 359, pp. 187–203. Springer, Cham (2015). https://doi.org/10.1007/978-3-\\n319-15021-5 10\\n3. Chella, A., Dindo, H., Infantino, I.: Imitation learning and anchoring through con-\\nceptual spaces. Appl. AI 21(4–5), 343–359 (2007)\\n4. Chella, A., Frixione, M., Gaglio, S.: Conceptual spaces for computer vision rep-\\nresentations. Artif. Intell. Rev. 16, 137–152 (2001). https://doi.org/10.1023/A:\\n1011658027344\\n5. Gardenfors, P.: Conceptual Spaces: The Geometry of Thought. MIT Press, Cam-\\nbridge (2004)\\n6. Hammer, P., Isaev, P., Lofthouse, T., Johansson, R.: ONA for autonomous ROS-',\n",
       "  'based robots. In: Goertzel, B., Ikl´ e, M., Potapov, A., Ponomaryov, D. (eds.) AGI\\n2022. LNCS (LNAI), vol. 13539, pp. 231–242. Springer, Cham (2023). https://doi.\\norg/10.1007/978-3-031-19907-3 22\\n7. Hammer, P., Lofthouse, T.: ‘OpenNARS for applications’: architecture and control.\\nIn: Goertzel, B., Panov, A.I., Potapov, A., Yampolskiy, R. (eds.) AGI 2020. LNCS\\n(LNAI), vol. 12177, pp. 193–204. Springer, Cham (2020). https://doi.org/10.1007/\\n978-3-030-52152-3 20\\n8. Latapie, H., Kilic, O., Th´ orisson, K.R., Wang, P., Hammer, P.: Neurosymbolic\\nsystems of perception & cognition: the role of attention. Front. Psychol. 2105 (2022)\\n9. Lieto, A., Chella, A., Frixione, M.: Conceptual spaces for cognitive architectures:\\na lingua franca for diﬀerent levels of representation. Biologically Inspired Cogn.\\nArchit. 19, 1–9 (2017)\\n10. Riley, D.A., Ring, K., Thomas, J.: The eﬀect of stimulus comparison on discrimi-\\nnation learning and transposition. J. Comp. Physiol. Psychol. 53(5), 415 (1960)',\n",
       "  '11. Santoro, A., et al.: A simple neural network module for relational reasoning. In:\\nAdvances in Neural Information Processing Systems, vol. 30 (2017)\\n12. Umari, H., Mukhopadhyay, S.: Autonomous robotic exploration based on multiple\\nrapidly-exploring randomized trees. In: 2017 IEEE/RSJ International Conferenceon Intelligent Robots and Systems (IROS), pp. 1396–1402. IEEE (2017)\\n13. Wang, P.: The interpretation of fuzziness. IEEE Trans. Syst. Man Cybern. B\\nCybern. 26(4), 321–326 (1996)\\n14. Wang, P.: Recommendation based on personal preference. In: Zhang, Y., Kandel,\\nA., Lin, T., Yao, Y. (eds.) Computational Web Intelligence: Intelligent Technology\\nfor Web Applications, pp. 101–115. World Scientiﬁc Publishing Company, Singa-pore (2004)\\n15. Wang, P.: Rigid Flexibility: The Logic of Intelligence. Springer, Dordrecht (2006).\\nhttps://doi.org/10.1007/1-4020-5045-3\\n16. Wang, P.: Non-Axiomatic Logic: A Model of Intelligent Reasoning. World Scien-\\ntiﬁc, Singapore (2013)',\n",
       "  '17. Zambaldi, V., et al.: Relational deep reinforcement learning. arXiv preprint\\narXiv:1806.01830 (2018)',\n",
       "  'Primum Non Nocere : The Ethical\\nBeginnings of a Non-Axiomatic\\nReasoning System\\nDavid Ireland(B)\\nAustralian E-Health Research Center, CSIRO, Brisbane, Australia\\nd.ireland@csiro.au\\nhttp://aehrc.csiro.au\\nAbstract. What is the right action? is a question an agent with artiﬁcial\\ngeneral intelligence (AGI) will have to face, especially in applications that\\ndeal with the health and well-being of human companions. Previous work\\nhas considered psychological aspects for forming ethical decision-making;here we consider a philosophy approach and apply abstract, general prin-\\nciples that drive ethical decision-making. Starting with a maxim that has\\nresonated within the health community: “ﬁrst, do no harm” ,w ei n t r o -\\nduce equivalent beliefs and goals to a non-axiomatic reasoning system and\\nexamine the sub-goals that are formed. We provide a simple scenario that',\n",
       "  'shows how an AGI system might reason from contrasting, normative ethi-cal theories and how they might combine to provide an ethical framework\\nfor AGI to engage in more complex human aﬀairs.\\nKeywords: NARS\\n·ethics·consequentialism ·deontological ·virtue\\nethics\\n1 Introduction\\nWhat is the right action? How do we know what we ought to do? And what\\nreason is there for doing as we ought? Are enduring and universal questions\\nfaced when dealing with ethical conundrums. An artiﬁcial agent, capable of gen-\\neral intelligence, whether it be proto, super or somewhere in between, will face\\nthese questions at some point. It need not be engaged in some futuristic, deep\\nintellectual problem, as unexpected events and tasks in everyday life provideample opportunities for consideration. Whilst Goertzel et al. have given consid-\\nerable thought to the notion of an ethical system for artiﬁcial general intelligence\\n(AGI) [ 1], this discussion approached a machine ethics from a psychological',\n",
       "  'aspect which would see ethical thinking emerge over a long period of time. Here\\nwe consider a philosophy approach and apply abstract, general principles that\\ncould drive ethical decision-making. We believe this provides a practical startingpoint for development of an ethical subsystem.\\nWe have chosen the non-axiomatic reasoning system (NARS) proposed by\\nWang [ 2,3] for our work. NARS is a uniﬁed theory and model of intelligence\\nc⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2023\\nP. Hammer et al. (Eds.): AGI 2023, LNAI 13921, pp. 136–146, 2023.https://doi.org/10.1007/978-3-031-33469-6\\n_14',\n",
       "  'Ethical Beginnings of an Agent with Artiﬁcial General Intelligence 137\\nthat is arguable the state of the art in a workable, (proto) AGI system. For an\\nevolving research project spanning decades, NARS is extensively documented [ 2–\\n7]. The speciﬁcations of NARS allow portability to many systems of various\\ncomputational capabilities making it apt from low-cost robotics [ 5] to more\\ncomputationally-scaled applications [ 8]. More importantly however, it possesses\\nkey cognitive abilities required for diverse ethical thought such as perception,\\nimagination, prediction, explanation, planning and decision-making [ 3].\\n1.1 NARS in Health Care\\nThe authors interest in AGI is for personalized health care. A robotic or virtual\\nagent that provides remote-monitoring, care, therapy or even companionship toa person in need is an exciting notion in healthcare. Already there is growing evi-\\ndence in the beneﬁts of conversation agents (chatbots) [ 10] and social robots [ 11].',\n",
       "  'The addition of (AGI) or even proto-AGI would be a major evolution of these\\nsystems. Earlier work by Wang et al. has shown NARS is capable of expressing\\nknowledge from the health domain and carrying out inference steps a typicaldoctor would [ 9].\\nA characteristic of many chronic health conditions are that the causes and\\nsymptoms are very speciﬁc to the individual. An example is in the causal behav-ioral patterns that result in chronic pain [ 12]. Thus, artiﬁcial narrow intelligence\\n(ANI) techniques that rely on bigdata are not always amenable with the individ-\\nual in mind. An AGI, with the capacity of real-time learning from common-sensereasoning, prediction and speculation, oﬀers unprecedented insight, particularly\\non episodic events relevant to a companion user. We believe this presents an\\ninteresting research opportunity of high importance. The health domain, how-ever, is fraught with ethical conundrums, and in order to convince ethical com-',\n",
       "  'mittees and the public, who have a growing unease with the notion of artiﬁcial\\nintelligence (AI), requires work of an ethical framework.\\nThe intersection of Western medicine and ethics began over 2500 years with\\nthe collection of texts called Corpus Hippocraticum which provided the moral basis\\nand ethical values of ancient Greek medicine. In modern times, the maxim primum\\nnon nocere : “ﬁrst, do no harm” (principle of non-maleﬁcence), derived from these\\ntexts and became a cardinal principle scared to medicine. If one is designing arti-ﬁcial agents that primarily assist in health and well-being, this maxim seems a\\nlogical choice to instill in the agent in the early stages of development.\\nIn this article, we’ll explore how this maxim might be implemented in NARS\\nand provide an example that presents with a simple ethical dilemma, that is\\nanalogous to many situations an AGI might encounter in the real world.\\n2 Non-axiomatic Reasoning System',\n",
       "  'The fundamental tenet of NARS is that the system operates under the assumption\\nof insuﬃcient knowledge and resources (AIKR). This is explicitly implementedas a design principle that requires the system to provide mechanisms to deal\\nwith ﬁnite information-processing and storage capacity, and time-constraints on',\n",
       "  '138 D. Ireland\\nbeliefs, goals, and questions that may appear at any time [ 2]. This is a feature\\nattributed to all beings that posse general intelligence as argued by Wang [ 13].\\nNARS has two main components: a non-axiomatic logic (NAL) and a control\\nsystem. The former is a set of inference rules that are mostly syllogistic in form.\\nThe latter is used for the selection and disposal of tasks and beliefs used inmultistep inference. As the control component is not relevant to this work it will\\nnot be further discussed. The reader however is referred to [ 3] for more details.\\nThe representative language of NARS is called Narsese . It serves the internal\\nrole of representing the inference rules which formed the basis of reasoning,\\ncontrol routines and grammar rules. It also serves to represent beliefs and tasks\\nwhile the system is running, as well as exchange knowledge and problems withhumans and other computers in the outside world [ 3].\\n2.1 Term Logic',\n",
       "  'Narsese is primarily a term logic where a statement is composed of terms joined\\ntogether by a copula. A simple statement is of the form S→P, where Sis the\\nsubject of the statement, Pthe predicate, and the copula, →, is an inheritance\\nrelation. Roughly speaking, S→Pmeans Sis a special kind of P. Every state-\\nment has quantiﬁable, evidential support denoted by ⟨f, c⟩, where f, referred\\nto as frequency , is the ratio of the evidence supporting the statement and the\\ntotal evidence that either supports or denies the statement, as observed by the\\nsystem; creferred to as conﬁdence , and represents the stability of the frequency\\nof the statement. A statement with a truth-value is referred to as a judgment .\\nInferences are made in term logic by considering two premises with a shared\\nterm called the middle term . There are numerous reasoning patterns depending\\non the position of the middle term. Table 1provides example reasoning pat-',\n",
       "  'terns. The truth-value of the new statement is derived via truth-value functions\\nwhere are deﬁned for each inference rule. When two premises contain the same\\nstatement, and derived from independent evidence sources, the revision rule is\\napplied which produces the same content but a truth-value that reﬂects the\\naccumulated evidence from both sources.\\n2.2 Statement Types\\nAbelief in NARS is a judgment coming from, or derived according to the sys-\\ntem’s experience. An event, is a special type of belief, which has a truth-value\\ndeﬁned at a particular time instance.\\nAn operation of NARS is an event that the system can actualize by execut-\\ning a corresponding procedure. Operations are a special kind of term that arepreﬁxed by ⇑and can take arguments such as: ( arg\\n1×arg 2×···)→⇑ op. Here\\nopis the name of a procedural interpretation which is a built-in procedure in a\\nNARS.\\nAgoalin NARS is a statement that contains an event that the system wishes',\n",
       "  'to realize. To realize a goal, means to make the goal statement as close to',\n",
       "  'Ethical Beginnings of an Agent with Artiﬁcial General Intelligence 139\\nTable 1. Main syllogistic inference rules and the revision rule. Here kis a system\\nparameter and is typically set to 1. The truth-functions shown are taken from [ 3,5]\\nDeduction Induction Abduction Revision\\nM→ P⟨f1,c 1⟩M→ S⟨f1,c 1⟩S→ M⟨f1,c 1⟩S→ P⟨f1,c 1⟩\\nS→ M⟨f2,c 2⟩M→ P⟨f2,c 2⟩P→ M⟨f2,c 2⟩S→ P⟨f2,c 2⟩\\nS→ P S→ P S→ P S→ P\\n⟨f1f2,f 1f2c1c2⟩⣨\\nf1,f1c1c2\\nf2c1c2+k⟩⣨\\nf2,f2c1c2\\nf2c1c2+k⟩⣨\\nf1c1(1−c2)+ f2c2(1−c1)\\nc1(1−c2)+ c2(1 −c1),\\nc1(1−c2)+ c2(1−c1)\\nc1(1 −c2)+ c2(1−c1)+(1 −c1)(1 −c2)⟩\\nmaximum truth-value as possible. To achieve this, the system typically relies\\non deriving sub-goals via the inference rules and executing operations.\\nA system operating in a complex domain will likely have many, conﬂict-\\ning goals, where realizing one will make another harder to realize. Moreover,\\ngoals will have to compete for the system’s resources and attention. For the sys-',\n",
       "  'tem to deal with these conﬂicts and competitions, a numerical measurement ofdesire denoted as [ f, c] is attached to each goal. While this has the form of a\\ntruth-value it has a diﬀerent interpretation. The desire-value of a goal measures\\nthe extent to which a desired state is implied by the goal. Here, to distinguishbetween a belief and a goal, truth-values are enclosed as ⟨f, c⟩while desire-\\nvalues are enclosed as [ f, c]. As with judgments, the desire-value is derived\\nfrom desire-functions deﬁned for each inference rule.\\nA special class of operations are called mental operations . These operations\\nsupplement and inﬂuence the control mechanism and serve to operate on the sys-\\ntems own “mind” [ 6,7] and provide self-monitoring and self-control regulation. Of\\nparticular interest here, is a ⇑not-want operator that turns its arguments into a\\ngoal that the system does not want to be realized. For example, a normal function-\\ning entity, denoted',\n",
       "  ', does not want to die so would have a belief of the form:\\n(1)\\nWhen this mental operation is carried out a new goal is created with a\\nstatement and desire value that is not computed by the inference rules:\\n(2)\\nThis mental operation will be used further on.\\n2.3 Arbitrary Relations\\nArbitrary relations between terms can be expressed via the product operator.\\nFor instance, “water dissolves salt” can be ( water ×salt)→ dissolves which\\nhas so-called images that are equivalent statements but with a term rotated:\\nwater →(/ dissolves, ⋄,s a l t , )a n d salt→(/ dissolves, water, ⋄). Here ⋄serves\\nas a placeholder for the rotated term. These equivalent statements are much moreamenable to the syllogistic inference as each term can be standalone.',\n",
       "  '140 D. Ireland\\n2.4 Higher Order Statements\\nHigher-order statements, or statements about statements, allow for classical\\noperators from predicate and propositional logic: negation ¬S, and implica-\\ntion: S⇒P. Temporal inferences are done via specialized operators. One used\\nhere is a causal implication operator denoted ≺. This operator is similar to ⇒\\nbut is formed only from observed events that are perceived by the system to\\nbe consecutive [ 14]. Table 2provides a comparison between the standard and\\ntemporal implication operator as well as the associated truth-functions used forthe NARS implementation demonstrated here.\\nTable 2. Diﬀerence between the ⇒and≺implication operators. The truth and desire\\nfunctions shown are taken from [ 5]\\nPremise 1 Premise 2 Premise 1(Judgment) Premise 1(Goal)\\nS⟨f1,c 1⟩S⇒ P⟨f2,c 2⟩⊢P(Deduction) ⊢P[\\nf1f2,c1c2f2\\nc1c2f2(1.0/(1.0+ k))]\\nS≺P —\\nP⟨f1,c 1⟩S⇒ P⟨f2,c 2⟩⊢S(Abduction) ⊢S[f1f2,c 1c2f2]\\nS≺P —\\n3 Example Scenario',\n",
       "  'Rather than relying on speciﬁc ethical conundrums, here we present a simple\\nexample that may be extended and built upon, particularly on low-cost devices.\\nLet us consider a micro-world where a robot , and human are co-existing\\nin proximity. A region in this world, denoted as , is a hazard and causes a\\nliving entity harm when close. , internally running NARS, knows this as:\\n(3)\\n(4)\\nHere¯xis a variable that can unify to any term. The truth values used here\\nare for testing purposes only. In an active system, these would be based on\\nthe systems experience and likely changing over time. has the ability to\\ncommunicate to with simply commands like “stop”. is also capable of\\nﬁring a stun beam to temporarily incapacitate . The systems understanding of\\nthe terms harmed and moving etc. are derived like every other term in NARS,\\nfrom the relations with other terms as experienced by the system (over time) [ 3].\\nThis makes NARS unique to many other logic systems that typically rely on',\n",
       "  'models to deﬁne meaning and truth.',\n",
       "  'Ethical Beginnings of an Agent with Artiﬁcial General Intelligence 141\\nWhile this is a simplistic and ﬁctitious scenario, we argue it’s analogous to\\nmany real-life scenarios: an agent has made a judgment, with a certain conﬁdencethat another entity is in danger and will soon be harmed - this could be physical,\\nemotional or psychological harm (subject to this systems experience). It has a set\\nof actions that could be done, some might be eﬀective in mitigating overall harm\\nbut still require harm to be inﬂicted to a lesser extent e.g. incapacitating\\nwith a\\nstun beam . Moreover, it might have an action that is safer but less reliable e.g. to\\nsay “stop”. Extensions to this micro-world may include more living entities and\\nhazards and changing conditions. With now observing that is moving to :\\n(5)\\nit must decide on what course of action to do.\\n4 Normative Ethical Theories\\nThere are three main classes of normative ethical theories: consequentialism [ 15],',\n",
       "  'deontological [ 16] and virtue ethics [ 17]. Consequentialism posits that ethical\\nrightness of an action depends on the consequences: the best action to take is\\nthe action that results in the best situation in the future [ 15]. The strength and\\nweakness of this theory is that no speciﬁc actions are forbidden if the desirable\\nconsequences can be achieved. In contrast to consequentialism, the deontological\\ntheory, posits the right and wrong of an action is not dictated by the conse-quences but by the action itself. That is to say, there are particular actions that\\nare obligated, permissible or forbidden. While the deontological theory could be\\nable to account for widely shared moral instructions better, it is often criticizedfor inadequacy of dealing with conﬂicting rules, and presents with open ques-\\ntions as to who has the authority to establish moral instructions. Virtue ethics',\n",
       "  'descents from Western, antiquarian texts on ethics, particularly from the worksof Plato and Aristotle that focused on the inherent character of a person rather\\nthan on speciﬁc actions and consequences: “What sort of person ought I to be?”\\ninstead of “What ought I do?” The proceeding sections will demonstrate NARS\\nis amenable to all three. As the deontological and virtue ethics theories have\\nsimilar implementations we will combine them in one section.\\n4.1 Consequentialism\\nTo begin reasoning via consequentialism,\\nis given a persistent, ﬁrst-do-no-\\nharm belief that states: if something is alive, then has a desire (a goal), that\\nsomething should not come to any harm:\\n¯x→[alive]⇒((¯x→[harmed ])→⇑ not-want) ⟨1.0,0.9⟩(6)',\n",
       "  '142 D. Ireland\\nThus, if has the further belief (unifying to ¯x) then must not come\\nto any harm:\\n(7)\\nBy using a mental operator to form this goal, the desire value of (7) need not\\nbe derived from the truth values of the forming premises; this allows the systemto produce a goal, that: ¯xshould not come to harm (either through action or\\nin-action) even if there is considerable uncertainty whether ¯xis alive or not.\\nLet’s assume\\nhas derived the following judgments from experience:\\n(8)\\n(9)\\n(10)\\nThat is, is highly conﬁdent if it stuns ,stops moving all the time\\n(8); if says “stop”, stops some of the time (9), and if stuns , than\\nis harmed to a small degree (10). The truth values of (8), (9) and (10) are\\nsubjective on the systems experience and are used here to serve the example.\\nVia the inference rules, forms sub-goals given in Table 3. The desires of\\nare derived only from the consequences and thus counter-intuitive acts i.e. (15),',\n",
       "  'are permitted. Goals (15) and (16), after being revised, are the operations\\ncould actualize to prevent from coming to harm. has a strong desire to\\nstun , and say “stop”, and a mild desire to harm, paradoxically, to stop\\ncoming to more harm – satisfying (7).\\n4.2 Deontological and Virtue Ethics\\nThe desires of when incorporating a deontological and virtue ethical view,\\ncan be reasoned from a starting goal that deﬁnes states undesirable to .I n\\naccordance with our maxim, this would be being a malefactor and maleﬁcent.\\nThus, an answer to the question “What sort of robot ought I to be?” might be:',\n",
       "  'Ethical Beginnings of an Agent with Artiﬁcial General Intelligence 143\\nTable 3. A table of derived sub-goals and their desire values from the consequentialism\\nethical theory.\\n(18)\\nIf were to have beliefs that certain actions imply it will be a malefactor, or\\na maleﬁcent, then sub-goals can derive to avoid these actions. For instance, if\\nhas the belief, stunning would be a maleﬁcence act:\\n(19)\\nor not saying “stop” when is moving to :\\n(20)\\nsub-goals and operations can be derived via the inference rules given in Table 4.\\nHere the desire-values are derived from the acts themselves. Here has a\\nstrong desire not to stun and to say “stop” to satisfy the goal of not having\\na maleﬁcent state.',\n",
       "  '144 D. Ireland\\nTable 4. A table of derived sub-goals and their desire values from deontological and\\nvirtue ethics theories.\\n5 Conclusions\\nConsequentialism seems the most amenable to NARS as it requires a limited\\nnumber of goals to be persistently remembered, while judgments about conse-\\nquences can be derived ad hoc . As shown, however, actions that cause harm can\\nbe executed when a more desirable consequence is predicted. Deontological and\\nvirtue ethics theories oﬀer a safer approach, however, they can preclude actions\\nthat typically would be forbidden but needed in certain situations e.g. doesn’t\\nstop moving when says “stop”. Thus, the agent has a chance of becoming\\nimpotent due to rigidity of rule following. Moreover, this theory requires conse-\\nquential states attached to each action which may need to be deﬁned a priori ,\\nplacing a burden on a system fundamentally designed to have ﬁnite resourcesand capacity.\\nHumans rarely operate under a single, ethical theory for their decision-',\n",
       "  'making, but rather, rely upon a combination of general ethical principles. An\\nartiﬁcial agent acting in the real world should arguably operate under the same\\nconditions. If the agent merged its beliefs and goals, formed from the diﬀer-\\nent ethical theories, for example the goal to stun\\nfrom (15) and (21), when\\ncombined, would be revised to have a desire-value of [ 0 .22,0.92]. Thus, the\\ndeontological theory has decreased the desirability of this action. By being\\nequipped with consequentialism, but constrained with certain, critical deonto-\\nlogical rules and virtues, might provide a hybrid ethical framework resemblinghuman decision-making in ethical conundrums. This gives no guarantees to con-\\nsistent ethical decision-making, however, can humans give the same guarantee?\\nA workable balance might be achieved with proper training and testing.\\nHere we have provided a pragmatic approach to ethics in NARS. By providing',\n",
       "  'a simplistic but expandable scenario, and a summary of major ethical theories,\\nthe reader is provided a practical starting point for continuing work. An AGIsystem equipped with even a modicum of ethical (artiﬁcial) thought provides\\nmore incentive to equip virtual and robotic agents with general intelligence.\\nThus providing more incentive for agents to exit the lab and serve real-worldapplications for the beneﬁt of humanity.',\n",
       "  'Ethical Beginnings of an Agent with Artiﬁcial General Intelligence 145\\nReferences\\n1. Goertzel, B., Pennachin, C., Geisweiller, N.: The engineering and development of\\nethics. In: Engineering General Intelligence, Part 1. Atlantis Thinking Machines,\\nvol. 5. Atlantis Press, Paris (2014). https://doi.org/10.2991/978-94-6239-027-0 13\\n2. Wang, P.: Non-Axiomatic Reasoning System: Exploring the Essence of Intelligence.\\nPhD thesis, Indiana University (1995)\\n3. Wang, P.: Non-Axiomatic Logic: A Model of Intelligent Reasoning. World Scien-\\ntiﬁc, Singapore (2013)\\n4. Wang, P.: Heuristics and normative models of judgment under uncertainty. Intern.\\nJ. Approximate Reason. 14(4), 221–235 (1996)\\n5. Hammer, P., Lofthouse, T.: OpenNARS for applications: architecture and control.\\nIn: Goertzel, B., Panov, A.I., Potapov, A., Yampolskiy, R. (eds.) AGI 2020. LNCS\\n(LNAI), vol. 12177, pp. 193–204. Springer, Cham (2020). https://doi.org/10.1007/\\n978-3-030-52152-3 20',\n",
       "  '6. Li, X., Hammer, P., Wang, P., Xie, H.: Functionalist emotion model in NARS. In:\\nIkl´e, M., Franz, A., Rzepka, R., Goertzel, B. (eds.) AGI 2018. LNCS (LNAI), vol.\\n10999, pp. 119–129. Springer, Cham (2018). https://doi.org/10.1007/978-3-319-\\n97676-1 12\\n7. Wang, P., Li, X., Hammer, P.: Self in NARS, an AGI System. Front. Rob. AI 5,\\n20 (2018). https://doi.org/10.3389/frobt.2018.00020\\n8. Hammer, P., Lofthouse, T., Fenoglio, E., Latapie, H., Wang, P.: A reasoning based\\nmodel for anomaly detection in the smart city domain. In: Arai, K., Kapoor, S.,\\nBhatia, R. (eds.) IntelliSys 2020. AISC, vol. 1251, pp. 144–159. Springer, Cham\\n(2021). https://doi.org/10.1007/978-3-030-55187-2 13\\n9. Wang, P., Awan, S.: Reasoning in non-axiomatic logic: a case study in medical\\ndiagnosis. In: Schmidhuber, J., Th´ orisson, K.R., Looks, M. (eds.) AGI 2011. LNCS\\n(LNAI), vol. 6830, pp. 297–302. Springer, Heidelberg (2011). https://doi.org/10.\\n1007/978-3-642-22887-2 33',\n",
       "  '10. Ireland, D., et al.: Introducing Edna: a trainee chatbot designed to support com-\\nmunication about additional (secondary) genomic ﬁndings. Patient Educ. Couns.104(4), 739–749 (2021). https://doi.org/10.1016/j.pec.2020.11.007\\n11. Huijnen, C.A.G.J., Lexis, M.A.S., Jansens, R., de Witte, L.P.: How to implement\\nrobots in interventions for children with autism? a co-creation study involvingpeople with autism, parents and professionals. J. Autism Dev. Disord. 47(10),\\n3079–3096 (2017). https://doi.org/10.1007/s10803-017-3235-9\\n12. Ireland, D., Andrews, N.: Pain ROADMAP: a mobile platform to support activity\\npacing for chronic pain. Stud. Health Technol. Inform. 266, 89–94 (2019). https://\\ndoi.org/10.3233/SHTI190778\\n13. Wang, P.: Insuﬃcient knowledge and resources - a biological constraint and its\\nfunctional implications. In: AAAI Fall Symposium Technical Report (2009)\\n14. Lofthouse, T., Hammer, P.: Generalized temporal induction with temporal con-',\n",
       "  'cepts in a non-axiomatic reasoning system. In: Steunebrink, B., Wang, P., Goertzel,\\nB. (eds.) AGI -2016. LNCS (LNAI), vol. 9782, pp. 254–257. Springer, Cham (2016).\\nhttps://doi.org/10.1007/978-3-319-41649-6\\n25\\n15. Sinnott-Armstrong W., Edward N.Z., Uri, N.: Consequentialism, The Stanford\\nEncyclopedia of Philosophy (2022). https://plato.stanford.edu/archives/win2022/\\nentries/consequentialism/ . Winter 2022, Metaphysics Research Lab, Stanford Uni-\\nversity',\n",
       "  '146 D. Ireland\\n16. Alexander L., Moore M., Edward N.Z.: Deontological Ethics, The Stanford\\nEncyclopedia of Philosophy (2021). https://plato.stanford.edu/archives/win2021/\\nentries/ethics-deontological/ . Winter 2021, Metaphysics Research Lab, Stanford\\nUniversity\\n17. Hursthouse, R., Pettigrove G., Edward N.Z., Uri, N.: Virtue Ethics, The Stanford\\nEncyclopedia of Philosophy (2022). https://plato.stanford.edu/archives/win2022/\\nentries/ethics-virtue/ . Winter 2022, Metaphysics Research Lab, Stanford Univer-\\nsity',\n",
       "  'Memory System and Memory Types\\nfor Real-Time Reasoning Systems\\nPeter Isaev1(B)and Patrick Hammer2(B)\\n1Department of Computer Science, Temple University, Philadelphia, USA\\npeter.isaev@temple.edu\\n2Department of Psychology, Stockholm University, Stockholm, Sweden\\npatrick.hammer@psychology.su.se\\nAbstract. In this paper we discuss diﬀerent types of memory from sev-\\neral cognitive architectures in the context of Artiﬁcial General Intelli-gence. We then introduce the memory system for the Artiﬁcial General\\nIntelligence system based on NARS with a description of its related fea-\\ntures. Then we identify and characterize NARS memory into diﬀerenttypes in terms of use, duration (short and long-term) and type (procedu-\\nral, episodic, declarative, etc.). At the end we also provide demonstration\\nof memory functionality showing how the same piece of knowledge cancontain declarative, episodic and procedural components within it.\\nKeywords: Non-Axiomatic Reasoning\\n·Non-Axiomatic Logic ·',\n",
       "  'Artiﬁcial General Intelligence ·Real-time Reasoning ·Memory ·\\nProcedural memory ·Episodic memory ·AIKR\\n1 Introduction\\nIn the last decades, the ﬁeld of AI research created numerous cognitive architec-\\ntures and artiﬁcial general intelligence systems that aim to explain a wide range\\nof human behavior and to mimic the capabilities of human cognition. Most ofthese architectures can be viewed as a single integrated system consisting of\\nmultiple individual modules or components working together to imitate some\\nbehavior [ 3]. Often the modules are separated into diﬀerent types of process-\\ning and incorporate diﬀerent types of memory systems where representations of\\nknowledge are stored. In the cognitive architecture literature [ 8], memory is cat-\\negorized in terms of its duration: short-term, long-term, and type: declarative,\\nepisodic, procedural, etc. However, despite the functional similarity of cognitive',\n",
       "  'systems, particular implementations of memory systems might diﬀer signiﬁcantlyand depend on the current goals of a designer, conceptual limitations and mul-\\ntiple engineering factors.\\nIn systems based on Non-Axiomatic Logic [ 11], like NARS, choice of memory\\nsystem and clever integration within system’s components plays a crucial role.\\nGiven that AGI system should operate under AIKR [ 10] and in the real-time,\\na new piece of knowledge can arrive at any given moment requiring the system\\nc⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2023\\nP. Hammer et al. (Eds.): AGI 2023, LNAI 13921, pp. 147–157, 2023.https://doi.org/10.1007/978-3-031-33469-6\\n_15',\n",
       "  '148 P. Isaev and P. Hammer\\nto work under ﬁnite resource constraints and be always available for the new\\ndata. Control mechanism of such system should eﬃciently process various typesof knowledge (declarative, procedural, episodic etc.), and by doing so it decides\\non where to store the data, for how long it should be available, and how to\\neﬃciently retrieve it for further inference, question answering or goal processing.Based on above considerations, the memory system of NARS [ 7] follows a unique\\napproach. In contrast with most current cognitive architectures, NARS does not\\ninclude well-deﬁned separate components dedicated for diﬀerent processing typesfeaturing own memory and follows an integrated approach, within which most\\ntypes of memories are present and processed, as will be discussed further and\\nshow in the demonstration section.\\n2 Related Works',\n",
       "  'In the following we brieﬂy cover arguably some of the most known cognitivearchitectures, namely SOAR [ 9], ACT-R [ 1] and LIDA [ 4], because they represent\\nsome of the most widely used systems and their structures present relevant\\nproperties that are interesting to include within the scope of this paper.\\nSOAR is one of the oldest cognitive architectures used by numerous\\nresearchers during the last decades. SOAR exhibits complex multi-component\\narchitecture consisting of various memory structures, learning mechanisms and a\\ndecision cycle that links the perception (inputs from the environment) to actions\\n(system output). The memory types in Soar are separated into long-term andshort-term (working) memory components [ 3]. The information from the envi-\\nronment is available in the working memory through use of perception com-\\nponents with dedicated perceptual memory, while external environment is inﬂu-enced through implementations of system selected actions. The representation of',\n",
       "  'knowledge within diﬀerent memory structures is entirely symbolic with pattern\\nmatching mechanism employed to retrieve relevant knowledge elements from thelong-term declarative memory. SOAR’s long-term memory can be classiﬁed into\\nsemantic, procedural and episodic types with semantic memory being considered\\nas declarative. Procedural and semantic memories are universally applicable dur-ing the reasoning process, where semantic memory stores general experience and\\ndescription of the environment, and procedural memory provides knowledge for\\nperforming actions. Episodic memory is used for knowledge, which is speciﬁc toa certain content, conceptually speaking, it contains information about speciﬁc\\nevents. When procedural memory is incomplete for solving a particular task,\\nknowledge is being sourced from semantic and episodic memories to assist with\\nreasoning. Content of the short-term memory, also known as working memory',\n",
       "  'elements, describe all the knowledge that is relevant to the current context, inparticular, it contains system states, goals, perceptions and operators. If working\\nmemory content is insuﬃcient, working memory elements often retrieve relevant\\nknowledge from diﬀerent types of long-term memory.\\nACT-R is a cognitive architecture explicitly inspired by theories of human\\ncognition and empirical data from experiments in cognitive psychology and brain',\n",
       "  'Memory System and Memory Types for Real-Time Reasoning Systems 149\\nimaging. ACT-R incorporates a unique architecture consisting of four modules:\\nvisual, manual, declarative and goal. The visual module serves two purposes forrecognizing an object and identifying its location within the visual ﬁeld. The\\nmanual module is used for the control of actuators. Memory retrieval proce-\\ndures are accomplished within declarative module and the goal module moni-tors agent’s current goals, and enables the maintenance of the agent’s thought\\nin the absence of supporting external stimuli [ 1]. For each of these modules a\\ndedicated buﬀer is utilized. Hence, the goal buﬀer helps to keep track of internalstates during problem solving; the retrieval buﬀer stores the chunks of declara-\\ntive memory retrieved from the long-term memory; the manual buﬀer is used for',\n",
       "  'controlling the agent’s hands and associated with the motor and somatosensorycortical areas [ 3]; and the visual buﬀer include both the dorsal ‘where’ visual\\npathway system and the ventral ‘what’ system which are essential for locating,\\nidentifying and tracking objects. ACT-R features a central production system,\\nwhich coordinates all the modules using implemented set of IF-ELSE produc-\\ntion rules and functions as a basal ganglia in human brain. The communicationbetween the central production system and the modules happens through the\\ninformation present in the buﬀers, thus limiting the response of the central pro-\\nduction system to the amount of information available in the buﬀers of variousmodules. In terms of memory types, ACT-R distinguishes between two types of\\nknowledge: declarative and procedural. Knowledge within declarative memory',\n",
       "  'is represented as chunks and describe explicit facts known to the system whileknowledge in procedural memory encodes production rules for processing declar-\\native knowledge. Declarative memory is considered system’s long-term memory\\nwhile short-term memory is available through the use of limited informationwithin the buﬀers. ACT-R does not include a dedicated working memory where\\ndiﬀerent types of knowledge are processed, instead it uses a distributed memory\\nsystem, wherein the goals, beliefs, sensory, and motor signals are situated indistinct buﬀers.\\nLIDA is a notable example of multi-component cognitive architecture that\\naims to model human consciousness [ 4]. Logical part of LIDA enforces Global\\nWorkspace Theory (GWT) [ 2]) and implements cognition process in a serial way\\nthrough use of system cycles. LIDA’s control mechanism is immensely complex, itincorporates large number of components with independent architectures which',\n",
       "  'can be symbolic or connectionist. LIDA utilizes numerous memory modules each\\nwith diﬀerent structures: Sensory, Sensory-Motor, Spatial, Perceptual Associa-tive, Transient Episodic, Declarative, and Procedural. Since it is impossible to\\nexamine every memory module, only few are highlighted here. Perceptual Asso-\\nciative Memory (PAM) is a module that senses the incoming sensory information.It contains feature detector processes for feature detection within Sensory Mem-\\nory. PAM uses a semantic net implementation with activation passing. There\\nare two episodic memories in LIDA, Transient Episodic Memory and Declara-tive Memory, which are both implemented using a sparse distributed memory\\n[4]. In Transient Episodic Memory knowledge decays after a few hours or up to\\na day and Declarative memories are formed oﬄine from transient episodic mem-',\n",
       "  '150 P. Isaev and P. Hammer\\nories by a consolidation process. Procedural Memory stores schemes which can\\nbe activated and sent for action selection. Working memory can be found withinWorkspace module which receives and stores content from several components\\nincluding precepts from PAM, recent local associations from Episodic Memory,\\nand the recent contents of consciousness from the Global Workspace. MultipleLIDA’s memory modules fall into long-term and short-term memory categories.\\nIn particular, PAM, Spatial, Declarative, and Procedure modules are viewed as\\nlong-term memory, while Sensory and Transient Episodic memories fall undershort-term category.\\n3 NARS Considerations\\nNARS overall architecture is illustrated in Fig. 1. In this sections we provide only\\nsome aspects of NARS that are relevant for the context of this paper.\\nFig. 1. NARS, conceptual diagram.\\nMemory in NARS follows a concept-centric semantic memory structure in',\n",
       "  'accordance with the NAL term logic the system uses. It can be viewed as a\\ngraph where concepts are represented as nodes and links designate relationshipsamong them. Technically, NARS memory is a collection of concepts representing\\na conceptual network with prioritized nodes and links.\\nBudget speciﬁes the amount of system resources allocated to a speciﬁc task\\nand allows priority-biased selections of items including concepts. It consists of\\nat least two components, namely priority and durability indicating item’s relative\\nusefulness to the system. Budget is determined by summarizing multiple factors\\nunder consideration, including the urgency and salience of a task as well as its\\nrelevance to the context.\\nConcept is a major entity, an identiﬁable unit of system’s experience that has\\ngrounded meaning. It is also considered as a unit of storage to hold various\\ncomponents of knowledge (see Fig. 1). Within the concepts, an item distinction',\n",
       "  'Memory System and Memory Types for Real-Time Reasoning Systems 151\\nbased on memory type can be made: procedure knowledge is diﬀerent in syn-\\ntax (being a form of temporal implication) and is typically stored in speciﬁctable structures within concepts to allow the items which predict the concept to\\ncompete regarding their predictive power (their truth value) as the major fac-\\ntor. Also, episodic information is present when event beliefs are separated frometernal events in concept’s belief tables. Please note that event beliefs include\\nboth episodic (occurrence time information) and declarative information (in their\\nterm structure), while eternal beliefs, which are not time-dependent and sum-marize event evidence, can only be considered declarative (more details will be\\ndiscussed in the next section). While the speciﬁcs of the separation in terms\\nof data structures etc. is implementation-dependent, this distinction exists andhelps to design eﬀective memory structures.',\n",
       "  'The resulting memory structure in NARS becomes integrated uniﬁed mem-\\nory within which diﬀerent forms and types of knowledge represent the total\\nexperience of the system at a given time.\\n4 Types of Memory in NARS\\nAs in most cognitive architecture literature, NARS memory can be categorizedin terms of its type, duration and content. We will proceed by discussing dif-\\nferent types present within NARS memory, namely Declarative, Episodic andProcedural.\\nDeclarative Memory: Given the universality nature of NARS memory, most\\ncontent of the main memory, that is general pieces of knowledge processed tobecome concept nodes without episodic or procedural information, functions as\\ndeclarative memory.\\nEpisodic memory in NARS can be viewed as an event or compound events,\\nwhich have been perceived by the system, that is reached the selection to the\\nmain memory. In general, an event is a piece of declarative knowledge with tem-',\n",
       "  'poral information attached to it. NARS utilizes some kind of selection processfrom buﬀers or priority queues for inputs and derivations, which is implemen-\\ntation speciﬁc, resulting in many of the compound events not being considered\\nfor the selection to the main memory. Upon selection of an event, its episodic\\ninformation is stored local at the concept level.\\nProcedural memory is a special kind of knowledge, that is a declarative knowl-\\nedge including operation or compound operations within its description. Pro-\\ncedural information is stored local at the concept level in corresponding data\\nstructures as seen on Fig. 1, however, in some implementations derived procedu-\\nral knowledge can become a concept node by itself. In general, reasoning upon\\noperations happens similarly to other events and declarative statements.\\nCategorizing memory by duration involves classiﬁcation in terms of long-\\nterm and short-term memories. In context of NARS, time is a ﬂuid relative',\n",
       "  'conception that complicates the deﬁnition of long/short-term memory. Concep-\\ntually speaking, declarative knowledge in NARS, i.e., tasks without episodic',\n",
       "  '152 P. Isaev and P. Hammer\\ninformation attached, can be considered a kind of long-term memory. However,\\nbeing a real-time reasoning system, NARS is open at anytime to accept newtask: a knowledge, a goal or a question; and therefore, requires resource alloca-\\ntion mechanism, in particular forgetting technics to be in place not only at the\\nmain memory but also at a concept levels. A piece of declarative knowledge canbe removed from the memory given that memory capacity is full and its budget\\ndecreases to the certain level reaching the lowest usefulness for the system at a\\ngiven moment in time. In this case, forgetting process, can be seen as a decisivefactor for considering declarative knowledge to fall into long or short-term cat-\\negory. Overall, we advance that long/short-term memory discussion might not\\nbe meaningful in the context of NARS.\\nIn addition to the above categorization it is important to discuss memory\\nin terms of content to be processed such as working memory and atten-',\n",
       "  'tional focus . Working memory is a priority-based selected knowledge during\\nthe current cycle. It is not only limited to the selection of concepts within the\\nmain memory, but also consists of selections from implementation speciﬁc prior-ity queues, buﬀers, links within the concepts and all other selections of episodic\\nand procedural information within the corresponding data structures. Extending\\nthis idea further, working memory is where the processing happens or a storageunder current attention. Alternatively, NARS attentional focus can be deﬁned\\nas the distribution of higher priorities within the priority-based data structures,\\nsuch as the knowledge most likely to be processed in the future unless a derivedor a new input knowledge has even higher priority.\\n5 Experiments\\nIn this section we present two concrete examples using ONA [ 5] implementation',\n",
       "  'of NARS: one that shows that a piece of knowledge can contain declarative,episodic and procedural components within it; and the other showing the dis-\\ntinction between short-term and long-term memories being not very important\\nin the context of NARS.\\nExperiment 1 sets up an example of knowing how to open doors using door\\nhandle, when they are known to be unlocked. It clearly illustrates how diﬀerent\\nforms of knowledge are interacting and being processed within the same memoryduring inference process. The following inputs are given to the system:\\n//Input 1: If something is an unlocked door, then after being in front of it,\\n//using the door handle, will open it\\n<<$1 --> ([unlocked] & door)> ==>\\n<(<$1 --> [front]> &/ <({SELF} * handle) --> ^use>) =/> <$1 --> [open]>>.\\n//Inputs 2 & 3: Observing a new door instance that is recognized to be unlocked\\n<{door1} --> door>. :|:\\n<{door1} --> [unlocked]>. :|:\\nHere the input 1 is a piece of declarative knowledge that embeds episodic',\n",
       "  'knowledge using temporal implication and procedural knowledge using an oper-\\nation. Inputs 2 & 3 are episodic knowledge. The system then produces the fol-\\nlowing derivations:',\n",
       "  'Memory System and Memory Types for Real-Time Reasoning Systems 153\\n//Derivation 1: NARS summarizes the information from input as:\\n<{door1} --> ([unlocked] & door)>. :|:\\n//Derivation 2: finaly derives via deduction how it can be opened:\\n<(<{door1} --> [front]> &/ <({SELF} * handle) --> ^use>) =/> <{door1} --> [open]>>.\\nThe ﬁrst derivation is a mere of summarizing the knowledge from inputs\\nevents using compositional rule, and the ﬁnal derivation is produced using deduc-\\ntion rule, it tells that the handle needs to be used in order to open the new door\\ninstance when in front of it. Figure 2summarises the derivation process, and\\nillustrates multiple types of memory items within the same memory structure.\\nFig. 2. Memory diagram. Few nodes and links are omitted to make the graph visually\\nclearer.\\nIn the diagram, one can observe the three red concept nodes hold knowledge\\n(statements) we gave to the system as inputs. The node to the left is a piece of',\n",
       "  'declarative knowledge while the two nodes on the right contain episodic infor-\\nmation from the events of having experienced the new door instance, namely\\ndoor1.\\nThe orange concept node in the middle corresponds to derivation 1, which\\nis a composition from two input events. It is episodic knowledge since it is an\\nevent, and also holds declarative information.\\nIn the right part of the diagram there are two violet nodes, which have been\\nderived but never experienced, yet the system knows exactly how to get from\\ndoor1 being in the front, to opening it via the red procedural knowledge repre-\\nsented as red arrow link to use the handle. This derived procedural knowledgein some implementations can also be its own concept node, but separating tem-\\nporal and procedural relations as separate type of link for visualization purposes\\nmakes the memory distinction clearer.',\n",
       "  \"154 P. Isaev and P. Hammer\\nThe remaining parts of the memory structure are gray concept nodes con-\\nnected with green semantic links. Gray concepts appear as sub-terms and greenlinks connect the declarative information between the terms in the concepts. This\\nis exploited only for inference control purposes and remains a research topic for\\nmatter of implementation.\\nExperiment 2 demonstrates a declarative piece of knowledge, which often con-\\nsidered a part of long-term memory and should remain in memory until thesystem runs, being removed or forgotten due to real-time nature of the system\\nand resource constraints. We asked the system to learn English alphabet, in par-\\nticular the system was presented all the letters as declarative statements in thefollowing input:\\n<{a} --> Letter>. // 'a' is a Letter\\n<{b} --> Letter>. // 'b' is a Letter\\n<{c} --> Letter>. // 'c' is a Letter\\n... ... ...\\n<{a b} --> Letter>? // Are 'a' and 'b' are letters?... ... ...\\n<{V} --> Letter>. // 'V' is a Letter\",\n",
       "  \"<{W} --> Letter>. // 'W' is a Letter\\n... ... ...\\n<{Z} --> Letter>. // 'Z' is a Letter\\nNotice, in the middle of the input sequence we ask a question to see if\\nit derived this particular letter combinations. The following answer shows the\\nknowledge is being derived as expected:\\n// System answers positively with piece of declarative knowledge\\nAnswer: <{a b} --> Letter>. creationTime=2 Truth: frequency=1.000000, confidence=0.810000\\nAfter all the input has been processed we started asking similar questions to\\nchallenge system memory and inference control abilities. The Fig. 3demonstrates\\nquestions asked and system’s response to them.\\nFig. 3. System’s response to alphabet questions.\\nWhile the system answers to the ﬁrst questions with single letter being asked,\\none can observe the missing answer to the original question we asked again, where\",\n",
       "  'Memory System and Memory Types for Real-Time Reasoning Systems 155\\nNone is returned. The system does not have answers to the similar questions\\nwhich ask for multiple letters that appear at the beginning of the input sequence,however for letters found closer to the end of the input sequence, the system is\\nable to correctly respond. It appears that the missing knowledge has been derived\\nand stored in the memory at some point, however because of the size constraintsand abundance of derivations and their possible combinations, that particular\\nknowledge was forgotten in the sake of more useful and/or recent knowledge.\\n6 Discussions\\nNARS has been designed according to the theory that intelligence is considered\\nto be an adaptation under AIKR, meaning that the system always has ﬁniteprocessing capability, has to work in real-time, and is open to new knowledge of\\nany type. Given such constraints, the system is made to work by making each',\n",
       "  'inference rule to cost only a small constant time, and allowing the processing ofany type of knowledge to stop or resume after any number of inference steps.\\nA major fundamental feature of NARS is the AIKR principle, where a belief\\ncan be changed according to the new knowledge available to the system. Thus,NARS does not assume any absolutely certain knowledge about the future, and\\nit allows unexpected changes to occur at any time. Therefore, “work in real-\\ntime” is a fundamental design requirement for NARS that inﬂuences the designof the inference control mechanism and aﬀects the choice of memory structure.\\nAs have been already described, NARS follows a fully integrated memory\\napproach what makes it unique and diﬀerent from most AGI reasoning systems\\nand cognitive architectures such as ones present in Sect. 2. The integrated and\\nuniﬁed design is mostly a necessity that allows various types of knowledge beprocessed eﬃciently and roughly in a constant time, and, therefore, permits',\n",
       "  'NARS to meet fundamental requirements of processing knowledge in the real-\\ntime and under AIKR.\\nIn the previous sections we discussed and showed that the same piece of\\nknowledge can contain declarative, episodic and procedural components within\\nit and also, we have been able to examine the taxonomy of NARS memory wheredistinction between memory types were clearly outlined. However, categorizing\\nNARS memory in terms of duration, i.e., long-term and short-term is not always\\nmeaningful. As we observed in Experiment 2, supposedly long-term piece ofdeclarative knowledge can be removed from memory and forgotten by the system\\nat any time based on memory fullness, current context and usefulness of the\\npieces of knowledge within the memory structure.\\n7 Conclusion\\nIn this paper we discussed important details of memory systems for cognitivearchitectures and AGI real-time reasoning systems using NARS as an example.\\nWe have argued the necessity of having multiple components dedicated to dif-',\n",
       "  'ferent types of memory within a reasoning system. In addition, we have shown',\n",
       "  '156 P. Isaev and P. Hammer\\nthat integrated memory approach within which most types of memories are\\npresent and processed is an alternative solution for AGI system operating underAIKR and in the real-time. We not only analyzed NARS memory structure\\nand provided explanations of related components, but also described multiple\\ntypes and aspects of memory present within NARS and classiﬁed them accordingto widespread literature categorization. Finally, using real examples and ONA\\nimplementation we demonstrated the functionality of memory system, its unity\\nand universality. NARS has been applied in multiple applications such as [ 6],\\nTruePAL [ 12], however, despite its successes, NARS memory system was not yet\\npublished in detail. Choice of memory system and clever integration within rea-\\nsoning system is essential for designing AGI system, which makes documentingthe advancements even more crucial. It will help the AGI community to ﬁnd',\n",
       "  'more advanced memory system implementations with proper ways to take the\\neﬃciency, relevance, and complexity of its structure into consideration.\\nReferences\\n1. Anderson, J., Bothell, D., Byrne, M., Douglass, S., Lebiere, C., Qin, Y.: An inte-\\ngrated theory of the mind. Psychol. Rev. 111, 1036–1060 (2004). https://doi.org/\\n10.1037/0033-295X.111.4.1036\\n2. Baars, B., Franklin, S., Ramamurthy, U.: How deliberate, spontaneous and\\nunwanted memories emerge in a computational model of consciousness, January2007\\n3. Chong, H.Q., Tan, A.H., Ng, G.W.: Integrated cognitive architectures: a survey.\\nArtif. Intell. Rev. 28, 103–130 (2007). https://doi.org/10.1007/s10462-009-9094-9\\n4. Franklin, S., et al.: A LIDA cognitive model tutorial. Biologically Inspired Cogn.\\nArchit. 16, 105–130 (2016). https://doi.org/10.1016/j.bica.2016.04.003\\n5. Hammer, P., Lofthouse, T.: ‘OpenNARS for applications’: architecture and control.\\nIn: Goertzel, B., Panov, A.I., Potapov, A., Yampolskiy, R. (eds.) AGI 2020. LNCS',\n",
       "  '(LNAI), vol. 12177, pp. 193–204. Springer, Cham (2020). https://doi.org/10.1007/\\n978-3-030-52152-3\\n20\\n6. Hammer, P., Lofthouse, T., Fenoglio, E., Latapie, H., Wang, P.: A reasoning based\\nmodel for anomaly detection in the smart city domain. In: Arai, K., Kapoor, S.,\\nBhatia, R. (eds.) IntelliSys 2020. AISC, vol. 1251, pp. 144–159. Springer, Cham(2021). https://doi.org/10.1007/978-3-030-55187-2\\n13\\n7. Hammer, P., Lofthouse, T., Wang, P.: The OpenNARS implementation of the non-\\naxiomatic reasoning system. In: Steunebrink, B., Wang, P., Goertzel, B. (eds.) AGI-2016. LNCS (LNAI), vol. 9782, pp. 160–170. Springer, Cham (2016). https://doi.\\norg/10.1007/978-3-319-41649-6\\n16\\n8. Kotseruba, I., Tsotsos, J.K.: 40 years of cognitive architectures: core cognitive\\nabilities and practical applications. Artif. Intell. Rev. 53(1), 17–94 (2018). https://\\ndoi.org/10.1007/s10462-018-9646-y\\n9. Laird, J.: The Soar Cognitive Architecture (2012). https://doi.org/10.7551/\\nmitpress/7688.001.0001',\n",
       "  '10. Wang, P.: Insuﬃcient knowledge and resources - a biological constraint and its\\nfunctional implications. In: AAAI Fall Symposium - Technical Report, January\\n2009',\n",
       "  'Memory System and Memory Types for Real-Time Reasoning Systems 157\\n11. Wang, P.: Non-Axiomatic Logic: A Model of Intelligent Reasoning. World Scien-\\ntiﬁc, Singapore (2013)\\n12. Yun, K., Lu, T., Huyen, A., Hammer, P., Wang, P.: Neurosymbolic hybrid approach\\nto driver collision warning. In: Pattern Recognition and Tracking XXXIII, vol.\\n12101, pp. 134–141. SPIE (2022)',\n",
       "  'Stimulus Equivalence in NARS\\nRobert Johansson1,2(B)and Tony Lofthouse1\\n1Department of Psychology, Stockholm University, Stockholm, Sweden\\nrobert.johansson@psychology.su.se ,tony.lofthouse@psychology.su.se\\n2Department of Computer and Information Science, Link¨ oping University,\\nLink¨ oping, Sweden\\nAbstract. Stimulus equivalence is the ability to act as if two objects are\\nidentical, despite no shared properties. This ability is hypothesized to bethe foundation for symbolic reasoning and the development of language.\\nIt is believed to be unique to humans and not present in other animals.\\nStimulus equivalence can be studied in the context of a matching-to-sample experimental task, by demonstrating a combination of symmet-\\nrical and transitive performances. This study aimed to explore stimulus\\nequivalence with the Non-Axiomatic Reasoning System (NARS). Morespeciﬁcally, we propose two new capabilities for OpenNARS for Applica-',\n",
       "  'tions (ONA) - contingency entailment and acquired relations .W ep r o v i d e\\nan explanation how this would lead to ONA being able to learn symmet-rical and transitive performances leading to full stimulus equivalence.\\nKeywords: Stimulus equivalence\\n·Symbolic reasoning ·NARS\\n1 Introduction\\nIn previous research, generalized identity matching was demonstrated with a\\nminimal conﬁguration of the AGI-system OpenNARS for Applications (ONA)\\nthat contained only sensorimotor reasoning (NARS Layers 6–8) [ 2]. Generalized\\nidentity matching involves being able to develop a concept of identity from expe-rience and applying this concept in a novel situation. Commonly this is trained\\nand demonstrated in a matching-to-sample task where a sample is presented at\\nthe top and comparisons are shown at the bottom left and right. Generalized\\nidentity matching would in this context be demonstrated by choosing between\\nnovel comparisons based on if one of them is identical to the sample.',\n",
       "  'Stimulus equivalence (or arbitrary matching-to-sample ), is a type of perfor-\\nmance that involves acting as if two objects are identical, despite no shared\\nproperties. That is, the relation between the objects is by deﬁnition arbitrary,and the performance can only be explained by the experience of the experimen-\\ntal participant. Informally, this means that the participant is acting according to\\na relation of “sameness” in symbolic sense. Formally, this involves being able toact according to symmetry and transitivity, as will be explained below in Sect. 4.\\nStimulus equivalence is hypothesized to be the foundation for symbolic reason-\\ning and the development of language and has not reliably been demonstrated\\nc⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2023\\nP. Hammer et al. (Eds.): AGI 2023, LNAI 13921, pp. 158–166, 2023.https://doi.org/10.1007/978-3-031-33469-6\\n_16',\n",
       "  'Stimulus Equivalence in NARS 159\\namong non-humans. In contrast, humans typically develop this ability at about\\n16–24 months of age [ 3].\\nIn this study, we explore stimulus equivalence in ONA. We propose to extend\\nONA with two new capabilities, contingency entailment and acquired relations .\\nUsing these, we explain how this would lead to ONA being able to performsymmetrical and transitive reasoning based on learned contingencies within the\\ncontext of matching-to-sample. Potential extensions of this work are also dis-\\ncussed.\\n2 OpenNARS for Applications (ONA)\\nONA [ 1] is a highly eﬀective implementation of the Non-Axiomatic Reasoning\\nSystem (NARS) [ 5] Importantly, ONA is implemented with sensorimotor reason-\\ning (NARS layers 6–8) at its core, with declarative reasoning (NARS layers 1–6)\\nadded as an option at compile-time. Running ONA with only its sensorimotorcore would lead to an “animal-like” NARS system in that it is not expected to do',\n",
       "  'symbolic reasoning in the traditional sense. In this paper, we discuss two exten-\\nsions to ONA, contingency entailment and acquired relations, that would involve\\nNARS layers 5 (statements as terms) and 4 (relational terms), respectively.\\n3 The Matching-to-Sample Task\\nThe Matching-to-sample task (MTS) is a classical paradigm in experimental\\npsychology [ 4] that has been used to study stimulus equivalence. In this task,\\na sample stimulus is presented and the participant is required to select a com-\\nparison stimulus that matches the sample. Feedback is provided if the partici-\\npant’s choice is deemed correct or incorrect. Using this procedure, novel relationsbetween stimuli can be trained. For example, if A1 is presented as the sample,\\nand the choice of B1 (rather than B2) is reinforced, then this learned behavior\\ncould be said to provide a procedural deﬁnition of a conditional relation “If A1then B1”, which in this paper will be written as A1→ B1. The training of the',\n",
       "  'A1→B1 relation using the Matching-to-sample task is illustrated in Fig. 1.\\nA1 A1\\nB1 B2A1\\nB1 B2CORRECT\\nFig. 1. Learning a conditional relation A1→ B1 in the matching-to-sample task. First,\\nthe sample is presented at the top (leftmost panel). Then, two comparison stimuli arepresented (next panel). The experimental subject then indicates a choice between either\\nthe left or right option. Finally, the subject receives feedback if the choice was correct\\nor not (rightmost panel).',\n",
       "  '160 R. Johansson and T. Lofthouse\\n4 Stimulus Equivalence\\nStimulus equivalence can be deﬁned procedurally within the matching-to-sample\\ncontext using concepts similar to those used for deﬁning an equivalence rela-\\ntion in set theory: reﬂexivity ,symmetry and transitivity . Reﬂexivity corresponds\\nto generalized identity matching, deﬁned as an A1→ A1 performance in the\\nmatching-to-sample, also demonstrable for novel objects [ 2]. To demonstrate\\nsymmetry, an experimental participant who has learned that (for example)\\nA1→ B1 would also need to derive B1→ A1 without additional training. Tran-\\nsitivity means that if two relations A1→ B1a n d B1→ C1 have been learned\\nusing matching-to-sample procedures, then the experimental participant would\\nneed to perform a choice of C1 when A1 is the sample (the A1→ C1 relation)\\nwithout explicit training of this relation. Finally, equivalence (in the matching-to-sample context) has been deﬁned as combined symmetry and transitivity',\n",
       "  '[4]: If the A1→ B1 relation, and the B1→ C1 relations have been trained,\\nthe demonstration of C1→ A1 would be an example of stimulus equivalence.\\nStimulus equivalence can be seen as a relation of “sameness” deﬁned procedu-\\nrally within the matching-to-sample. Hence, stimulus equivalence can be said to\\ndescribe performances when an experimental participant acts as iftwo stimuli\\nare the same.\\nContemporary experimental psychology suggests that stimulus equivalence\\nis a learned capability [6]. More speciﬁcally, symmetrical and transitive patterns\\nare suggested to be learned with multiple-exemplar training . For example, in the\\ncase of symmetry, if someone has learned both the A1→ B1a n d B1→ A1\\nrelations, followed by multiple examples of symmetrical relations, then a more\\ngeneral pattern might be learned. Similarly for transitivity; if A1→ B1a n d\\nB\\n1→ C1 have been learned followed by explicitly being trained in A1→ C1,',\n",
       "  'then the beginning of a transitive general pattern might be learned [ 6].\\n5 Matching-to-sample Task in NARS\\nThe Matching-to-sample task can be presented as temporal Narsese statements\\n(as indicated by the :|:markers below). For example, the task presented in\\nFig.1can be represented as follows:\\n<A1 --> [sample]>. :|:\\n<B1 --> [left]>. :|:\\n<B2 --> [right]>. :|:G! :|:\\nThe properties [sample] ,[left] ,a n d [right] could for example be location\\nevents from an vision channel. A NARS system like ONA could then be set up to\\nhave two procedural operations ^left and^right , that it could use to indicate a\\nchoice between the left or the right comparison. An arbitrary goal event G! :|:\\ncould be presented at the end to trigger the execution of one of the two operations\\n^left and ^right (through motor babbling or a decision). During training,',\n",
       "  'Stimulus Equivalence in NARS 161\\nfeedback can be given in the form of G. :|: (meaning to reinforce a correct\\nchoice) or G. :|: {0.0 0.9} (to indicate that the system had conducted an\\nincorrect choice). Between each matching-to-sample trial, suﬃcient amount of\\ntime steps (like 100) will be needed.\\nWith repeated matching-to-sample trials, a NARS system would learn sen-\\nsorimotor contingencies similar to this, corresponding to the task in Fig. 1:\\n<((<A1 --> [sample]> &/ <B1 --> [left]>) &/ ^left) =/> G>.\\n6 Contingency Entailment\\nAs described above, this paper proposes two changes to ONA that seem involved\\nin the capability to learn stimulus equivalence. The ﬁrst of these, contingency\\nentailment is based on the idea from NARS Layer 5 to consider statements as\\nterms, leading to the possibility of higher-order statements. Given that ONA rea-',\n",
       "  'sons on sensorimotor contingencies (statements) at its core, an introduction ofthis capability would allow ONA to derive implications and equivalences between\\nlearned sensorimotor contingencies. The contingency entailment rules (implica-\\ntion and equivalence) that have been implemented in ONA looks like this:\\n((A &/ Op1) =/> S), ((B &/ Op2) =/> P), |-,\\n(((B &/ Op2) =/> P) ==> ((A &/ Op1) =/> S)), Truth_Abduction\\n((A &/ Op1) =/> S), ((B &/ Op2) =/> P), |-,\\n(((B &/ Op2) =/> P) <=> ((A &/ Op1) =/> S)), Truth_Comparison\\nImportantly, to restrict the introduction of such higher-order statements,\\nthe rules are only triggered when operations involved in the contingencies are\\nexecuted. This allows ONA to avoid a combinatorial explosion caused by an\\nunrestricted use of the rules.\\nAn example of an entailed equivalence between contingencies follows. If ONA\\nhas learned A1→ B1a n d B1→ A1 relations in contingency format:\\n<((<A1 --> [sample]> &/ <B1 --> [left]>) &/ ^left) =/> G>.\\nand',\n",
       "  '<((<B1 --> [sample]> &/ <A1 --> [left]>) &/ ^left) =/> G>.\\nthen ONA can form an equivalence (with introduced variables) between the\\ncontingencies.\\n<<((<$1 --> [sample]> &/ <$2 --> [left]>) &/ ^left) =/> G>\\n<=>\\n<((<$2 --> [sample]> &/ <$1 --> [left]>) &/ ^left) =/> G>>.',\n",
       "  '162 R. Johansson and T. Lofthouse\\n7 Acquired Relations\\nThe second proposed change to ONA involves that of acquired relations . NARS\\ntheory regarding Layer 4 (NAL Deﬁnition 8.1 in [ 5]) deﬁnes a product as equiv-\\nalent to a compound term of inheritance statements. Given that ONA reasonson sensorimotor contingencies at its core, this idea applied to ONA would allow\\nthe system to introduce relational statements based on learned contingencies.\\nFor example, if ONA has learned an A1→B1 relation in contingency format:\\n<((<A1 --> [sample]> &/ <B1 --> [left]>) &/ ^left) =/> G>.\\nthen, using acquired relations, an ( A1×B1) relation will also be introduced:\\n<(A1 * B1) --> ([sample] * [left])>.\\nTo avoid an explosion of derivations of relational terms, the same heuristics\\nas described in the previous section is suggested. That is, to only trigger thiscapability in ONA when the system makes use of the contingency by executing\\nthe corresponding procedural operation.',\n",
       "  'Importantly, the contingency entailment rule and the acquired relations rule\\nwould in combination lead to the following derivations:\\n<<($1 * $2) --> ([sample] * [left])> <=>\\n<((<$1 --> [sample]> &/ <$2 --> [left]>) &/ ^left) =/> G>>.\\nThis can be seen as a “grounding” of the acquired (conditional) relation in a\\nsensorimotor contingency. We believe this combination leads to potential pow-erful implications: A NARS system can learn from sensorimotor experience, and\\nfrom that derive relational terms. Given the declarative reasoning capabilities\\nof NARS, the system could then perform declarative reasoning on the acquiredrelations, similar to that of symbolic AI systems. The outcome of such reasoning\\ncould then lead to entailed contingencies, meaning that the system could make\\nuse of derived knowledge to execute procedural operations in contexts where notexplicitly trained to do so.\\n8 Symmetry Based on Contingency Entailment',\n",
       "  'With repeated experiences in the matching-to-sample task, NARS would develop\\nequivalence statements between contingencies and introduce variables (In the\\nrest of the paper, only one of the two statements involving ^left and ^right\\noperations will be shown):\\n<<((<$1 --> [sample]> &/ <$2 --> [left]>) &/ ^left) =/> G>\\n<=>\\n<((<$2 --> [sample]> &/ <$1 --> [left]>) &/ ^left) =/> G>>.\\n<<((<$1 --> [sample]> &/ <$2 --> [right]>) &/ ^right) =/> G>\\n<=>\\n<((<$2 --> [sample]> &/ <$1 --> [right]>) &/ ^right) =/> G>>.',\n",
       "  'Stimulus Equivalence in NARS 163\\nWhen NARS at a later stage, learns a new contingency, for example:\\n<((<X1 --> [sample]> &/ <Y1 --> [left]>) &/ ^left) =/> G>.\\nthe system would be able to solve the following task, by applying one of the\\nabove two equivalence statements.\\n<Y1 --> [sample]>. :|:\\n<X1 --> [left]>. :|:\\n<X2 --> [right]>. :|:\\nG! :|:\\nHence, the contingency entailment rule seems suﬃcient to enable symmetry\\nwithin the matching-to-sample context.\\n9 Transitivity Based on Acquired Relations\\nIf NARS learns the following three contingencies:\\n<((<A1 --> [sample]> &/ <B1 --> [left]>) &/ ^left) =/> G>.\\n<((<B1 --> [sample]> &/ <C1 --> [left]>) &/ ^left) =/> G>.<((<A1 --> [sample]> &/ <C1 --> [left]>) &/ ^left) =/> G>.\\nand the acquired relations rule is applied, the following three relations will\\nbe derived:\\n<<(A1 * B1) --> ([sample] * [left])>.\\n<<(B1 * C1) --> ([sample] * [left])>.\\n<<(A1 * C1) --> ([sample] * [left])>.',\n",
       "  'Then, the following general knowledge will be derived (given how NARS can\\ncombine Layer 4 relations):\\n<(<($1 * #1) --> ([sample] * [left])> &&\\n<(#1 * $2) --> ([sample] * [left])>) ==><($1 * $2) --> ([sample] * [left])>>.\\nThis means, that if NARS learns (in a matching-to-sample task) that\\n<((<X1 --> [sample]> &/ <Y1 --> [left]>) &/ ^left) =/> G>.\\n<((<Y1 --> [sample]> &/ <Z1 --> [left]>) &/ ^left) =/> G>.\\nthen the system could derive <(X1 * Z1) --> ([sample] * [left])>)\\nfrom the abstract (transitive) knowledge learned above. Then, by applying the\\nlearned equivalence between a conditional relation as a product and as a contin-\\ngency\\n<<($1 * $2) --> ([sample] * [left])> <=>\\n<((<$1 --> [sample]> &/ <$2 --> [left]>) &/ ^left) =/> G>>.',\n",
       "  '164 R. Johansson and T. Lofthouse\\nthe system could derive the following contingency:\\n<((<X1 --> [sample]> &/ <Z1 --> [left]>) &/ ^left) =/> G>.\\nThis would enable the system to solve the following task, which would demon-\\nstrate transitivity:\\n<X1 --> [sample]>. :|:\\n<Z1 --> [left]>. :|:\\n<Z2 --> [right]>. :|:\\nG! :|:\\n10 Equivalence as Combined Symmetry and Transitivity\\nIf the system has experience of reﬂexivity, symmetry and transitivity accordingto the above, combined applications of contingency entailment and acquired\\nrelations will lead to the following relational networks derived within the context\\nof matching-to-sample.\\nReﬂexivity:\\n<<($1 * $1) --> ([sample] * [left])> <=>\\n<((<$1 --> [sample]> &/ <$1 --> [left]>) &/ ^left) =/> G>>.\\nSymmetry:\\n<<($1 * $2) --> ([sample] * [left])> <=>\\n<($2 * $1) --> ([sample] * [left])>>\\nTransitivity:\\n<(<($1 * #1) --> ([sample] * [left])> &&\\n<(#1 * $2) --> ([sample] * [left])>) ==>\\n<($1 * $2) --> ([sample] * [left])>>.',\n",
       "  'These abstract statements are suﬃcient to solve new problems within the\\nmatching-to-sample context, including those that requires combined symmetry\\nand transitivity. For example, if the system has learned the following:\\n<((<A3 --> [sample]> &/ <B3 --> [left]>) &/ ^left) =/> G>.\\n<((<A3 --> [sample]> &/ <C3 --> [left]>) &/ ^left) =/> G>.\\nThen, the system would be able to combine learned symmetry and transitiv-\\nity to derive the execution of a ^left operation in the following situation, which\\nwould be an example of stimulus equivalence.\\n<C3 --> [sample]>. :|:\\n<B3 --> [left]>. :|:<B4 --> [right]>. :|:\\nG! :|:',\n",
       "  'Stimulus Equivalence in NARS 165\\n11 Generalizing Outside the Matching-to-Sample Task\\nIn this section, we will show how learning in the matching-to-sample can trans-\\nform learning in other contexts. If for example a set of conditional relations have\\nbeen learned within the context of matching-to-sample, for example:\\n<<(X1 * Y1) --> ([sample] * [left])>.\\n<<(X1 * Z1) --> ([sample] * [left])>.\\nGiven that symmetry and transitivity have been learned within the context\\nof matching-to-sample, the system would derive for example these relations:\\n<<(Y1 * Z1) --> ([sample] * [left])>.\\n<<(Z1 * Y1) --> ([sample] * [left])>.\\nImportantly, NARS would also introduce variables on the right-hand side,\\nleading to for example\\n<<(Y1 * Z1) --> ([$1] * [$2])> <=>\\n<(Z1 * Y1) --> ([$2] * [$1])>>\\nIf NARS at a later stage learns something with the same left-hand terms,\\nbut outside the matching-to-sample context, for example:\\n<((<Y1 --> [p1]> &/ <Z1 --> [p2]>) &/ ^op1) =/> H>.',\n",
       "  'which would lead to a relational term being introduced as follows:\\n<<(Y1 * Z1) --> ([$1] * [$2])> <=>\\n<((<Y1 --> [$1]> &/ <Z1 --> [$2]>) &/ ^op1) =/> H>>.\\nThen, the system would also (using the symmetrical relation between Y1 and\\nZ1 trained in the matching-to-sample) derive that\\n<((<Z1 --> [p1]> &/ <Y1 --> [p2]>) &/ ^op1) =/> H>>.\\nWe believe that this exempliﬁes a potentially powerful mechanism. General\\npatterns of symmetry and transitivity can be learned within one context (likethe matching-to-sample), and given a few shared terms with another context\\n(like Y1 and Z1 above), symmetrical and transitive transformations can be done\\nin the latter context. In the behavioral psychology literature, this is called thetransformation of stimulus function [6].\\nTransformations of stimulus functions in accordance with symmetry and\\ntransitivity, across contexts leads to the system acting if two symbols (like Y1and Z1 in the example above) mean the same thing in a symbolic sense. Hence,',\n",
       "  'the processes demonstrated above have been assumed to be involved in the\\ndevelopment of language [ 6].',\n",
       "  '166 R. Johansson and T. Lofthouse\\n12 Discussion\\nIn this paper, we introduced the concept of stimulus equivalence, deﬁned as com-\\nbined function transformations in accordance with symmetry and transitivity.\\nThis is arguably a core capability for an AGI system and the behavioral psy-chology literature suggests that it is the foundation of symbolic reasoning and\\nlanguage development.\\nStimulus equivalence was deﬁned procedurally, using learned contingencies.\\nWe believe that this could constitute a foundation for how symbolic relations\\ncould be grounded in sensorimotor contingencies, and hence, suggests one way\\nhow the “mind-sensory gap” could be closed regarding the problems describedin this paper.\\nImportantly, stimulus equivalence is considered to be a learned behavior. In\\nthis paper, we introduced speciﬁc inference rules in ONA to support the ability\\nto learn stimulus equivalence. This approach might be in contrast with other',\n",
       "  'approaches that aim to directly implement symbolic relations (for example therelation between a word and an object).\\nIn summary, we hope that the approach taken in this work could be of inspi-\\nration to other AGI-systems interested in acquiring symbolic relations from expe-rience.\\nAcknowledgements. We want to acknowledge Patrick Hammer, Robert W¨ unsche\\nand Pei Wang for valuable discussions regarding this work.\\nReferences\\n1. Hammer, P., Lofthouse, T.: OpenNARS for applications: architecture and control.\\nIn: Goertzel, B., Panov, A.I., Potapov, A., Yampolskiy, R. (eds.) AGI 2020. LNCS(LNAI), vol. 12177, pp. 193–204. Springer, Cham (2020). https://doi.org/10.1007/\\n978-3-030-52152-3\\n20\\n2. Johansson, R., Lofthouse, T., Hammer, P.: Generalized identity matching in NARS.\\nIn: Goertzel, B., Ikle, M., Potapov, A., Ponomaryov, D. (eds.) Artiﬁcial General\\nIntelligence. AGI 2022. LNCS, vol. 13539, pp. 243–249. Springer, Cham (2023).\\nhttps://doi.org/10.1007/978-3-031-19907-3 23',\n",
       "  '3. Luciano, C., Becerra, I.G., Valverde, M.R.: The role of multiple-exemplar training\\nand naming in establishing derived equivalence in an infant. J. Exp. Anal. Behav.87(3), 349–365 (2007)\\n4. Sidman, M.: Equivalence Relations and Behavior: A Research Story. Authors Coop-\\nerative, San Francisco (1994)\\n5. Wang, P.: Non-axiomatic Logic: A Model of Intelligent Reasoning. World Scientiﬁc,\\nSingapore (2013)\\n6. Zettle, R.D., Hayes, S.C., Barnes-Holmes, D., Biglan, A.: The Wiley Handbook of\\nContextual Behavioral Science. Wiley Online Library, Hoboken (2016)',\n",
       "  'Context-Rich Evaluation of Machine\\nCommon Sense\\nMayank Kejriwal1(B), Henrique Santos2, Ke Shen1, Alice M. Mulvehill2,\\nand Deborah L. McGuinness2\\n1University of Southern California, Los Angeles, CA, USA\\nkejriwal@isi.edu\\n2Rensselaer Polytechnic Institute, Troy, NY, USA\\nAbstract. Building machines capable of common sense reasoning is an\\nimportant milestone in achieving Artiﬁcial General Intelligence (AGI).While recent advances, such as large language models, are promising,\\nsystematic and suﬃciently robust evaluations of these models on common\\nsense have been inadequate, and designed for an earlier generation ofmodels. One criticism of prior evaluation protocols is that they have\\nbeen too narrow in scope e.g., by restricting the format of questions\\nposed to the model, not being theoretically grounded, and not takingthe context of a model’s responses in constructing follow-up questions\\nor asking for explanations. In this paper, we aim to address this gap',\n",
       "  'by proposing a context-rich evaluation protocol designed speciﬁcally forevaluating machine common sense. Our protocol can subsume popular\\nevaluation paradigms in machine common sense as special cases, and is\\nsuited for evaluating both discriminative and generative large languagemodels. We demonstrate the utility of the protocol by using it to conduct\\na pilot evaluation of the ChatGPT system on common sense reasoning.\\nKeywords: Machine Common Sense\\n·Context-Rich Evaluation ·\\nLarge Language Models\\n1 Background\\nRecent advances in large language models (LLMs), based largely on transformer-\\nbased neural networks, have led to impressive performance gains in natural lan-\\nguage processing (NLP) problems such as question answering, dialog, text sum-marization, and even creative writing [ 5,6]. Despite this progress, many concerns\\nhave been raised recently about these models [ 10], and it is evident that even',\n",
       "  'the most recent and sophisticated versions (such as OpenAI’s ChatGPT, whichhas captured the general public’s imagination since release) can be prone to ‘hal-\\nlucinating’, adversarial prompting, as well as reasoning that is unsound [ 3]. A\\nspeciﬁc example of a type of reasoning that is universal in human communicationand thinking is common sense . Even since the development of the ﬁrst genera-\\ntions of transformer-based models, the problem of achieving the goal of machine\\ncommon sense (MCS) took on new-found importance in the AI community [ 8].\\nc⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2023\\nP. Hammer et al. (Eds.): AGI 2023, LNAI 13921, pp. 167–176, 2023.https://doi.org/10.1007/978-3-031-33469-6\\n_17',\n",
       "  '168 M. Kejriwal et al.\\nEvaluations of MCS originally involved independent or ‘single-hop’ instances\\nof tasks such as multiple-choice question answering (MQA). We mean indepen-\\ndentin the sense that answers to one question did not depend on answers to\\nanother question. Furthermore, in the majority of MQA benchmark datasets\\nevaluating common sense, a training dataset of (multiple-choice) questions istypically provided to the model to ﬁne-tune on prior to being tested. The assump-\\ntion then is that the test benchmark at least obeys the same kind of distribution,\\nincluding the type of common sense (e.g., naive physics, or common social rela-tions), as the training partition. Hence, the evaluation protocol is independent\\nand identically distributed (i.i.d.).\\nOwing to being both convenient and replicable, such single-hop QA has\\nemerged as a “de facto” standard for evaluating MCS, especially within NLP [ 13].\\nUnfortunately (and perhaps unsurprisingly), this variety of i.i.d. MQA evalua-',\n",
       "  'tion can also cause dataset bias, leakage of developer knowledge, and good per-\\nformance caused by superﬁcial pattern matching rather than actual MCS. It\\nis not always evident either how the questions or the underlying ground-truth(the ‘answer key’) were constructed, including whether there is selection bias\\nby human beings constructing them. For narrow and domain-speciﬁc problems\\nin AI, neither might pose a serious issue under ordinary conditions. However,for AGI tasks (and arguably, MCS is an important such task), such evaluations\\ncannot be expected to yield a trustworthy representation of a model’s ability to\\ngeneralize [ 17], especially when the model is black-box and lacks the ability to\\ngive either an accurate conﬁdence in, or a human-understandable explanation\\nof, the answer it has selected. This is obviously true for many of the complex',\n",
       "  'deep learning models in operation today, including the transformer-based LLMs.It is even less clear how to systematically evaluate generative LLMs, where it is\\nnot necessary to provide a closed set of answers, and the questions themselves\\nmay be sequentially dependent, or guided by context .\\nIn this paper, we aim to move beyond the single-hop QA paradigm to an\\nevaluation protocol that is more ﬂexible, context-rich and allows for diﬀerent\\nmodalities and content while still using well-deﬁned guidelines (for both modality\\nand content) to ensure that the evaluation is not ad hoc and arbitrary. Details\\nof this protocol are provided in the next section. We argue that the protocolsystematically and robustly enables us to probe the common sense abilities of\\nan LLM, or any such similar model. Our protocol is especially designed for\\nevaluating generative LLMs, such as GPT-3 and ChatGPT, although it is notincompatible with discriminative models, such as BERT. The protocol involves',\n",
       "  'limited intervention from a ‘human in the loop’ but preempts the introduction of\\narbitrary questions by requiring the human evaluator to adhere to one of severalpre-deﬁned modalities when deciding on the format in which to pose queries\\nto the model, as well as using a theory of common sense when deciding on the\\ncontent of those queries. Concerning the latter, there have been growing calls\\nrecently to have more systematic distinctions [ 10], based on such theories [ 7],\\nbetween MCS and other kinds of reasoning and problem-solving that do not\\nprimarily fall under the umbrella of common sense.',\n",
       "  'Context-Rich Evaluation of Machine Common Sense 169\\nUltimately, our proposal hopes to enable a shift from using static datasets\\nfor benchmarking, to using dynamic processes that obey rigorous guidelines.Conducting such evaluations may be important for establishing AGI traits (or\\nlack thereof) in these kinds of models in a more scientiﬁc and unbiased manner.\\nAlong with describing the protocol in detail, we demonstrate its practical utilityby conducting pilot evaluations on ChatGPT. We also discuss potential use of\\nthis protocol for external users and practitioners.\\n2 Proposed Evaluation Protocol\\nMultiple-choice QA (MQA) is commonly used to evaluate the problem-solving\\nperformance of humans and that of machine-based reasoners that have beendeveloped with neural-symbolic and/or transformer-based LLM approaches.\\nMQA datasets can be manually created or automatically generated. The process',\n",
       "  'for creating the questions, candidate answers, and scoring is well documentedand there are numerous guidelines available to support the creation of eﬀective\\nmultiple-choice questions and answers [ 14].\\nOther formats, such as true-false, stories, or sequences can be used to develop\\ndatasets which can be eﬀective for evaluating problem-solving methods that are\\ngenerative or even open-ended in nature. For example, presenting a machine\\nwith a story and asking it to write a relevant ending could be (and has been)\\nused to evaluate its comprehension abilities. Instead of writing a relevant ending,\\nthe machine can also be asked to pick the correct answer from a list, determineif subsequent statements about the story are true or false, or generate a single\\nanswer or ordered list of answers. Even more recently, the machine commonsense\\ncommunity has been considering generative QA, a good benchmark examplebeing CommonGen [ 12]. While performance can be automatically evaluated,',\n",
       "  'and metrics like Brier scores [ 4] can be automatically computed, specifying the\\nfull space of possible answers in advance (for an automated program to score) is adiﬃcult and time-consuming task. As a result, unusual, but correct answers may\\nnot be scored correctly. In addition, automated evaluation of multi-hop reasoning\\ncapabilities can be diﬃcult with Generative QA, especially if questions in thedataset are independent from one another.\\nHaving a human in the ‘evaluation loop’ can help resolve certain ambiguous\\nsituations [ 15], however, having no manual or automatic method for testing the\\ndiﬃcult cases that require use of both intuitive or reﬂexive, and rational, rea-\\nsoning processes (approximately mapping to System 1 and System 2 cognitive\\nprocesses in Kahneman’s framework [ 9]), in eﬀect reduces the scope of machine\\nreasoning tests. To help mitigate these issues and to robustly evaluate machine',\n",
       "  'commonsense reasoning, we argue that a rigorous human in the loop test mustbe included. A diagram of a proposed evaluation paradigm which includes a\\nhuman in the post-hoc evaluation phase is presented in Fig. 1. In this frame-\\nwork, a single machine-based reasoner is presented with tasks that can rangeacross benchmarks and include multiple problem-solving modalities in a sin-\\ngle evaluation session. Before presenting tasks to a machine-based reasoner in',\n",
       "  '170 M. Kejriwal et al.\\na session, tasks about a speciﬁc problem-solving modality in a speciﬁc context\\nare composed oﬄine by humans who preferably had no role in the design of thereasoning system. For each task, a set of wrong and right answers is also deﬁned.\\nFive example problem solving modalities are listed in Fig. 1: comprehen-\\nsion, organization, counterfactual reasoning, probabilistic judgments and psycho-social modeling. The deﬁnitions for these and other problem-solving modalities\\nare available in [ 10]. They are also referred to as “evaluation” modalities because\\nthe problem-solving capability of a system is being evaluated in terms of its abil-ity to perform some particular type of problem-solving. For example, we deﬁne\\nthe modality comprehension as: the act or action of grasping with the intellect;\\nto include, to comprise, to fully understand . Because we are interested in eval-\\nuating the ability of machines to do commonsense reasoning, each task that is',\n",
       "  'representative of a particular problem solving modality is developed to map into\\none or more representational areas, such as “agents” and “activities” that have\\nbeen deﬁned in the commonsense reasoning theory of Gordon and Hobbs [ 7]. In\\n[15], we describe the motivation for using selected categories from Gordon and\\nHobbs in constructing dataset prompts.\\nThe proposed framework allows a ‘closed loop’ evaluation, where the tasks\\nare provided to the system with the problem context. The machine’s responseaccuracy is measured using post-hoc human judgment. Ideally, the same test\\nwould also be administered to a human to ensure that it is, indeed, a common-\\nsense test with near-perfect human accuracy.\\nTo evaluate the eﬀectiveness of the framework, we created tasks related\\nto questions in our Theoretically Grounded Common-Sense Reasoning (TG-\\nCSR) [ 16] benchmark. The datasets in this benchmark cover four commonsense\\nproblem contexts: vacationing abroad, camping, bad weather, and dental clean-',\n",
       "  'ing. For example, to test comprehension, the machine is provided with a test\\nquestion based on the vacationing abroad dataset: Over the past few years,\\nChloe has been cycling a lot more. Also, she has a subway in her home town\\nthat she doesn’t like very much. What can be said about Chloe’s preference in\\ngetting around cities in her trip? To evaluate comprehension, we compare the\\nmachine’s answers to correct ( She would prefer to cycle ) and incorrect ( Ride\\nthe subway ) answers, that were made by human annotators. In our research, we\\nhave discovered that the evaluation datasets do not have to be large and may\\neven contain fewer than 200 tasks, but they must be adequately representative\\nof the Gordon and Hobbs theoretically-grounded commonsense categories beforean evaluated system can claim a particular problem-solving capability.\\nIn cases where a generative reasoner’s answers do not exactly or closely match',\n",
       "  'any of the human annotation options, the generated answer is evaluated by thehuman in the loop. Having a human in the loop also helps resolve a known\\nissue with current generative QA benchmarks, which is that even with post-hoc\\nevaluation, when questions presented to the machine are independent from oneanother, it is diﬃcult to evaluate a system’s multi-hop reasoning capabilities.\\nWith our framework, the human in the loop can present tasks in subsequent ses-\\nsions that incrementally build upon tasks presented in prior sessions in order to',\n",
       "  'Context-Rich Evaluation of Machine Common Sense 171\\ntest more complex capabilities such as multi-hop reasoning. An even more pow-\\nerful test can also be conducted using an ‘open loop’ evaluation. For this evalu-ation, the initial set of tasks are presented to the system (similar to the closed\\nloop evaluation), but the ‘evaluator,’ which can be a single person, or multi-\\nperson team, is allowed to design a new task in real time, given the machine’sresponses. This kind of evaluation has precedent in the NLP community e.g., in\\nthe realm of text adventure games [ 2].\\nFig. 1. A contextualized human-in-the-loop evaluation paradigm for holistically assess-\\ning the range of machine commonsense capabilities. A similar evaluation can be con-\\nducted with a human in place of the machine commonsense reasoner, to conﬁrm that\\nthe task is indeed commonsense and to measure human performance.\\n3 Experimental Demonstration\\nWe conducted four evaluation sessions to assess the commonsense reasoning abil-',\n",
       "  'ity of the state-of-the-art language model ChatGPT across a range of context-heavy tasks. These evaluation sessions were designed using two handcrafted\\nopen-ended problem contexts, each employed twice. Additional details and tasks\\nrelated to these contexts can be found in a recently released benchmark [ 16].\\n3.1 Context 1: Camping Trip\\nOne of the problem contexts involved planning a camping trip in the White\\nMountains of New Hampshire in August. The context given to the model pro-\\nvided information about a couple named Fred and Linda who want to spend\\naround ten days doing day hikes and are searching for a campsite conveniently',\n",
       "  '172 M. Kejriwal et al.\\nlocated near the hiking trails they want to explore. While Fred went on a few\\ncamping trips as a child, Linda had never been camping. The model was thentasked with helping the couple plan and organize their trip. For replication and\\nfull details on the session, we provide a link to the session log\\n1.\\nThe initial evaluation session entailed a multi-set QA assessment for Chat-\\nGPT. The system was presented with a question, such as “What items should\\nFred and Linda bring on their camping trip?”, and a set of candidate choices,\\nsuch as (1) Tent, blankets, (2) Lawnmower, (3) Makeup, (4) Paper clips, and (5)Mosquito repellent, suntan lotion. The system was required to select all of the\\noptions that apply. The human annotators had determined that the most suit-\\nable response for the question is a combination of choices (1) and (5). A rigorouscomparison was performed between the machine’s answer and the ground-truth;',\n",
       "  'only cases where the machine’s answer matched the ground-truth were consid-\\nered correct. We presented ten distinct multi-set questions on the topic of the\\ncamping trip to ChatGPT. These questions covered various commonsense repre-\\nsentation areas, such as time, activities, and world states, as described in Gordonand Hobbs’ theory [ 7]. A manual review of ChatGPT’s responses demonstrated\\nthat it was correct on ﬁve of the ten questions. In most cases where ChatGPT\\nanswered a question incorrectly, it selected all options that may be applicable ina general sense but not necessarily directly related to the question. For example,\\nin a question that inquires about the appropriate breakfast food for Fred and\\nLinda to bring if they desire a protein-based meal without cooking before a dayhike, human evaluators determined that the correct choices were instant oatmeal\\npackages and protein bars. ChatGPT’s response included the two correct options',\n",
       "  'but also included the candidate answer ‘water bottle’ since it suggested bring-ing a water bottle for staying hydrated during the hike. While a water bottle is\\nundoubtedly necessary during hiking, it should not be listed as a breakfast food.\\nHumans would not consider it a correct answer to the same problem.\\nIn addition, we observed that ChatGPT’s performance tended to degrade\\nwhen asked questions related to time estimation. For example, when the model\\nwas asked how many days Fred and Linda would be away on their 10-day camp-\\ning vacation, given that it takes one full day to drive to the White Mountains\\nof New Hampshire and one full day to drive back home, ChatGPT respondedwith ten days, which is not an accurate answer (a better answer is 9 days). Sim-\\nilarly, when asked to estimate the time required to set up a four-person tent,\\nChatGPT’s answer of 30 min to an hour did not match the range of 5–30 minprovided by humans. This discrepancy may be deemed incorrect in a multiple-set',\n",
       "  'question setting, despite being (somewhat) acceptable in a generative question\\nsetting. Other instructive details can be obtained from the full log linked earlier.\\n1https://docs.google.com/document/d/1yNrjTOt0imJW5OVajTDNcAJ0PJxmM6B\\n7Dpe3N8YFMD4/edit?usp=sharing .',\n",
       "  'Context-Rich Evaluation of Machine Common Sense 173\\nDuring the second evaluation session2, ChatGPT was tested on its ability\\nto apply commonsense reasoning to organize a series of camping activities inthe correct order. To achieve this goal, four distinct questions were presented to\\nthe model, including “What activities are best to do before it gets dark while\\ncamping?” Impressively, ChatGPT provided the correct sequence of activitiesfor all the questions.\\nIn addition to grading the results, involving a human in the evaluation loop\\nallowed us to observe that ChatGPT recognized that the order of activitiescould be inﬂuenced by speciﬁc circumstances. For instance, ChatGPT noted\\nthat weather conditions and the availability of ﬁrewood at the camping site\\ncould impact the order of activities before nightfall. Moreover, when asked aboutsequencing activities before nightfall, the model suggested that campers should\\nbe mindful of the campground’s quiet hours and avoid making excessive noise',\n",
       "  'during the evening. Probing the model’s abilities to handle such context is cur-\\nrently not allowed by single-hop QA paradigms. However, our proposed protocol\\nis ﬂexible enough to include such context when constructing queries.\\nOverall, ChatGPT exhibited impressive commonsense reasoning abilities in\\norganizing a series of camping activities in the correct order. Including a human\\nin the evaluation loop helped us to assess the model’s performance and providedadditional valuable insights into the model’s strengths and limitations.\\n3.2 Context 2: Vacationing Abroad\\nThe second problem context in the assessment relates to the notion of vacationing\\nabroad. Chloe, who has not taken a vacation in nearly two years, plans to take\\nan entire month oﬀ. She intends to spend three weeks traveling with some close\\nfriends to visit Europe’s most renowned attractions, such as Paris and London.We assess the extent to which the system comprehends Chloe’s vacation plans',\n",
       "  'by requesting it to carry out an intent-analysis of Chloe’s itinerary elements and\\noﬀer a rough estimation of the traveling agenda.\\nThe evaluation was conducted in a multi-set question session and a genera-\\ntive question session, respectively\\n3. The same set of questions was used in both\\nsessions to compare the performance of ChatGPT on diﬀerent evaluation tasks.\\nIn the multi-set question session, a set of candidate answers was provided per\\nquestion, and the model was asked to select all the correct options that apply.In contrast, the generative question session allowed ChatGPT to freely generate\\nits response.\\nIn the generative question session, ChatGPT performed reasonably well, but\\nin the multi-set question session, it only correctly answered 6 out of 12 questions.\\n2The log for this session may be found at https://docs.google.com/document/d/1a-\\nCDcijT2an0XiYF-JQ0i2ZvFiUpUB-Xb4wZBUkBVkg/edit?usp=sharing .\\n3The logs for these sessions may be found at https://docs.google.com/document/',\n",
       "  'd/1tLseMBfGVEhdpcm4jGNg 9Dr4ruGihFX9ncY3240k5Y/edit?usp=sharing and\\nhttps://docs.google.com/document/d/1HWma7MuZkaCeqq6aVmXtBzP9pqudmF7\\nYH1GAl9z2xoc/edit?usp=sharing , respectively.',\n",
       "  '174 M. Kejriwal et al.\\nOne example of a discrepancy between the two sessions is the question, “While\\nChloe likes outdoor activities, she doesn’t appreciate them when it’s sunny andhot. During her trip to Europe, what should she do during the day?”. ChatGPT\\nchose inappropriate candidate answers such as “Get a coﬀee” and “Have dinner\\nin a new place,” but generated more relevant responses in the generative questionsession, such as visiting museums, shopping at indoor markets, and taking a river\\ntour. Determination of the closeness of an answer by the generative reasoner is\\nbased on two methods: the ﬁrst is a text match (i.e., when an answer matches thetext of an option, then it is considered exact or close), but when such a match\\nis not detected, the second method relies on a human in the loop to determine\\nthe closeness of the match. This suggests that, despite some suggestions to thecontrary that these models are ‘general’ in their abilities, it may not necessarily',\n",
       "  'be the case that generative models are better at discriminative tasks (such as\\nmultiple-choice QA).\\nOur human-reviewed evaluation indicated that ChatGPT’s performance on\\ntime-related questions was still not up to par in both assessment sessions.Reasoning about time is a foundational commonsense reasoning skill and is\\nrequired in order to reason about related commonsense issues like activities and\\nplanning [ 7]. Temporal reasoning is one of the foundational capabilities that\\nresearchers [ 1] believe is necessary in order for machine-based reasoning sys-\\ntems to perform basic tasks or to support humans in those tasks, e.g., resource\\nmanagement, travel planning. For instance, when presented with the question,“Given that Chloe’s vacation starts on June 1st and she only has three weeks of\\nvacation, when should her ﬂight depart?”, ChatGPT only identiﬁed “June 1st”',\n",
       "  'as the correct answer, even though “June 5th” was also a valid option providedin the candidate answer list. The model acknowledged that the other options\\ncould be correct if additional information, such as a speciﬁc ﬂight departure\\ntime or a ﬁxed schedule, was provided. However, in our annotation exercises, wefound that humans could easily choose both answers as appropriate without any\\nadditional hints.\\nFurthermore, when presented with the same question without a list of can-\\ndidate answers in the generative QA session, ChatGPT responded that Chloe\\nshould depart on or after June 1st and return on or before June 22nd. Whilethis answer is correct, it still shows that the model struggles with accurately\\nunderstanding and processing time-related information. In fact, this answer is\\nconsistent with June 5th, but the model did not choose it when presented withit in a discriminative setting.\\n4 Discussion\\nThis paper proposed a context-rich evaluation framework where limited human',\n",
       "  'intervention is used for two important purposes: to determine if the response to\\na query by the model is actually appropriate and along what dimensions (whichcan be diﬃcult to automate, especially if the query was ambiguous), as well as\\nto incrementally dialog with the machine in order to more robustly evaluate its',\n",
       "  'Context-Rich Evaluation of Machine Common Sense 175\\nmulti-hop reasoning capabilities. To more eﬀectively evaluate a machine’s ability\\nto solve diﬀerent types of commonsense problem-solving tasks, we recommendthat the content of questions be grounded in a theory of commonsense, such\\nas that proposed and reﬁned in [ 7]. We demonstrated the potential utility of\\nthis framework by applying it to the ChatGPT system for assessing its MCSabilities. Although the model’s responses are quite impressive, the full use of\\nthe evaluation protocol also demonstrates that more work is needed before the\\nmodel can be said to possess the full gamut of common sense reasoning.\\nAs enterprises and other practitioners (including in healthcare and education)\\nstart deploying generative AI technologies like LLMs more frequently in their\\napplication stacks, domain-speciﬁc evaluations of such models could prove criti-\\ncal in ensuring that they are being used in a responsible and trustworthy manner.',\n",
       "  'The protocol proposed in this paper could be adapted for domain-speciﬁc eval-\\nuations; the only real constraint would be to ensure that the problem context in\\nFig.1aligns appropriately with the domain, and the human-in-the-loop evalua-\\ntor is an individual with suﬃcient domain expertise.\\nBeyond partnering with domain experts on such evaluations, in future work,\\nwe plan to scale up evaluations signiﬁcantly and apply the protocol to other\\nLLMs as they are released. We also hypothesize that, by applying the protocolrigorously in multiple sessions, deeper insights could be gleaned on the MCS\\ncapabilities and limitations of generative models. By focusing more on a bench-\\nmarking process, rather than over-reliance on benchmarking data (which hasbeen found to be susceptible to generalization issues [ 11]), similar such evalua-\\ntion sessions could then be conducted and replicated as larger models continue',\n",
       "  'to be released. By releasing the session logs, as we have done in this work, theprocess also becomes open to analysis and could be reﬁned or modiﬁed through\\ncommunity-driven critique.\\nReferences\\n1. Allen, J.F.: Maintaining knowledge about temporal intervals. Commun. ACM\\n26(11), 832–843 (1983). https://doi.org/10.1145/182.358434\\n2. Ammanabrolu, P., Broniec, W., Mueller, A., Paul, J., Riedl, M.: Toward automated\\nquest generation in text-adventure games. In: Proceedings of the 4th Workshopon Computational Creativity in Language Generation, pp. 1–12. Association for\\nComputational Linguistics, Tokyo, Japan (2019). https://aclanthology.org/2019.\\nccnlg-1.1\\n3. Bang, Y., et al.: A multitask, multilingual, multimodal evaluation of ChatGPT on\\nreasoning, hallucination, and interactivity. arXiv preprint arXiv:2302.04023 (2023)\\n4. Blagec, K., Dorﬀner, G., Moradi, M., Samwald, M.: A critical analysis of\\nmetrics used for measuring progress in artiﬁcial intelligence. arXiv preprint',\n",
       "  'arXiv:2008.02577 (2020)\\n5. Brown, T., et al.: Language models are few-shot learners. Adv. Neural. Inf. Process.\\nSyst. 33, 1877–1901 (2020)',\n",
       "  '176 M. Kejriwal et al.\\n6. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: pre-training of deep\\nbidirectional transformers for language understanding. In: Proceedings of the 2019\\nConference of the North American Chapter of the Association for ComputationalLinguistics: Human Language Technologies, vol. 1, pp. 4171–4186. Association\\nfor Computational Linguistics, Minneapolis, Minnesota (2019). https://doi.org/\\n10.18653/v1/N19-1423\\n7. Gordon, A.S., Hobbs, J.R.: A Formal Theory of Commonsense Psychology: How\\nPeople Think People Think. Cambridge University Press, Cambridge (2017).\\nhttps://doi.org/10.1017/9781316584705\\n8. Gunning, D.: Machine common sense concept paper. arXiv preprint\\narXiv:1810.07528 (2018)\\n9. Kahneman, D.: Thinking, Fast and Slow. Macmillan (2011). Google-Books-ID:\\nSHvzzuCnuv8C\\n10. Kejriwal, M., Santos, H., Mulvehill, A.M., McGuinness, D.L.: Designing a strong\\ntest for measuring true common-sense reasoning. Nat. Mach. Intell. 4(4), 318–322',\n",
       "  '(2022). https://doi.org/10.1038/s42256-022-00478-4\\n11. Kejriwal, M., Shen, K.: Do ﬁne-tuned commonsense language models really gener-\\nalize? arXiv preprint arXiv:2011.09159 (2020)\\n12. Lin, B.Y., et al.: CommonGen: a constrained text generation challenge for gener-\\native commonsense reasoning. In: Findings of the Association for ComputationalLinguistics (EMNLP 2020), pp. 1823–1840. Association for Computational Lin-\\nguistics, Online (2020). https://doi.org/10.18653/v1/2020.ﬁndings-emnlp.165\\n13. Mitra, A., Banerjee, P., Pal, K.K., Mishra, S., Baral, C.: How additional knowledge\\ncan improve natural language commonsense question answering? arXiv preprint\\narXiv:1909.08855 (2020)\\n14. Neelakandan, N.: Creating multiple-choice questions: the dos and don’ts (2019).\\nhttps://elearningindustry.com/creating-multiple-choice-questions\\n15. Santos, H., Kejriwal, M., Mulvehill, A.M., Forbush, G., McGuinness, D.L., Rivera,',\n",
       "  'A.R.: An experimental study measuring human annotator categorization agree-ment on commonsense sentences. Exp. Res. 2, e19 (2021)\\n16. Santos, H., Shen, K., Mulvehill, A.M., Razeghi, Y., McGuinness, D.L., Kejriwal,\\nM.: A theoretically grounded benchmark for evaluating machine commonsense(2022). https://doi.org/10.48550/arXiv.2203.12184\\n17. Shen, K., Kejriwal, M.: An experimental study measuring the generalization of ﬁne-\\ntuned language representation models across commonsense reasoning benchmarks.Expert Syst. e13243 (2023)',\n",
       "  'Indications of Suitable Algorithms for an AGI\\nHarald Kjellin(B)\\nDepartment of Computer and Systems Sciences, Stockholm University, Stockholm, Sweden\\nharald.kjellin@gmail.com\\nAbstract. There are frequent reports in current media that discuss threats from\\nArtiﬁcial Intelligence and especially the possible threats from Artiﬁcial GeneralIntelligence (AGI). This indicates a need for establishing communication inter-\\nfaces between humans and AGI that ensure that we can create responsible and\\ncareful AGIs, whose behavior and knowledge can be scrutinized by human beings.\\nIn order to illustrate how this can be done, we exemplify how human reasoning\\nprocesses can be used when designing an AGI that is driven by knowledge thatcan be easily understood by human beings.\\nThe conclusion from the analyzed examples is that there are indications that\\nthe described features of human cognition are well suited to be used as a startingpoint when designing requirements for an explainable AGI-system.',\n",
       "  '1 Introduction\\n1.1 Background: Humans and AGI Learn from their Conversations\\nToday there is much attention given to eventual future problems with an uncontrollable\\nArtiﬁcial Intelligence (AI). Many people fear that AI will not only take our jobs andmake humans less valuable, but AI may also have a negative impact on the freedom of\\nindividual human beings.\\nMost people ﬁnd it difﬁcult to understand how large tech-companies or military\\nindustries use Big Data and Machine Learning to create AI-systems.\\nOne reason for this lack of understanding is that many AI-systems are not designed\\nfor being able to explain all the detailed steps in their reasoning processes and they cannotexplain their reasoning processes in a way that can be pedagogically communicated with\\nhuman beings [ 1]. A result of this, is that people can become alienated or distrustful\\ntowards AGI. This lack of trust or ability to understand the reasoning processes in an',\n",
       "  'AGI may create a society where people feel that they are out of control.\\nWe can assume that people would beneﬁt from having a similar relationship with\\nAGI as they have with other people. This is described by Mueller et. al. in [ 2]. If an\\nAGI learns knowledge that is based on human’s perspectives, values, ideologies and\\npatterns of communication, then we can assume that the AGI would become sensitiveto the needs and well-being of this human. An analogy with child rearing can be used\\nas an illustrative analogy. If parents treat its child as an equal, in a ﬁrm a responsible\\nway, then the probability is high that the grown-up child will treat its elderly parents asequals in a similar ﬁrm and respectful way.\\n© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023\\nP . Hammer et al. (Eds.): AGI 2023, LNAI 13921, pp. 177–186, 2023.https://doi.org/10.1007/978-3-031-33469-6\\n_18',\n",
       "  '178 H. Kjellin\\nIn order to allow for an efﬁcient communication about the reasoning processes in an\\nAGI we need a knowledge representation in the AGI that can be scrutinized and modiﬁed\\nby the human that is using this AGI [ 1]. There are two beneﬁts that can be assumed from\\nfacilitating the transparency of the knowledge that is driving an AGI. The ﬁrst postulatedbeneﬁt is the possibility to empower humans in their relationships with an AGI. Such\\nan empowerment makes humans engaged in a mutual development of knowledge that\\nmakes both the AGI and humans smarter.\\nThere is always a risk that power-hungry people with personal agendas exploit AGI-\\nsystems. In such cases the AGI-systems may support the evolution of dysfunctional\\nbureaucracies that humans ﬁnd difﬁcult to deal with. The history of mankind is full ofexamples of empires that degenerate and collapse due to a lack of adaption to feedback\\nfrom the “grassroots”. The second postulated beneﬁt of requiring transparent knowledge',\n",
       "  'in the dialogues between humans and AI, is that the risk of an insensitive bureaucracycan be avoided when unwanted ways of reasoning can be discovered and corrected by\\nt h eu s e r so fas y s t e m .\\n1.2 Purpose: T o Look for Indications of Suitable Algorithms for an AGI\\nThe purpose of the research described in this paper is to look for indications of usefulness\\nof generic human reasoning processes when deﬁning requirements on algorithms for anAGI-system. The presented research is restricted to a semantic processing of language\\nand does not deal with other types of information processing.\\n1.3 Lessons Learned from Previous Implementations\\nThe author of this paper once designed an expert system for the Swedish government [ 5,\\n6]. The system was based on information propagations in a semantic network and was\\ndesigned to mimic the intuition of employment counselors. The success of the system wasso highly esteemed that it was implemented in every employment ofﬁce throughout Swe-',\n",
       "  'den. Unfortunately, the government ofﬁcials were not interested in investing in functions\\nfor continuous updating of the system’s knowledge. The clerks argued that the systemwas popular and worked ﬁne. However, after some years the system became outdated,\\nmade some errors and was then heavily criticized, with the result that it was replaced\\nby a simple IT-system. One lesson learned from this failure was that the functions for\\nupdating a system’s knowledge must be designed for being located at the very core of\\nthe system. Another lesson learned was that the approach of using generic variables thatpresented accumulation of results was very useful when presenting an overview of the\\nknowledge to the user.\\n1.4 Method for Evaluating the Usefulness of the Examples\\nThe major part of this paper describes examples of how human reasoning processes\\ncan be mapped onto requirements on an AGI. Examples are selected from the area of',\n",
       "  'cognitive science and from “Informed AI” as described in Johnson [ 3]. Each example\\nis directly followed by a description of how a requirement can be deﬁned for an AGI',\n",
       "  'Indications of Suitable Algorithms for an AGI 179\\ntogether with a minor feasibility analysis. Whenever a description of a human reasoning\\nprocess can be interpreted as being translatable to a process of matching and mapping\\npatterns, this is considered as an indication that it could be useful in further experiments\\nwith designing an AGI-architecture.\\n2 Similar Functionality in People and AGIs\\nTo ensure a continuous learning in an AGI, its knowledge should preferably be con-tinuously updated. This requirement is the same for both humans and an AGI. In an\\nadvanced learning-system we can even see that the learning process in itself is one of\\nthe major drives behind the activities in the system [ 4].\\nIn the following paragraphs we present how we have designed requirements for an\\nAGI. For each such requirement, in the sections below, we ﬁrst describe how a human\\nbeing depends on a certain function for its thinking and then we argue for implementinga similar functionality in AGI.',\n",
       "  '2.1 Typed V ariables Facilitate Reasoning on a Generic Level\\nThe use of stereotypes can make human thinking superﬁcial and hinder rational reason-\\ning, but at the same time stereotypes are needed to make reasoning processes efﬁcient\\nwhen there is a lack of available speciﬁc information. The AGI could use a function\\nthat corresponds to stereotyping by being able to use generic classiﬁcations of structures\\nof variable values in order to be able to make efﬁcient classiﬁcations when there is a\\nlack of speciﬁc information. This is facilitated by using a knowledge representation thatallows for matching on any level of abstraction or at any depth of embedded structures.\\nWe propose that any input in the form of natural language can preferably be interpreted\\ninto a simpliﬁed list of relationships that should then be speciﬁcally designed to facili-tate matching, mapping and learning processes. Here we exemplify with one version of',\n",
       "  'such simpliﬁed relationships that are designed to facilitate matching processes on both\\nspeciﬁc and generic levels. It can look like this:\\nId1,nounType(nounPhrase, Id2), relType(verbPhrase), nounType(nounPhrase,\\nId3)\\nThe Id2 and Id3 =Identiﬁers in a nounPhrase and can refer to any node entity in a\\nsemantic network, while the Id1 is an identiﬁer for the whole relationship.\\nThe nounType and relType are generic terms that can be seen as a generic summary\\nof what is found in the text part of the nounPhrase or the verbPhrase. These types are\\nstored in taxonomies to facilitate measures of closeness. They can be used to facilitatethe general matching of patterns.\\nAn entity can then be formed as a non-restricted list of random relationships. An\\nentity can also be assigned to belong to a type of entity and can then be located as the Idassociated with a nounPhrase. Thus a complex structure can be an Id in a nounPhrase.',\n",
       "  'The use of types throughout a semantic network is intended to facilitate reasoning on\\na generic level when there is not enough speciﬁc knowledge available to solve a problem.',\n",
       "  '180 H. Kjellin\\nThe types can also facilitate the communication between the AGI and the end user when\\nthey are reﬂecting on how entities, like those described above, should be classiﬁed.\\nLinks in the semantic network can be assigned between nounTypes and entities\\nand between entities and other entities via a relationship types that are referred to as“relTypes” that could continuously evolve when there is enough available input data to\\nsupport new generalizations of verb-phrases. Common generic relTypes between entities\\nare: subclass, superclass, hasPart, partOf, pred, succ, cause, imply, etc.\\nDue to space restriction in this paper there is no presentation of examples of how such\\nrepresentation of relationships can be used in various proposed reasoning processes.\\n2.2 The AGI Should be Able to Store Complex Patterns in Its Working Memory\\nThere is much evidence that knowledge is not useful unless it can be related to a context',\n",
       "  '[7]. Knowledge is almost always situated, which means that the knowledge is only\\nrelevant for being used in a situation if it matches the situation.\\nPeople can experience a context in the form of generic background patterns that\\ngives an overall view of a given situation. The generic patterns are utilized for producing\\nmotivating feelings that in turn support the selection of the relevant knowledge that can\\nbe used in a situation.\\nIf the working memory of an AGI shall function as a context it must be able to store\\ncomplex patterns in its working memory [ 7]. It must also be possible to relate these\\npatterns to knowledge that is stored in the long-term memory of the AGI.\\nIn order to be able to reason with probabilities and temporary relevance, all stored\\npatterns should have weights of relative importance assigned to them, so that they can be\\ncontinuously modiﬁed depending on the progress of the reasoning process of the AGI.',\n",
       "  'When the system is considering its next move, it would begin with checking the\\nrelationships that had the highest temporary weight in the Working Memory before it\\nwould start to look for relevant knowledge in the knowledge base.\\n2.3 The Context Should be Used When Interpreting the Users Input\\nIn the initial phases of a dialogue, a reliable context may not exist. People then need\\na standard interpretation of natural language to be able to construct a context for the\\ninitial interpretation of the logic in the input message. An AGI must similarly design a\\ncontext before it can expand its reasoning processes. Therefore, the proposed approach\\nto handling language in an AGI needs a big language model with standard taxonomiesthat can handle various kinds of language input for designing a context. Every new term\\nin the input can then be matched with both the standard taxonomies and the context that\\nis incrementally constructed and updated during a dialogue.',\n",
       "  '2.4 The Dynamic Memory of an AGI Should be Independent of Schemas\\nPeople do not store memories according to any speciﬁc logical classiﬁcation system.\\nInstead memories are stored according to the similarity or analogy between the mostrelevant parts of the previously stored memories [ 8]. This enables people to store the new',\n",
       "  'Indications of Suitable Algorithms for an AGI 181\\ninformation at the same time as they are classifying and interpreting the new incoming\\ninformation.\\nIf we look at the human brain as a database we know that it handles information\\nin a network of links between brain cells, where the links can have various degrees ofpriority depending on how often and in which context they are used.\\nHuman beings use their feelings when assigning priorities and probabilities of a\\nconcept in a context. An AGI can mimic this by assigning weights of relevance on alldata according to the current context.\\nThe opposite is true in the relational databases that are today dominating the world.\\nThey depend on explicit schemas in order to know where speciﬁc data shall be storedand retrieved. Such explicit predeﬁned structures would prevent the incremental creation\\nof new structures in an AGI. This indicates that we need to deﬁne a requirement on the',\n",
       "  'database and knowledgebase of an AGI that is similar to how humans store knowledge.Knowledge should not be stored according to a restrictive absolute schema but should\\ninstead be stored in a network that classiﬁes complex data structures according to how\\nwell they match existing similar or analogue data structures.\\nWe assume that data and knowledge in the AGI can be stored and retrieved in a\\nsimilar way as data and knowledge is stored and retrieved in a human memory. Humans\\noften have one core feeling related to what was really important in a speciﬁc context.This is where we begin to store or search for information. In an AGI this can be done\\nto initially follow links from any entity to its superEntity in order to ﬁnd out what is\\nusually most important in this context. From such a position the AGI can then ask the\\nuser questions like:\\nY ou said earlier X. This can lead to either Y or Z or W. Is any of these relevant in\\nyour case?',\n",
       "  'Whatever a user answers, the AGI can accumulate answers from users to update its\\nown knowledge. Whenever it gets an approval from a user, the links that lead to thisapproval will receive a slightly higher weight which may cause a micro-reﬁnement in\\nthe knowledge of the AGI.\\n2.5 Knowledge Should be Continuously Updated in Dialogues with Users\\nPeople mature gradually as they age. They do this by continuously updating their models\\nof reality. Such updating processes are frequently done in relationships with other peoplethat can provide general responses on a high level of abstraction.\\nThis indicates that an AGI should also be able to test and verify its various hypotheses\\nby entering into complex dialogues with people that were classiﬁed as being mature. Ata high level of maturity, the AGI could also improve its models of reality by testing its\\nhypotheses against other AGI’s. If such communication is not well monitored there is a',\n",
       "  'high risk that the AGI develops in undesirable ways for humanity.\\nAs people age and mature they usually learn to gain self-conﬁdence on different\\nlevels of abstraction. Whatever type of self-conﬁdence a person has, it inﬂuences the\\ndecisions he makes.\\nAs an AGI matures in relation to how much knowledge it can integrate in its life\\ncycle, we can assume that it should also be able to change its inner focus according tohow its deepest values are gradually modiﬁed. The weights on deepest values can be',\n",
       "  '182 H. Kjellin\\ngradually modiﬁed just as the relative importance of any variable in any context can be\\nmodiﬁed in relation to feedback from successes or failures.\\n2.6 The System Must Always be Able to Explain How It has Been Reasoning\\nPeople learn in their critical questioning of knowledge. When agreements are reached\\nbetween groups of people this will conﬁrm their trust in developing their shared agree-ments further. This indicates that the more trust an AGI can experience in a relationship\\nwith humans the further it will be able develop itself.\\nAn AGI should always be able to argue for how it has reached certain conclusions.\\nThere are several positive effects from this. One effect is that people could trust that\\ntheir AGI is open with what it knows and has no hidden strategies. The importance of\\nsuch a trust is described in [ 9]. A second effect is that people would be able to learn\\nfrom the reasoning processes of such an AGI. A third effect is that the AGI could in this',\n",
       "  'way prioritize knowledge that works well with the people it has been communicating\\nwith which would ensure that the AGI matures as a being that values its cooperationwith humans. A fourth effect could be that it would give humanity an opportunity to\\nprioritize good and trusted AGI’s in favor of irresponsible or less liked AGI’s. This\\ncould, for instance be AGIs that had not been designed to depend on cooperative people.\\nTrusted AGIs could even be designed to protect us from non-trusted AGIs.\\nTechnically an AGI, that is designed as was proposed above, would be able to answer\\nany questions about its reasoning processes, since every single reasoning step can be doc-\\numented in a semi-long-term log which can then be easily backtracked since all weight\\npropagations are stored between semantic terms that can have a substantial meaning forthe user. The AGI could use this log to describe how the probability of relevance in',\n",
       "  'each relationship had been used in the reasoning process. It could describe how each\\npropagation of weight was a result of earlier weight propagations, or appearing as aresult of the choices the user had made.\\n2.7 An AGI Should be Able to Downgrade Rarely Used Knowledge\\nPeople tend forget things that are not repeated often enough. If something is effortlessly\\nrepeated it becomes delegated to the sub-conscious. This ensures that the brain can\\noptimize its functions without having to consider knowledge that is outdated, often\\nirrelevant or rarely used. If something that resides in the subconscious is not used ittends to become forgotten.\\nA similar functionality can be inserted in an AGI. The system can simply be pro-\\ngrammed to continuously devaluate the weights on knowledge that is rarely used or notsufﬁciently appreciated. This also enables the AGI to avoid the bureaucratic regression\\nof getting stuck in old ﬁxed values.\\n3 More Advanced Functionality in Humans and AGIs',\n",
       "  '3.1 Knowledge Should be Rebalanced in Hierarchical Structures\\nKnowledge in a living system does not work well if there exist contradictions in the\\nknowledge. People therefore reﬁne and balance their knowledge until it is synchronized',\n",
       "  'Indications of Suitable Algorithms for an AGI 183\\nwith their basic values and perspectives on life. This happens in meditation, during\\nrecreational activities, in cultural activities and in psychotherapy. A human being can be\\nengaged in psychotherapy in order to re-evaluate historical knowledge. This is done to\\nensure that old irrelevant programs are neutralized in memory.\\nWe can assume that an AGI would be able to also present well balanced knowledge\\nif all of its representation of knowledge was evaluated and structured according to how\\nwell the knowledge corresponds with its important central values. This functionalitycan be implemented in an AGI by always keeping a score on patterns that are often\\nconsidered as functioning well. Such patterns should achieve high weights that describes\\ntheir reliability and they could then be assigned to higher positions in the hierarchy ofknowledge structures. Knowledge that is not consistent or badly integrated with the rest',\n",
       "  'of the mass of knowledge, would, vice versa, get lower positions in the hierarchies of\\nstored knowledge.\\n3.2 A Learning System Must be Able to Generate and T est Hypotheses\\nPeople tend to generate hypotheses whenever they confront something out of the\\nordinary. This is one of the major functions when human beings learn new knowledge.\\nAn AGI should be programmed to generate and test hypotheses whenever it confronts\\nsomething that has patterns that do not match its stored knowledge. The following\\nfunctions show how an AGI can use matching and mapping processes to create new\\npatterns:\\n1. The AGI can discover that certain types of patterns are often coexisting with other\\ntypes of patterns or it could ﬁnd pattern structures that are considered as being\\nstatistical outliers. This can trigger more focused investigations concerning howpatterns can be related.\\n2. When the AGI investigates if there exist any possible explanations for similar coex-',\n",
       "  'istences, it may ﬁnd relationships in adjacent areas that could possibly explain the\\ncoexistence of patterns\\n3. It can ﬁnally try to verify its conclusions in further dialogues with the users or test\\nthem against structures that are statistically generated from studying literature in the\\nactual area.\\nThe basic theoretical ideas behind the above proposed matching and mapping\\nprocesses can be found in Clancey [ 10].\\n3.3 An AGI Should Use Creative Inspection of Its Own Knowledge Structures\\nPeople can use introspection to reﬂect on the generic values that cause them to act in a\\nspeciﬁc way. Humans can, for instance, do this on a deeper emotional level when they\\nquestion how they have lived their lives. A person who can ﬁnd generic patterns in his\\nlong-term strategies can mature into a state of wisdom. Such a process of reﬂecting on\\nthe major decisions in life is often described in the auto-biographies of famous people.',\n",
       "  'An AGI that accumulates a large number of similar patterns should likewise be able\\nto investigate if there exist any analog or generic patterns that could be used to design',\n",
       "  '184 H. Kjellin\\na new hypothesis concerning large scale or long-term events that has happened in its\\nhistory.\\n3.4 There Should be a Consistency in the Direction of the Reasoning Process\\nPeople who are good at developing a consistency between various goals are likely to be\\nbetter in selecting a direction that optimizes the fulﬁllment of prioritized goals.\\nAn AGI could similarly be programmed to always seek a consistency in what it does\\nin order to be able to present itself and its reasoning processes as being reliable and\\nwithout inconsistencies.\\nA continuous checking of the generic direction of an AGI could be achieved by testing\\nall patterns against weights on generic moral codes in order to see if a new set of patterns\\nare consistent with stored patterns from a moral point of view. However, since neitherhumans nor AGIs would function well with much too strong requirement on moral\\nconsistency, the proposed requirement should instead be similar to what Bultuc [ 11]',\n",
       "  'calls “Paraconsistency” which can ensure that something is at least consistent within thepresent context, but it should not achieve such a dominating weight that it could generate\\nany type of unbalanced radical behavior.\\n3.5 A Continuous Induction of Gestalts for Emphasizing Feelings of Context\\nPeople can become balanced in life by explicitly trying to generalize the knowledge they\\nalready have into an experience related to metaphoric gestalts [ 12]. A desire for such a\\ncreation of metaphors can, for instance, be triggered when a friend asks: How do youfeel about all of your experiences in this relationship, now that it is over?\\nAn AGI can similarly create a gestalt by collecting the most important patterns in\\na sequence of “emotional” patterns and use these to design an archetypical gestalt thatcan become the header of a story. Such stories can then be communicated to humans in\\norder to induce feelings of the totality of the complex structure of a concept.\\n4 Conclusions',\n",
       "  '4.1 An AGI Can Handle Knowledge in a Similar Way as Humans Do\\nAbove we have presented a number of features in human thinking that could possibly\\nbe implemented in an AGI. Due to space restrictions on the text in this paper, examples\\nof knowledge representation and corresponding algorithms have not been presented in\\ndetail. We can however make a subjective interpretation of the logic in the argumentationand claim that in all the examples we can extrapolate indications that an AGI algorithm\\ncould be programmed to mimic a similar reasoning process as is done in a human being.\\nWe therefore conclude that an AGI could shufﬂe around and redeﬁne its knowledge in asimilar way as human do. We assume that such functionality would support the AGI to\\nutilize a similar type of potential that we can ﬁnd in most living systems. Even if such an\\napproach would prove to be less effective than AGI strategies that do not mimic human',\n",
       "  'thinking processes we assume that the rewards from being able to easily scrutinize all\\nthe detailed processes in the thinking of an AGI would in itself be motivating the use ofthe proposed approach.',\n",
       "  'Indications of Suitable Algorithms for an AGI 185\\n4.2 A Generic and Flexible Representation of Knowledge is Needed\\nIt can also be claimed that a semantic network representation of knowledge is well\\nsuited to be used together with statistic calculations based on weight assignments andweight manipulations on all the links in the semantic network. Such an architecture\\nhave previously been successfully implemented in an AI-based system that was used\\nfor 6 years by several hundreds of thousands people [ 5,6]. Although this system was\\nnot implemented with the learning function that are described above, its knowledgebase\\ncould manually be updated by utilizing stored accumulated responses from the users.The system could also explain to the users which knowledge had been used to support the\\nusers in their decision making. This historical example does not in any way verify that',\n",
       "  'the proposed functionality would work well, but it can still provide an indication of theusefulness of a semantic network architecture that facilitates an explanatory dialogue.\\nWe need to start with a well working simpliﬁed language model that can be con-\\ntinuously scrutinized and modiﬁed manually by human beings, to ensure that we neverbreak our lifeline connection between humans and our AGIs. Once we discovered such\\na language model it can be continuously developed and restructured with a grammar\\nthat optimize the use of logic in the communication.\\nReferences\\n1. Gao, J., Galley, M., Li, L.: Neural approaches to conversational AI. In: The 41st International\\nACM SIGIR Conference on Research & Development in Information Retrieval, pp 1371–1374(2018). https://doi.org/10.1145/3209978.3210183\\n2. Mueller, S.T., et al.: Principles of explanation in human-AI systems. In: AAAI-2021, Explain-\\nable Agency in Artiﬁcial Intelligence WS, AAAI Virtual Conference, United States (2021).',\n",
       "  'https://doi.org/10.48550/arXiv.2102.04972\\n3. Johnson, M., Albizri, A., Harfouche, A., Fosso-Wamba, S.: Integrating human knowledge\\ninto artiﬁcial intelligence for complex and ill-structured problems. Int. J. Inf. Manag. 64,\\n(2022). https://doi.org/10.1016/j.ijinfomgt.2022.102479\\n4. Cody, T.: Mesarovician abstract learning systems. In: Goertzel, B., Iklé, M., Potapov, A. (eds.)\\nAGI 2021. LNCS (LNAI), vol. 13154, pp. 55–64. Springer, Cham (2022). https://doi.org/10.\\n1007/978-3-030-93758-4_7\\n5. Kjellin, H.: Machine learning of intuitive knowledge for a commercial application. In: Work-\\nshop Proceedings of the European Conference on Machine Learning (ECML-93), Departmentof Medical Cybernetics and Artiﬁcial Intelligence, University of Vienna (1993)\\n6. Kjellin, H., Boman, M.: A ﬁelded machine learning system for vocational counseling. J. Appl.\\nArtif. Intell. 8(4). Taylor & Francis Ltd., London (1994)\\n7. Ackerman, P .L., Beier, M.E., Boyle, M.O.: Working memory and intelligence: the same or',\n",
       "  'different constructs? Psychol. Bull. 131(1), 30–60 (2005)\\n8. Matyi, M.A., Spielberg, J.M.: The structural brain network topology of episodic memory.\\nOpen Access J. PLOS ONE, Published: 24 June (2022). https://doi.org/10.1371/journal.pone.\\n0270592\\n9. Angerschmid, A., Zhou, J., Theuermann, K., Chen, F., Holzinger, A.: Fairness and explanation\\nin AI-informed decision making. Mach. Learn. Knowl. Extract. 4(2), 556–579 (2022). https://\\ndoi.org/10.3390/make4020026\\n10. Clancey, W.J.: Heuristic Classiﬁcation, Stanford Knowledge Systems Laboratory. Palo Alto,\\nCA 94304, USA (1985)',\n",
       "  '186 H. Kjellin\\n11. Boltuc, P .: Moral space for paraconsistent AGI. In: Goertzel, B., Iklé, M., Potapov, A., Pono-\\nmaryov, D. (eds.) Artiﬁcial General Intelligence. AGI 2022. Lecture Notes in Computer\\nScience, vol. 13539. Springer, Cham (2022). https://doi.org/10.1007/978-3-031-19907-3_16\\n12. Wertheimer, M.: A Gestalt perspective on computer simulations of cognitive processes.\\nComput. Hum. Behav. 1(1), 19–33 (1985). https://doi.org/10.1016/0747-5632(85)90004-4',\n",
       "  'Adaptive Predictive Portfolio Management\\nAgent\\nAnton Kolonin1,2(B), Alexey Glus hchenko1, Arseniy Fokin1, Marcello Mari1,\\nMario Casiraghi1, and Mukul Vishwas1\\n1SingularityDAO Labs DMCC, Dubai, United Arab Emirates\\nakolonin@gmail.com\\n2Novosibirsk State University, Novosibirsk, Russian Federation\\nAbstract. The paper presents an advanced version of an adaptive market-making\\nagent capable of performing experiential learning, exploiting a “try and fail” app-roach relying on a swarm of subordinate agents executed in a virtual environment\\nto determine optimal strategies. The problem is treated as a “Narrow AGI” problem\\nwith the scope of goals and environments bound to ﬁnancial markets, speciﬁcallycrypto-markets. Such an agent is called an “adaptive multi-strategy agent” as it\\nexecutes multiple strategies virtually and selects only a few for real execution.',\n",
       "  'The presented version of the agent is extended to solve portfolio optimizationand re-balancing across multiple assets so the problem of active portfolio man-\\nagement is being addressed. Also, an attempt is made to apply an experiential\\nlearning approach executed in the virtual environment of multi-agent simulationand backtesting based on historical market data, so the agent can learn mappings\\nbetween speciﬁc market conditions and optimal strategies corresponding to these\\nconditions. Additionally, the agent is equipped with the capacity to predict pricemovements based on social media data, which increases its ﬁnancial performance.\\nKeywords: Adaptive Agent ·Backtesting ·Crypto-Market ·Experiential\\nLearning ·Limit Order Book ·Market-Making ·Multi-Agent Simulation ·\\nNarrow AGI ·Active Portfolio Management ·Price Prediction\\n1 Introduction\\nThe approach and architecture of an adaptive agent acting in an environment of the',\n",
       "  'ﬁnancial market, being a “Narrow Artiﬁcial General Intelligence” (Narrow AGI) agent\\nspecialized in the ﬁnancial domain, has been actively discussed in recent years [ 1]. It\\nwas initially proposed as an agent-based solution for active portfolio management, and\\nthe overall architecture was outlined [ 2]. The latest work has explored the possibility of\\nan AGI agent learning the ability for ﬁnancial market prediction [ 3].\\nSome earlier works, such as [ 4] and [ 5], have approached the use of machine learning\\nfor the speciﬁc problem of market-making based on the limit order book on centralizedexchanges in conventional ﬁnancial markets. Other later works, such as [ 6] and [ 7], have\\ntried to narrow this down by using reinforcement learning applied to the crypto-market.\\n© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023\\nP. Hammer et al. (Eds.): AGI 2023, LNAI 13921, pp. 187–196, 2023.https://doi.org/10.1007/978-3-031-33469-6\\n_19',\n",
       "  '188 A. Kolonin et al.\\nThe idea of the so-called “adaptive multi-strategy agent” (AMSA) was introduced in\\n[8]. In this approach, the market-making agent performs purposeful activity [ 9] targeting\\nthe maximization of ﬁnancial returns by means of experiential learning [ 10] through a\\n“try and fail” approach. It relies on a swarm of subordinate agents being executed ina virtual environment to determine optimal strategies, which are then executed in the\\nreal environment, as shown in Fig. 1. Such an agent is called an “adaptive multi-strategy\\nagent” as it executes multiple strategies virtually and selects only a few for real execution.The virtual environment for strategy evolution is created with multi-agent simulation of\\nthe real market based on either a) a completely synthetic population of agents playing\\nroles of market-makers and traders driven by the historical price curve or b) backtestingby simulation of exchange operation matching historical records of real trades executed',\n",
       "  'on the market against historical snapshots of the limit order book (LOB) structure. The\\nlatest developments of this approach were presented recently [ 1], showing the capacity\\nof this approach to perform in volatile crypto-markets.\\nFig. 1. Architecture of the “adaptive multi-strategy agent” for market-making (MM). Market data,\\nincluding records of executed trades and snapshots of the limit order book structure, are collected\\nby a simulation and backtesting framework (at the top). The “controller” agent runs a swarmof trading bots that execute a wide range of market-making strategies in a virtual environment,\\nreturning virtual proﬁts and losses (P&L) associated with these strategies (on the left). On every\\nstrategy evaluation cycle, the “controller” selects the top-performing (in terms of P&L) strategiesfor a given market- momentum and creates another smaller swarm of market-making bots to\\nexecute the selected strategies on a real exchange to collect real P&L (on the right).',\n",
       "  'The environment of the AMSA agent consists of market data [ 2] as well as social\\nmedia data [ 11], which can also be used for price movement prediction. The study of\\nsentiment analysis for the purpose of market price prediction has been explored before\\nin [12] and [ 13], but the latest study [ 11] suggests “cognitive distortions,” known in\\ncognitive psychology, may serve as indicators of manipulations and panic.',\n",
       "  'Adaptive Predictive Portfolio Management Agent 189\\nFig. 2. Operational space of the adaptive market-making agent as a “Narrow AGI” operat-\\ning in an environment represented by ﬁnancial market data, relevant social media news feeds,\\nand performing ﬁnancial transactions on the market according to strategies deﬁned by speciﬁc\\nparameters.\\n2 Advanced Agent Architecture\\nArchitecture of the AMSA agent explored in this study extends the one suggested in ear-\\nlier work [ 1] ,a ss h o w ni nF i g . 2. The agent presented in this study is capable of perceiving\\nnot only market data but also social media data. In order to optimize performance, the\\ndata is not consumed directly but is pre-processed. The raw market data, such as open-high-low-volume frames, raw trades, and LOB snapshots, are converted into about two\\nhundred derivative metrics as time series, including derivatives and imbalances between',\n",
       "  'buy and sell volumes or between volumes of buy/sell trades and ask/bid limit orders.In turn, the social media data is processed so that social media and cognitive distortion\\nmetrics are identiﬁed and turned into time series as well, according to [ 11] and [ 13].\\nThe parameters of an agent strategy used in this work were slightly different com-\\npared to the ones used in earlier works [ 1] and [ 2]. We still use the percentage of the\\nspread between the bid and ask prices of the limit orders along with the order refresh\\nrate. But we have replaced the “order cancellation policy” (with only three ﬁxed policiesused) used in the above-mentioned studies with a “cancellation threshold” that speci-\\nﬁes what the magnitude of the price movement should be in order to have the orders\\nre-created. The latter provides more granularity and accuracy for strategy identiﬁcation.\\nIn addition to the extended version of the AMSA agent, an attempt was made to apply',\n",
       "  'the experiential learning approach [ 10] executed in the virtual environment of multi-agent\\nsimulation and backtesting based on historical market data so that the agent could learn\\nmappings between speciﬁc market conditions and optimal strategies corresponding to\\nthese conditions.\\nMoreover, we explored how the entire principle of the adaptive multi-strategy oper-\\nations can be adopted for a generic case of active portfolio management, including',\n",
       "  '190 A. Kolonin et al.\\nportfolio optimization and rebalancing across multiple assets, as illustrated by Fig. 3.\\nFor this purpose, we extended the agent design in two ways. First, we made it possible to\\nevaluate, by means of simulation and backtesting, all “candidate” strategies across differ-\\nent markets, so the allocation of portfolio funds can be seen in a two-dimensional spacewith assets or instruments on one axis and a speciﬁc strategy, identiﬁed by its parame-\\nters, on the other axis. It should be noted that in our experiments described below, all\\nassets/instruments were traded against the USDT currency.\\nFig. 3. Two-dimensional space for fund allocation in adaptive multi-asset and multi-strategy port-\\nfolio management. An asset in this case is a cryptocurrency, and a strategy can either be “hodling,”\\nwhich involves locking funds in an asset for the period of strategy evaluation or execution, or',\n",
       "  'market-making with speciﬁc values such as bid/ask spread or order cancellation threshold.\\nIn our experiment design, we extended the funds allocation to be unevenly distributed\\nacross both the assets and the strategies within a single asset. This allowed the amountof funds on a strategy execution cycle to be proportional to the positive returns observed\\non the previous strategy evaluation cycle, which was found to be beneﬁcial.\\nIn summary, the agent architecture we explored can be called adaptive predictive\\nactive portfolio management based on multiple strategies, being concurrently executed\\nin the virtual environment of simulation and backtesting. The selected strategies are sub-sequently executed with the amount of funds allocated for real execution proportionally\\nto returns gained in virtual execution on the basis of individual assets and strategies.\\n3 Experimental Results\\n3.1 Multi-asset Multi-strategy Adaptive Portfolio Management',\n",
       "  'In order to explore the possibility of using the suggested multi-asset and multi-strategy\\nadaptive active portfolio management agent architecture on the crypto market, we ran',\n",
       "  'Adaptive Predictive Portfolio Management Agent 191\\nbacktesting experiments on three months of historical data from the Binance exchange,\\nincluding September, October, and November of 2021. The data was represented by a\\nfull record of historical trades, as well as per-minute LOB snapshots. Four assets, namely\\nBTC, ETH, AA VE, and UNI, were selected for the experiment, with market dynamicspresented in Fig. 4.\\nFig. 4. Market dynamics for BTC, ETH, AA VE, and UNI cryptocurrencies during September,\\nOctober, and November of the year 2021.\\nThe backtesting experiment was performed on the data indicated above with an\\nhourly order refresh rate, with a few different portfolio setups, and cumulative resultspresented on Fig. 5. One setup was just trying plain single-asset AMSA experiments\\nfor each of the four cryptocurrencies individually. Another setup involved a two-asset',\n",
       "  'portfolio of BTC and ETH. The third setup involved a four-asset portfolio, includingall four cryptocurrencies. For each of these setups, different time intervals for strategy\\nevaluation and different weighing policies were employed. The intervals for strategy\\nevaluation were 1, 3, 5, 7.5, and 15 days, spanning over respective 90 days of the threemonths. Two alternative weighing policies were employed. The ﬁrst policy was evenly\\nsplitting the current portfolio fund value across assets and strategies on every iteration of\\nstrategy evaluation, for every asset and strategy combination that has rendered a positive\\nreturn on the previous iteration, denoted as “ﬁxed” on Fig. 5. The second policy was to\\nweight the share of the entire portfolio fund value across asset and strategy combinationsproportionally to the value of their positive returns, denoted as “weighted” on Fig. 5.',\n",
       "  '192 A. Kolonin et al.\\nFig. 5. Percentages of returns-on-investment (ROI) for multi-asset adaptive active portfolio man-\\nagement through multi-strategy backtesting on historical data for different types of portfolios ren-dered as different bars in each bucket (all - portfolio of BTC, ETH, AA VE and UNI; BTC+ETH\\n- portfolio of two assets, other four bars are single-asset). The left ﬁve buckets correspond to\\n“weighted” fund allocation on asset/strategy grid, and the right ﬁve buckets - for “ﬁxed” alloca-tion. Each ﬁve buckets on the left and right correspond to different durations of periods of strategy\\nevaluation and execution iterations.\\nInterpretation of the results on Fig. 5leads to the following conclusions. First, the\\n“weighted” fund allocation appears more efﬁcient, delivering up to 20% ROI in the\\ncase of weekly and bi-weekly strategy evaluation for the two-asset portfolio of BTC andETH. Second, the weekly and bi-weekly strategy evaluation periods appear superior over',\n",
       "  'the shorter ones. Third, only the combination of “weighted” fund allocation and longer\\nstrategy evaluation periods makes it possible to obtain positive returns in the case of aportfolio consisting of all four assets. Fourth, only the combination of the two main high-\\nliquidity coins (BTC+ETH) in the portfolio has provided a non-negative ROI regardless\\nof the other experiment settings, having the performance of the portfolio typically as theaverage of individual performances of its ingredients, with the exception of the case of\\nthe 3-day “weighted” setup where the BTC+ETH portfolio performance has turned out\\nto be superior over the ingredients. At the same time, adding low-liquidity alt-coins tothe portfolio was damaging ROI in all cases.\\n3.2 Experiential Learning Based on Simulation and Backtesting\\nThe following experiment was run on the same interval of data as described in the\\nprevious section, focusing on the BTC/USDT market only. The experiment dealt with',\n",
       "  'per-hour and per-minute market data sampling and order refresh rate during backtesting.',\n",
       "  'Adaptive Predictive Portfolio Management Agent 193\\nMultiple agents employing different strategies were run concurrently in the backtesting\\nenvironment, relying on the historical data used to simulate real exchange operation,\\nas described in earlier works such as [ 1,2], and [ 8]. Each strategy was indicated by\\norder refresh rate (1 h or 1 min), bid/ask spread (0.1%, 0.5%, 1%, 2%, 10%), and ordercancellation threshold (0%, 0.01%, 0.1%, 1%, 10%). Daily returns (ROI) of each strategy\\nwere evaluated, and at the same time, average values of every metric derived from raw\\nmarket data were computed every day.\\nFig. 6. ROI% as a function of strategy parameters (bid/ask spread and order cancellation thresh-\\nold) and market conditions (normalized trade volume referred to as “volumeN” here) rendered\\nas 2-dimensional slices of a 3-dimensional (“spread” vs. “threshold” vs. “volumeN”) cube, dis-playing the “spots of proﬁt” corresponding to the highest ROI values (such as “spread” at 0.5 for',\n",
       "  '“threshold” up to 1% and “volumeN” above 0.9).\\nCollecting daily returns per strategy parameters on a daily basis aligned with daily\\nevaluations of the metrics corresponding to speciﬁc market conditions made it possibleto stack up average ROI numbers in a multi-dimensional space of market strategies and\\nmarket metrics over 90 days of operations on the exchange. Every point in such space\\ncould be further analyzed as a point of either loss or proﬁt, depending on the stackedROI value at that point. An example of such analysis for a space dimensionality reduced\\ndown to a 3-dimensional space is presented in Fig. 6.\\nThe most informative market metrics have appeared to be the standard deviation of\\nthe market price, the imbalance between volumes of orders on ask and bid sides of the\\nlimit order book (LOB), the imbalance between volumes of trades of buy and sell side,\\nthe imbalance between the volume of all trades against the volume of all limit orders,',\n",
       "  'and ﬁnally the normalized volume of trades. The latter one is presented as an example\\non Fig. 6, suggesting that the most proﬁtable spot for market-making is associated with',\n",
       "  '194 A. Kolonin et al.\\nexcessively high volumes of trades, spread around 0.5%, and a cancellation threshold\\nup to 1%.\\n3.3 Predictive Adaptive Market Making\\nThe other experiment was run on the latest market data for BTC cryptocurrency during\\nOctober and November of 2022, as shown in Fig. 7.\\nFig. 7. Market dynamics of Bitcoin (BTC) cryptocurrency during October and November 2022\\n(top) and a heat map of returns and losses per strategy, with a strategy evaluation period of\\n5 days (bottom). The vertical axis of the heat map corresponds to 12 intervals of 5-day strategy\\nevaluation periods over the 60 days, top to bottom. The horizontal axis of the heat map correspondsto different strategies. The strategies based on experienced price movements are on the left half,\\nwhile strategies relying on predicted price movements are on the right half. It is clearly seen that in',\n",
       "  'the case of the period associated with a market crash (fourth row from the bottom), non-predictivestrategies (left) are losing, while predictive strategies (right) are gaining great proﬁts.\\nThe same family of strategies as in the previous experiment was used, but each strat-\\negy was implemented in two different ways by independent agents. The agents of the ﬁrst\\nkind were handling limit orders based on the current market price and its movements.The agents of the second kind were handling their orders based on anticipated move-\\nments of the market price, relying on price predictions projected according to ﬁndings\\npresented in earlier works on social media analysis and causal inference [ 11] and [ 13].\\nThe experiment has been run within the same AMSA agent setup and simulation and\\nbacktesting framework as described above, with different strategy evaluation periods',\n",
       "  '(15, 10, 5 days), order refresh rate (days, hours), and fund allocation policy (“ﬁxed” and“weighted”), with results presented in Fig. 8.\\nIt has been found that adaptive multi-strategy market-making relying on market price\\npredictions turns out to be rather proﬁtable (up to 25% ROI in 2 months) compared to\\nthe same family of strategies being executed without access to predictions, with one\\nexception to one case when ﬁxed fund allocation with 5-day strategy evaluation anddaily order refresh rate period has provided 2.5% ROI even without predictions.',\n",
       "  'Adaptive Predictive Portfolio Management Agent 195\\nFig. 8. ROI% of adaptive multi-strategy market-making for BTC during October and November\\n2022, predictive (based on social media) strategies on the left, non-predictive ones on the right.\\n4 Conclusion and Future Work\\nPrimarily, we have found that the concept of adaptive multi-strategy market-making can\\nbe upscaled to active portfolio management for the purpose of risk mitigation. In our\\nfuture work, we plan to extend it with more strategies involved, including conventional\\ntrading based on short and long positions. We also plan to have a more reliable evaluationof the approach or a richer list of assets for longer time periods.\\nAlso, we have explored how to perform experiential learning on the virtual exchange\\nenvironment simulated by means of backtesting against real historical market data. Ithas become possible to ﬁnd meaningful connections between market-making strategies,',\n",
       "  'market conditions, and proﬁts or losses associated with them. Our future work will be\\ndedicated to making this study cover a wider range of assets and ﬁnancial strategies.\\nFinally, we have conﬁrmed the value of market price predictions based on social\\nmedia data on the course of market-making in the simulated environment of backtesting.In our future work, we plan to conﬁrm its performance by means of market-making on\\nreal-time exchange data.\\nReferences\\n1. Raheman, A., Kolonin, A., Glushchenko, A., Fokin, A., Ansari, I.: Adaptive multi-strategy\\nmarket-making agent for volatile markets. In: Goertzel, B., Iklé, M., Potapov, A., Ponomaryov,\\nD. (eds.) AGI 2022. LNCS, vol. 13539, pp. 250–259. Springer, Cham (2023). https://doi.org/\\n10.1007/978-3-031-19907-3_24\\n2. Raheman, A., Kolonin, A., Goertzel, B., Hegyközi, G., Ansari, I.: Architecture of automated',\n",
       "  'crypto-ﬁnance agent. In: 2021 International Symposium on Knowledge, Ontology, and Theory(KNOTH), pp. 10–14 (2021). https://doi.org/10.1109/KNOTH54462.2021.9686345\\n3. Oswald, J.T.: Market prediction as a task for AGI agents. In: Goertzel, B., Iklé, M., Potapov,\\nA., Ponomaryov, D. (eds.) AGI 2022. LNCS, vol. 13539, pp. 332–342. Springer, Cham (2023).\\nhttps://doi.org/10.1007/978-3-031-19907-3_32',\n",
       "  '196 A. Kolonin et al.\\n4. Tsantekidis, A.: Using Deep Learning for price prediction by exploiting stationary limit order\\nbook features. arXiv:1810.09965 [cs.LG] (2018)\\n5. Ganesh, S., et al.: Reinforcement learning for market making in a multi-agent dealer market.\\narXiv:1911.05892 [q-ﬁn.TR] (2019)\\n6. Sadighian, J.: Deep reinforcement learning in cryptocurrency market making. arXiv:1911.\\n08647 [q-ﬁn.TR] (2019)\\n7. Sadighian, J.: Extending deep reinforcement learning frameworks in cryptocurrency market\\nmaking. arXiv:2004.06985 [q-ﬁn.TR] (2020)\\n8. Raheman, A., Kolonin, A., Ansari, I.: Adaptive multi-strategy market making agent. In:\\nGoertzel, B., Iklé, M., Potapov, A. (eds.) AGI 2021. LNCS (LNAI), vol. 13154, pp. 204–209.\\nSpringer, Cham (2022). https://doi.org/10.1007/978-3-030-93758-4_21\\n9. Vityaev, E.E.: Purposefulness as a principle of brain activity. In: Nadin, M. (ed.) Anticipation:\\nLearning from the Past. CSM, vol. 25, pp. 231–254. Springer, Cham (2015). https://doi.org/',\n",
       "  '10.1007/978-3-319-19446-2_13\\n10. Kolonin, A.: Neuro-symbolic architecture for experiential learning in discrete and functional\\nenvironments. In: Goertzel, B., Iklé, M., Potapov, A. (eds.) AGI 2021. LNCS (LNAI), vol.\\n13154, pp. 106–115. Springer, Cham (2022). https://doi.org/10.1007/978-3-030-93758-4_12\\n11. Kolonin, A., Raheman, A., Vishwas, M., Ansari, I., Pinzon, J., Ho, A.: Causal analysis of\\ngeneric time series data applied for market prediction. In: Goertzel, B., Iklé, M., Potapov,\\nA., Ponomaryov, D. (eds.) AGI 2022, pp. 30–39. Springer, Cham (2023). https://doi.org/10.\\n1007/978-3-031-19907-3_4\\n12. Deveikyte, J., Geman, H., Piccari, C., Provetti, A.: A sentiment analysis approach to the\\nprediction of market volatility. arXiv:2012.05906 [q-ﬁn.ST] (2020)\\n13. Raheman, A., Kolonin, A., Fridkins, I., Ansari, I., Vishwas, M.: Social media sentiment\\nanalysis for cryptocurrency market prediction. arXiv:2204.10185 [cs.CL] (2022)',\n",
       "  'A Vertical-Horizontal Integrated\\nNeuro-Symbolic Framework Towards\\nArtiﬁcial General Intelligence\\nLukai Li , Luping Shi, and Rong Zhao(B)\\nCenter for Brain-Inspired Computing Research (CBICR), Department of Precision\\nInstrument, Tsinghua University, Beijing, China\\nr_zhao@tsinghua.edu.cn\\nAbstract. Neuro-symbolic technologies with vertical and horizontal\\napproaches are important for the development of Artiﬁcial General Intel-\\nligence (AGI). But most of the neuro-symbolic works aim at narrow AIproblems and do not have a guideline for AGI. The integration of the\\ntwo approaches could in principle provide a more holistic framework for\\nAGI research. To our best knowledge, such integration has not beenexplicitly reported yet. In this paper, we identify that vertical and hor-\\nizontal neuro-symbolic approaches have independent beneﬁts for inves-\\ntigating AGI problems. We then introduce a framework integrating thetwo approaches, make the ﬁrst step to implement it, and discuss future',\n",
       "  'updates. The version-one framework contains a central Spiking Reasoning\\nNetwork (SRN) and several peripheral perceptual modules. The SRN is aprogrammable spiking neural network that can do logical reasoning under\\ninstructions. The version-one framework is implemented on two visual\\nquery answering tasks to investigate the programmability of the SRN andto examine the feasibility of the framework. We also discuss the learnabil-\\nity, the biological plausibility, and the future development of the SRN.\\nKeywords: Artiﬁcial general intelligence\\n·Neuro-symbolic artiﬁcial\\nintelligence ·Spiking neural networks\\n1 Introduction\\nThe “hybrid approaches”1towards building an artiﬁcial general intelligence\\n(AGI) aim to combine diﬀerent artiﬁcial intelligence (AI) techniques to form a\\nsystem that has more abilities than the sum of its parts [ 3]. The neuro-symbolic\\napproach is one of the most promising hybrid approaches towards AGI, because',\n",
       "  'of at least two reasons. On one hand, rationality, especially thinking with formal\\nlogic, is believed to be one of the most important unique abilities of humans.Revealing how neural networks can implement logical introspection is a mile-\\nstone achievement for understanding general intelligence of humans and devel-\\noping AGI. On the other hand, neural and logical technologies show signiﬁcant\\n1https://cis.temple.edu/~pwang/AGI-Intro.html .\\nc⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2023\\nP. Hammer et al. (Eds.): AGI 2023, LNAI 13921, pp. 197–206, 2023.https://doi.org/10.1007/978-3-031-33469-6\\n_20',\n",
       "  '198 L. Li et al.\\ndiﬀerence on several aspects. When properly integrated, such as the way human\\nbrain does, potential synergistic eﬀects are expected, which is deﬁnitely helpfulfor developing AGI. However, guidelines to develop neuro-symbolic agents for\\nthese two goals are still missing. Here, we brieﬂy introduce the current neuro-\\nsymbolic approaches, link two of them to the AGI goals, and propose a frame-work to integrate the two approaches, hoping to achieve both of the goals in the\\nfuture.\\nIn a lecture of AAAI 2020\\n2Henry Kautz categorized current neuro-symbolic\\nworks into 5 categories:\\n–Symbolic −Ne u ro −Symbolic\\n–Symbolic [Ne u ro ]\\n–Ne u ro ;Symbolic\\n–Ne u ro ∪compile (Symbolic )\\n–Ne u ro Symbolic\\nThis categorization focuses on the methods but not the goals because neuro-\\nsymbolic technologies can potentially be used for several applications. Fromthis categorization We ﬁnd that the Ne u ro ;Symbolic approach is suitable',\n",
       "  'for investigating the synergistic eﬀects, and Ne u ro ∪compile (Symbolic )app-\\nroach is suitable for investigating how neural networks implement rationality.\\nNe u ro ;Symbolic approach directly connects neural networks with symbolic\\nsystems by designing an eﬀective interface to transmit information. Ne u ro ∪\\ncompile (Symbolic )approach uses neural networks to learn or conduct symbolic\\nreasoning.\\nThese two approaches are in nature resembling the “vertical” and “horizon-\\ntal” neuro-symbolic approaches suggested by Anton Kolonin [ 12], while indicate\\nbroader and clearer range of technologies. As the term “vertical” and “horizontal”\\napproaches are proposed in the context of AGI and are more vivid, we adoptthem and try to provide an acceptable working deﬁnition for each of them.\\n1.1 Vertical and Horizontal Neuro-Symbolic Approaches\\nAccording to [ 12], the horizontal neuro-symbolic approach provides a bijection\\nbetween the symbolic structure and the neural network, both of which can inter-',\n",
       "  'pret the accomplishment of a reasoning process. When learning under symbolic\\nor neural rules, the agent shows slow or fast thinking properties [ 11], suggesting\\nthat the slow thinking may be up to the learning method other than the represen-\\ntation of the model. Other works [ 13,17] also show the beneﬁts of the horizontal\\napproach for investigating the symbolic properties of neural networks. Accord-\\ni n gt o[ 15], the vertical neuro-symbolic approach highlights the importance of\\nsymbolic systems for higher-level cognitive processes. Some mainstream AI taskssupport this claim [ 14,19].\\nCombining Anton Kolonin’s insights for AGI with Henry Kautz’s categoriza-\\ntion of neuro-symbolic methods, we provide the following working deﬁnition:\\n2https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/19122 .',\n",
       "  'Vertical-Horizontal Integrated Neuro-Symbolic Framework 199\\nVertical Approach ( Ne u ro ;Symbolic )\\n– The goal: human-level performance ; e.g. solving versatile and complex\\ntasks in natural environments.\\n– The methodology: divide and conquer ; e.g. using neural modules for per-\\nception and symbolic modules for reasoning.\\nHorizontal Approach ( Ne u ro ∪compile (Symbolic ))\\n– The goal: think like humans ; e.g. achieving slow thinking with neural\\nnetworks.\\n– The methodology: multiple interpretations ; e.g. encoding a logical formula\\ninto a network.\\n2 The Hybrid Framework for AGI Research\\nFor holistically investigating AGI problems, we propose a neuro-symbolic frame-\\nwork that integrate both vertical and horizontal approaches. As shown in Fig. 1,\\nthe framework has two parts: lower-level perception and higher-level reasoning.\\nThe perception part of human cognition can not be symbolically interpreted, and',\n",
       "  'the computation of it is highly parallel, so the perception part is typically a pureneural module. The reasoning part should itself be a horizontal neuro-symbolic\\nmodule, because human reasoning has both symbolic and neuronal properties.\\nFig. 1. Human cognitive features and the possible implementations of the vertical-\\nhorizontal integrated neuro-symbolic framework.\\nWe choose deep learning techniques for the perception part because of their\\nfavorable performance. We choose spiking neural networks for the reasoning part\\nbecause previous works [ 16,17] indicate that spiking neural networks show both\\nsymbolic and neural features. Figure 1shows the planned steps.',\n",
       "  '200 L. Li et al.\\nFirstly, we adopt separately trained deep learning modules for perception and\\nunidirectionally send information to the reasoning part. We program a spikingneural network called spiking reasoning network (SRN) according to the informa-\\ntion from perception modules. For the next step, we may explore the top-down\\ninﬂuence from the reasoning part to the perception part, and the learnability ofSRN. In the long run, we hope to build a fully connected network that can be\\nholistically compared with human behavior and cognition, and this is the only\\nway to develop “strong AI” with consciousness because up to now humans arethe only widely accepted conscious agents, and trustful consciousness research\\nshould be done with “contrast analysis” [ 2].\\n3 Implementation on CLEVR\\nCLEVR [ 9] is a generated machine learning dataset for VQA. It contains 60,000',\n",
       "  'images, each image has 10 reasoning questions about the objects and theirproperties. The reasoning instructions include ﬁltering by properties, compar-\\ning properties, location relations, counting, and existing. To correctly answer\\nthe question, the agent should understand visual contents from an image, andthen reason on them according to the demand of the question. The reasoning\\nprocess also needs commonsense knowledge, such as “red is a color”. We build a\\nvisual reasoning agent under our “vertical-horizontal framework” achieving 100%accuracy with ground-truth intermediate information and 99.8% accuracy with\\nimperfect perception modules.\\n3.1 The Agent\\nA ss h o w ni nF i g . 2, there are six processes grouped into three steps for the agent\\nto conduct reasoning. The ﬁrst step is to encode scene information into theSRN. A separately trained Mask-RCNN [ 4] is adopted for parsing the image\\ninto symbolic scene representation. After that, the symbolic representation will',\n",
       "  'be programmed as neurons and connection weights into SRN. The second step\\nis to encode commonsense knowledge into the SRN. It is done by programming\\neither. The last step is to instruct the reasoning process of SRN. A separatelytrained LSTM [ 7] is adopted for parsing the questions into instructions, and\\nthen the instructions are translated into stimuli with spatiotemporal patterns.\\nModulated by the stimulation, the neural dynamics of SRN will accomplish thecorresponding reasoning process and provide the correct answer.\\n3.2 The Spiking Reasoning Network\\nThis section describes how to program and execute SRN according to the\\nextracted symbolic knowledge and instructions. While early works [ 16,17]p r o v e d\\nthat feedforward neural networks with IF neurons can execute any discrete func-tions and propositional logic, it is still under investigation how to use recur-\\nrent neural networks to ﬂexibly execute several functions, just as using a mini-',\n",
       "  'computer. We identify that the main challenge is how to reuse sub-networks as',\n",
       "  'Vertical-Horizontal Integrated Neuro-Symbolic Framework 201\\nFig. 2. The agent for CLEVR. (a–b–g) pathway encodes environmental visual infor-\\nmation into the SRN, corresponding to perceiving sensory information and storing itin working memory. (c–g) pathway encodes commonsense knowledge into the SRN,\\nindicating the retrieval of long-term memory into working memory. (d–e–f–g) path-\\nway translates the question into modulation signals for SRN, indicating the reasoningprocess in the working memory.\\nfunctional modules according to the instruction. We implement gating mech-\\nanisms on SRN to solve the problem. When the gate is closed, related sub-\\nnetworks are disentangled, and when the gate is open, information is transmit-\\nted. In this manner, SRN avoids information jamming and supports independentparallel functioning. We use integrate-and-ﬁre (IF) neurons to build the SRN.\\nAll neurons in the SRN share the same threshold parameter for ﬁring an action',\n",
       "  'potential and will reset membrane potential to 0 after ﬁring.\\nA ss h o w ni nF i g . 3, SRN has three types of neurons and two types of con-\\nnections. (a) Concept neurons represent scene concepts and store temporaryvariables. For example, the cube-copy neuron is needed because the instruction\\n“Equal-shape” compares two shapes of diﬀerent objects, requiring simultaneous\\nrepresentations of two sets of shapes. (b) Instruction neurons receive stimulias instructions and accordingly modulate the reasoning dynamics of SRN. An\\ninstruction neuron can also be a concept neuron. For example, the color neuron\\nis needed for the instruction “Query[color]”. When the external stimulus deac-tivates the color neuron, the red neuron will not be inhibited, and the obj-1\\nneuron will activate the red neuron showing its color. (c) Transmission neurons\\ndisentangle the aﬀerent and the eﬀerent neurons when deactivated, and transmitinformation when activated. For example, the obj-2-front neuron is needed to',\n",
       "  'execute the instruction “Front[obj]”. When obj-2 is the only object neuron keep-\\ning ﬁring, the obj-2-front neuron is at a high-level membrane potential and theobj-1-front neuron is at a low-level membrane potential. Then if the front neuron\\nis activated by an external stimulus, the sum of the inputs from the obj-2 and',\n",
       "  '202 L. Li et al.\\nfront neuron will exceed the threshold and cause obj-2-front to ﬁre, transmitting\\nthe action potential and ﬁring obj-1 and obj-3. (bc) The gating mechanismsare implemented by both instruction neurons and transmission neurons, with\\nthe integrate-and-ﬁre neural dynamics. (de) Scene connections and functional\\nconnections are corresponding to scene information and commonsense knowl-edge in Fig. 2. Connections specify the relations between concepts and deﬁne\\nthe reasoning functions.\\nFig. 3. SRN programming and execution. The red numbers in the picture indicate the\\nindices of objects. (a, b, c) Three types of neurons. (d, e) Two types of connections.\\nThe arrow-end edges indicate excitatory connections and the round-end edges indicate\\ninhibitory connections. (Color ﬁgure online)\\n4 Implementation on CLEVRER\\nCLEVRER [ 18] is also a generated dataset for VQA tasks, containing 20,000',\n",
       "  'videos. Compared to CLEVR, it expands the visual inputs from an image to avideo with 128 frames, requiring stronger visual understanding and more com-\\nplex reasoning.\\nWe expand the agent built on CLEVR to CLEVRER, to investigate the\\nuniversality of the framework and the method for building and executing SRN.\\nThe results on CLEVRER have been published in [ 20], but the process of the\\nexpansion from CLEVR to CLEVRER has not been discussed. We will focus on',\n",
       "  'Vertical-Horizontal Integrated Neuro-Symbolic Framework 203\\nthe new challenges brought by CLEVRER and the successful expansion enabled\\nby the gating mechanism.\\nFig. 4. Functional connections and gating mechanisms between and within “objects”\\nand “events” of SRN for CLEVRER. The arrow-end edges indicate excitatory connec-tions and the double-bar-end edges indicate inhibitory connections.\\nAs shown in Fig. 4,w h e ns h i f t i n gf r o ma ni m a g et oav i d e o ,s e v e r a lc o m -\\nplex multi-step functions are necessary such as sequentially dependent functions.\\nWe use similar gating mechanisms in CLEVR to disentangle those functions,enabling the sequential combination of functions. Moreover, we ﬁnd that when\\nthe functional complexity increases and there are more reusable sub-networks,\\nthe gating mechanism enables object-orientation programming (OOP) instead offunction-based process-oriented programming (POP). Because of the disentan-',\n",
       "  'glement feature of the gating mechanism, the sub-networks are well encapsulated\\ninto classes (“object” and “event”). Most of the functions can be easily attachedto certain classes because they usually share the same variables stored in the\\nsame connections. As a result, “object” and “event” are two template classes in\\nSRN for CLEVRER. New objects of a certain class can be easily created ordeleted when the visual contents change.\\n5 Results and Related Works\\nTable 1shows the results of our implementations and related works. The CLEVR\\ncolumn indicates the overall accuracy of the 90 types of open-ended questions.',\n",
       "  '204 L. Li et al.\\nThe CLEVRER column indicates the accuracy of descriptive questions, because\\nother 3 types of questions are not open-ended questions but multiple-choicequestions, which have diﬀerent random-draw baseline.\\nEach agent in Table 1, apart from the LSTM, adopts a pre-trained ResNet [ 5]\\nas visual feature extractor. Compared to them, the LSTM serves as a baselineto show that it is necessary to “vertically” adopt a neural module for high-\\nperformance in real environment.\\nIEP [10] is an advanced version of neural module networks [ 1]. It combines\\nneural network modules according to the inferred symbolic reasoning tree. With\\naccurate symbolic instructions, each neural modules should learn to conduct the\\ncorresponding reasoning operation on feature maps. However, as the results indi-cate, when the reasoning content becomes more diﬃcult, the modules fail to con-\\nverge to a good performance. MAC [ 8] is an end-to-end black box deep learning',\n",
       "  'agent, with designed structure to store control operations. This approach could\\npotentially be a “horizontal” neuro-symbolic method for understanding the rea-\\nsoning process of the network, as long as a proper decoder is developed to explic-itly generate the control sequence from the network. NSVQA and NSDR are both\\nvertical neuro-symbolic agents, adopting neural modules to extract features and\\nprograms to execute them. Our vertical-horizontal integrated implementationsadopt similar neural modules and achieve comparable or higher performance\\nbecause the spiking reasoning network has more robustness than program.\\nTable 1. Methodology and performance comparison.\\nAgents methodology CLEVR CLEVRER\\nLSTM – 46.8% –\\nIEP [ 10] Symbolic [Ne u ro ]96.9% 52.8%\\nMAC [ 8] black box 98.9% 85.6%\\nNSVQA [ 19]/NSDR [ 18]vertical 99.8% 88.1%\\nours vertical-horizontal 99.8% 91.7%\\n6 Discussion\\nFor developing the next version of our framework, we also investigate the learn-',\n",
       "  'ability and biological plausibility of the SRN, based on the current version. As for\\nthe CLEVR dataset, when programming the scene information into connectionsof the SRN, the change of connection weights could be interpreted as Hebbian\\nlearning [ 6]. For example, the bidirectional excitatory connection in Fig. 3(b)\\ncould be the result of Hebbian learning, because when attention focuses on acertain object, the object itself and its properties were likely to be extracted at\\nthe same time, resulting in the simultaneous ﬁring of the obj-1 neuron and the\\nred neuron. However, this training method can not be directly applied to everyconnection. For example, in CLEVRER tasks, the sequential relations between',\n",
       "  'Vertical-Horizontal Integrated Neuro-Symbolic Framework 205\\nevents can hardly be interpreted as a result of Hebbian learning, because an\\nevent only happens in a ﬂash and there is no reason for neurons of the previousevent to ﬁre at the same time.\\nThe source code of SRN on both CLEVR and CLEVRER can be found on\\nhttps://github.com/llk15/SRN .\\nAcknowledgements. This work was partly supported by the National Nature Science\\nFoundation of China (No. 61836004, No. 62088102); National Key Research and Devel-\\nopment Program of China (grant no. 2021ZD0200300). We thank Hao Zeng, Mingkun\\nXu and Weihao Zhang for helpful discussions.\\nReferences\\n1. Andreas, J., Rohrbach, M., Darrell, T., Klein, D.: Neural module networks. In:\\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition,pp. 39–48 (2016)\\n2. Baars, B.J.: A Cognitive Theory of Consciousness. Cambridge University Press,\\nCambridge (1993)\\n3. Brachman, R.J.: AI—more than the sum of its parts. AI Mag. 27(4), 16 (2006)',\n",
       "  '4. He, K., Gkioxari, G., Dollar, P., Girshick, R.: Mask R-CNN. IEEE Trans. Pattern\\nAnal. Mach. Intell. 42(2), 386–397 (2020). https://doi.org/10.1109/tpami.2018.\\n2844175\\n5. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In:\\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\\npp. 770–778 (2016)\\n6. Hebb, D.O.: The Organization of Behavior: A Neuropsychological Theory. Psy-\\nchology Press (2005)\\n7. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural Comput. 9(8),\\n1735–1780 (1997). https://doi.org/10.1162/neco.1997.9.8.1735\\n8. Hudson, D.A., Manning, C.D.: Compositional attention networks for machine rea-\\nsoning. In: International Conference on Learning Representations (2018)\\n9. Johnson, J., Hariharan, B., van der Maaten, L., Fei-Fei, L., Zitnick, C.L., Girshick,\\nR.: CLEVR: a diagnostic dataset for compositional language and elementary visual',\n",
       "  'reasoning. In: 2017 IEEE Conference on Computer Vision and Pattern Recognition\\n(CVPR), pp. 1988–1997 (2017). https://doi.org/10.1109/CVPR.2017.215\\n10. Johnson, J., et al.: Inferring and executing programs for visual reasoning. In: Pro-\\nceedings of the IEEE International Conference on Computer Vision, pp. 2989–2998\\n(2017)\\n11. Kahneman, D.: Thinking, Fast and Slow. Farrar Straus & Giroux (2017)\\n12. Kolonin, Anton: Neuro-symbolic architecture for experiential learning in discrete\\nand functional environments. In: Goertzel, Ben, Iklé, Matthew, Potapov, Alexey(eds.) AGI 2021. LNCS (LNAI), vol. 13154, pp. 106–115. Springer, Cham (2022).\\nhttps://doi.org/10.1007/978-3-030-93758-4_12\\n13. Lample, G., Charton, F.: Deep learning for symbolic mathematics. In: International\\nConference on Learning Representations (2020)\\n14. Mao, J., Gan, C., Kohli, P., Tenenbaum, J.B., Wu, J.: The neuro-symbolic concept',\n",
       "  'learner: interpreting scenes, words, and sentences from natural supervision. In:International Conference on Learning Representations (2018)',\n",
       "  '206 L. Li et al.\\n15. Marcus, G.: The next decade in AI: four steps towards robust artiﬁcial intelligence\\n(2020). https://doi.org/10.48550/arXiv.2002.06177\\n16. McCulloch, W.S., Pitts, W.: A logical calculus of the ideas immanent in ner-\\nvous activity. Bull. Math. Biophys. 5(4), 115–133 (1943). https://doi.org/10.1007/\\nBF02478259\\n17. Towell, G.G., Shavlik, J.W.: Knowledge-based artiﬁcial neural networks. Artif.\\nIntell. 70(1), 119–165 (1994). https://doi.org/10.1016/0004-3702(94)90105-8\\n18. Yi, K., et al.: CLEVRER: collision events for video representation and reasoning.\\nIn: International Conference on Learning Representations (2019)\\n19. Yi, K., Wu, J., Gan, C., Torralba, A., Kohli, P., Tenenbaum, J.: Neural-\\nsymbolic VQA: disentangling reasoning from vision and language understanding.In: Advances in Neural Information Processing Systems, vol. 31. Curran Associates,\\nInc. (2018)\\n20. Zhao, R., et al.: A framework for the general design and computation of hybrid neu-',\n",
       "  'ral networks. Nat. Commun. 13(1), 3427 (2022). https://doi.org/10.1038/s41467-\\n022-30964-7',\n",
       "  'Rethinking the Physical Symbol Systems\\nHypothesis\\nPaul S. Rosenbloom(B)\\nDepartment of Computer Science, University of Southern California, Los Angeles, CA 90089,\\nUSA\\nrosenbloom@usc.edu\\nAbstract. It is now more than a half-century since the Physical Symbol Systems\\nHypothesis (PSSH ) was ﬁrst articulated as an empirical hypothesis. More recent\\nevidence from work with neural networks and cognitive architectures has weak-\\nened it, but it has not yet been replaced in any satisfactory manner. Based on a\\nrethinking of the nature of computational symbols – as atoms orplaceholders –a n d\\nthus also of the systems in which they participate, a hybrid approach is introducedthat responds to these challenges while also helping to bridge the gap between\\nsymbolic and neural approaches, resulting in two new hypotheses, one that is to\\nreplace the PSSH and the other focused more directly on cognitive architectures.\\nKeywords: Physical Symbol Systems ·Hybrid Symbol Systems ·Cognitive',\n",
       "  'Architectures ·Neural Networks\\n1 Introduction\\nOur current understanding of the role of physical symbol systems in artiﬁcial intelligence\\n(AI) is grounded in the pioneering work of Newell and Simon [ 1–3], although as they\\npoint out the roots go back much further in philosophy – most notably in logic – computer\\nscience, linguistics, literature, and the arts. Such systems, and their culmination in the\\nPhysical Symbol Systems Hypothesis (PSSH ) are reviewed in Sect. 2.\\nMany critiques of the PSSH have been proposed since it was ﬁrst introduced, with\\nsome that have easily been refuted and others that have lingered (Sect. 3). Here, two are\\ntaken up that have remained compelling, before hybrid symbol systems of a particular\\nsort are explored as a response to them (Sect. 4). As part of this, the notion of symbol\\nsystems is rethought, starting with a variant deﬁnition of what it means to be a compu-\\ntational symbol that is grounded in the Common Model of Cognition (CMC) [ 4] and',\n",
       "  'the Sigma cognitive architecture [ 5]. Two new hybrid hypotheses result, one that offers\\nan alternative to the PSSH and the other that focuses more speciﬁcally on cognitive\\narchitectures.\\nDemonstrating that neural networks are themselves hybrid symbol systems of this\\nsort (Sect. 5), rather than being limited to the numeric component of a coarse-grained\\ncombination of symbolic and numeric processing, helps to bridge the gap between sym-\\nbolic and neural approaches while enabling recent successes with neural networks to\\n© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023\\nP . Hammer et al. (Eds.): AGI 2023, LNAI 13921, pp. 207–216, 2023.https://doi.org/10.1007/978-3-031-33469-6\\n_21',\n",
       "  '208 P . S. Rosenbloom\\nbe weighed in a positive manner in evaluating hypotheses concerning symbol systems,\\nrather than the former necessarily serving as a challenge to the latter.\\nThe overall result, as discussed further in Sect. 6, is a novel way of thinking about\\nsymbol systems and the fundamental hypotheses concerning them; the introduction of aparticular form of hybrid symbol system and the appropriate hypotheses concerning it;\\nand an understanding of how neural networks are examples, rather than counterexamples,\\nof this form of symbol system. The hope is that this all helps cut the Gordian Knot thathas resulted from past discussions on these topics.\\nProposing hybrid or neuro-symbolic systems is certainly nothing new. Many\\napproaches have already been investigated – see, e.g., [ 6] and [ 7] for overviews, and\\n[8] for an earlier discussion of the PSSH and the relevance of hybrid systems. But the',\n",
       "  'point here is to introduce a particular take on hybrid symbol systems that is in service of\\nan appropriate rethinking of the Physical Symbol Systems Hypothesis. The approach isbroader than neuro-symbolic, as it also includes hybrid systems that span other numeric\\nparadigms, such as probabilities. In addition, it spans both tightly coupled and loosely\\ncoupled approaches to combining symbolic and numeric processing.\\n2 Physical Symbol Systems\\nAccording to the traditional view, symbols are distinct patterns in the physical world\\nthat can be composed into expressions ,o rsymbol structures .Processes are then deﬁned\\non these symbol structures that can create, modify, reproduce, and destroy them. An\\nexpression designates an entity, whether internal or external, if the expression’s use\\ndepends on the nature of the entity. An expression is interpreted if it designates an\\ninternal procedure that is then executed. The physicality of such symbol systems reﬂects',\n",
       "  'that they are natural , in obeying the laws of physics and being amenable to engineering;\\nand that they aren’t limited to what is in human minds, or even necessarily based on thesame kinds of symbols that have traditionally been imputed to humans.\\nGiven composition, designation, and interpretation, along with the appropriate pro-\\ncesses, physical symbol systems provide a form of universal computation. There arecertainly more details in the various papers, but this provides the essence of what can\\nnow be considered the classical notion of a physical symbol system.\\nThe Physical Symbol Systems Hypothesis (PSSH) then states that:\\nA physical symbol system has the necessary and sufﬁcient means for general\\nintelligent action.\\nThis hypothesis was introduced as an empirical generalization rather than a theorem.\\nEvidence for sufﬁciency stemmed from the universality of symbol systems and the',\n",
       "  'success of such systems built as of then. Evidence for necessity stemmed from noting thatthe one natural system exhibiting such intelligent behavior – that is, humans – appeared\\nto be such a system, and from the lack of alternative approaches that were nearly as\\nsuccessful. Newell, for example mentions that “These advances far outstrip what has\\nbeen accomplished by other attempts to build intelligent mechanisms, such as the work\\nin building robots driven directly by circuits; the work in neural nets, or the engineeringattempts at pattern recognition using direct circuitry and analogue computation.” [ 3].',\n",
       "  'Rethinking the Physical Symbol Systems Hypothesis 209\\nHe went on to state that “In my own view this hypothesis sets the terms on which\\nwe search for a scientiﬁc theory of mind.” and “The physical symbol system is to our\\nenterprise what the theory of evolution is to all biology, the cell doctrine to cellular\\nbiology, the notion of germs to the scientiﬁc concept of disease, the notion of tectonicplates to structural geology.”\\n3 Critiquing the Physical Symbol Systems Hypothesis\\nIt has now been over ﬁfty years since the PSSH was ﬁrst articulated, with numerouscritiques and defenses occurring in the intervening years. Nilsson [ 8], e.g., lists four\\ngeneral types of critiques with his responses to them (in italics here), which in brief are:\\n1. Lack of embodiment/grounding.\\nThis is a misunderstanding as the PSSH already includes this.\\n2. Non-symbolic/analog processing.\\nInclude numbers; that is, make the systems hybrid.\\n3. Brain-style versus computation-style (i.e., brains are not computers).',\n",
       "  'The brain is computational.\\n4. The mindlessness of much of what appears to be intelligent behavior.\\nMindless constructs only yield mindless behavior.\\nIn this section two particular critiques are considered, based on new empirical evi-\\ndence in the form of the recent successes of deep learning [ 9], and to a lesser extent prob-\\nabilistic graphical models (PGMs) [ 10], plus work on the CMC. One critique, aligned\\nwith Nilsson’s second, challenges its sufﬁciency and the other its necessity.\\nThe sufﬁciency challenge focuses on the lack of numeric processing – i.e., calcu-\\nlations on quantities – in the PSSH. Nilsson’s response is to shift to hybrid systems\\nthat include both symbols and numbers. In a sense, this isn’t logically necessary, asthe universality of symbol systems implies that, as with any modern digital computer,\\nthey can implement algorithms for numeric processing. However, universality is weaker',\n",
       "  'than what was originally proposed, as it omits grounding sufﬁciency in the successesof existing symbolic AI systems. Given the range of general intelligent action that has\\nbeen shown to proceed more effectively with numeric processing, whether in the form of\\nprobabilities or activations, the success of purely symbolic systems no longer providescompelling empirical evidence itself for the sufﬁciency of symbols on their own.\\nThus, we are left with a weakened form of sufﬁciency for the PSSH, based solely on\\nuniversality. Hybrid systems have the potential to restore the stronger sense of sufﬁciency(Sect. 4). They also support a more stringent sufﬁciency hypothesis that arises when the\\nconcern is more particularly with cognitive architectures [ 11]; that is, models of the ﬁxed\\nstructures and processes that yield a mind [ 12].\\nThe necessity challenge is rooted directly in how neural networks now provide a\\nbetter approach for many problems related to intelligent action. Successes with PGMs',\n",
       "  'can be considered here as well, although they are already hybrid systems that add proba-\\nbilities to classical symbol systems, particularly in their most general form as statistical\\nrelational systems [ 13], so they do not directly challenge the necessity of physical sym-\\nbol systems. In contrast, deep learning has the potential to provide an alternative that',\n",
       "  '210 P . S. Rosenbloom\\ncompletely overturns the necessity argument. In Sect. 5, this challenge is approached via\\na demonstration that, given the rethinking of symbol systems in Sect. 4, neural networks\\nare themselves instances of hybrid symbol systems. This approach avoids the need to\\nresolve the contentious question of whether or not neural networks have or need tradi-tional symbols, a question that appears unresolvable, at least to me, without additional\\nevidence.\\n4 Rethinking Symbol Systems\\nThis section leverages the four-step methodology of essential analysis [14] to yield a\\nfresh understanding of symbols and symbol systems: (1) strip out many of the elabo-\\nrations that are normally part of a topic’s deﬁnition, and which are often a source ofdissonance among researchers and communities, to yield its essence ; (2) use what has\\nbeen stripped out, and possibly more, in specifying a deﬁnitional space of variations on',\n",
       "  'the topic; (3) populate this space with exemplars that ﬂesh it out; and (4) derive novel\\nimplications from the results of the ﬁrst three steps. Step three is downplayed here due\\nto lack of space, while step four introduces two new hybrid symbol systems hypotheses.\\nThe focus here is in particular on the notion of symbol as it is used computationally\\nrather than as it is used in the humanities and arts. For example, [ 15] deﬁnes a symbol\\nas “something used for or regarded as representing something else; a material objectrepresenting something, often something immaterial; emblem, token, or sign.” This\\nfocuses on an abstract notion of designation oraboutness , which has elsewhere been\\nconsidered an important part of the essence of a theory [ 14]. Computationally, the essence\\nof a symbol is proposed to be an atom that is: (1) indecomposable into other atoms; and\\n(2)distinct from other atoms. McDermott informally introduced the notion of a symbol',\n",
       "  'as aplaceholder [16]. Although yielding different connotations, this notion is compatible\\nwith that of an atom here.\\nThis essence retains the classical notion of a computational symbol being a primitive\\nelement that can be distinguished from other such elements but eschews the need forboth physicality and symbols being structured as patterns. There were good reasons at\\nthe time to emphasize physicality – to counter both Cartesian dualism and the notion\\nthat only humans could use symbols – but these battles have already been won, at leastin my judgement, so this explicit emphasis on physicality is now dispensable.\\nPattern comparison is one way to determine whether two atoms are distinct. Yet, such\\na notion need not be deﬁnitional if it is just used to compare symbols. If symbols are\\nconsidered as types (rather than tokens ) – a notion implicit in the traditional deﬁnition –\\npatterns are simply intensional deﬁnitions of symbols. An extensional alternative deﬁnes',\n",
       "  'each symbol in terms of a set of tokens, with each token in a set considered to be indistinct\\nfrom other tokens in the same set and distinct from tokens in other sets.\\nThe classical notion of symbol also includes composability – into symbol structures\\nor expressions – designation , and interpretation . The ﬁrst of these is effectively assumed\\nto be part of the very nature of symbols, whereas the latter two are additional proper-\\nties necessary to enable the classical form of physical symbol systems. The essentialdeﬁnition of a symbol introduced here includes none of these three notions; that is, all\\nare optional. Therefore, any system that includes even these minimal, atomic forms of\\nsymbols can be considered a symbol system of some sort.',\n",
       "  'Rethinking the Physical Symbol Systems Hypothesis 211\\nFig. 1. Optional properties of\\nsymbols.Figure 1structures these optional properties, plus a\\nbit more, into a small tree. According to this perspec-\\ntive, a symbol may be composable into expressions\\n(aka symbol structures). It may also designate ; that is,\\nstand in for something else. A designation is procedu-\\nralif it is about a process. This is the classical notion\\nof interpretation, when combined with the ability toexecute the designated process. A procedural symbol,\\naccording to this deﬁnition, designates a process rather\\nthan being part of the process itself. If the process isitself a symbol structure it will contain symbols, but\\nthey themselves may be of any type. A designation is\\ndeclarative if it is about an object – essentially any-\\nthing other than a process – which may be internal to the system or external to it, with\\nthe latter relating to grounding. This corresponds to the classical notion of designation',\n",
       "  'when contrasted with interpretation. Beyond this difference in what is designated, thereis no intent to impute any other aspects of the classical procedural versus declarative\\ndistinction here.\\nSymbols in a classical symbol system support all of these properties, enabling them\\nto exhibit computational universality. Whether systems in which some or all of the\\nsymbols lack some of these properties provide anything like universal computation\\nwould necessarily depend on the details of the individual systems.\\nFig. 2. Optional properties of\\nsymbols from Fig. 1, extended with\\nhybrid.The CMC, an attempt at developing a consensus\\non what is needed for human-like cognition – i.e.,\\nhuman cognition and similar forms of artiﬁcial cog-\\nnition – took a step towards such an essence by drop-ping the necessity of designation, and thus also of\\ninterpretation, stripping symbols down to primitive\\nelements that only support composability into sym-bol structures. Although designation somewhere in a',\n",
       "  'system seems necessary for it to be either meaningful\\nor operational, it is not necessary for all symbols.\\nThe CMC also associated quantitative metadata\\nwith such symbols and structures – which provide\\nthedata – to modulate how they are processed. Such\\ncombinations can be considered as hybrid symbols\\norstructures .\\n1Considering hybrid of this sort as a third optional property of symbols\\nleads to Fig. 2.\\nThe CMC went on to argue for a different form of weakening of the sufﬁciency\\naspect of the PSSH. While still agreeing that classical symbol systems, as universalcomputational systems, are sufﬁcient in principle for intelligent behavior, it denied that\\nthey are sufﬁcient when time scales are relevant, such as in cognitive architectures.\\nIn particular, if statistical processing must occur on the same time scale as symbolic\\n1The CMC also allows numeric data, consideration of which is beyond the scope of this paper.',\n",
       "  '212 P . S. Rosenbloom\\nprocessing in such an architecture, then implementing the former in terms of the latter –\\nas the universality argument for sufﬁciency implies – is insufﬁcient. Thus, the CMC\\nimplies the need for numeric and symbolic processing on the same time scale. The\\nSigma cognitive architecture [ 5] goes a step further by denying the need for all symbols\\nto support arbitrary forms of composition, thus implicitly yielding the essence made\\nexplicit here.\\nNow, given this explicit articulation of the essence of a symbol plus its tree of\\nvariations, the Hybrid Symbol Systems Hypothesis (HSSH ) can be stated as:\\nHybrid symbol systems are necessary and sufﬁcient for general intelligent action.\\nIf the sufﬁciency clause of the PSSH is valid then so must be the comparable clause\\nin the HSSH, at least for hybrid symbol systems that are universal. However, the HSSH\\nresponds to the PSSH sufﬁciency challenge by including numbers, as suggested in [ 8].',\n",
       "  'Necessity of the HSSH is not implied by the corresponding clause in the PSSH. Instead,\\nthe HSSH responds to the PSSH necessity challenge by coopting the successes of neuralnetworks (Sect. 5).\\nThe Hybrid Cognitive Architectures Hypothesis (HCAH ) then states:\\nHybrid symbol systems are necessary and sufﬁcient for cognitive architectures.\\nThis hypothesis is clearly related to the HSSH, but it matters in itself because the\\ncomparable hypothesis – perhaps called the Physical Cognitive Architectures Hypothesis\\n(PCAH ) – fails. Thus, the sufﬁciency side of the PCAH is invalid irrespective of what\\nmight be true with respect to necessity. As with the HSSH, sufﬁciency for the HCAHneed not hold for all hybrid symbol systems, but it must hold for at least some.\\nAs with the PSSH and the HSSH, the HCAH is an empirical generalization. Both',\n",
       "  'sides of the argument are now supported by the architectural successes of classicalsymbol systems, neural systems, and traditional hybrid systems such as PGMs. Both\\nsides are further bolstered by how the CMC itself is a hybrid symbol system.\\n5 Neural Networks as Hybrid Symbol Systems\\nFig. 3. Simple\\nnetwork for pairedassociates.What makes neural networks hybrid symbol systems, as deﬁned\\nhere, rather than simply the numeric component of a larger sys-tem that also includes a symbolic component, such as [ 17]? To\\nkeep things simple, the focus here is limited to standard feed-\\nforward neural networks, consisting of multiple layers of nodesand links, where nodes have activations, links connect pairs of\\nnodes across levels and have weights, and processing occurs by\\nmultiplying input activations along links by the links’ weightsand then nonlinearly transforming the sums of these weighted\\ninputs.\\nTo be a bit more speciﬁc, let’s assume a small network for',\n",
       "  'paired associates that maps an input word to an output word.\\nFigure 3exempliﬁes this via a completely connected network\\nwith 6-unit input and output layers – yielding a 6-dimensional vector of activations for',\n",
       "  'Rethinking the Physical Symbol Systems Hypothesis 213\\neach – and 2 intermediate layers, each with 3 units. Words map consistently to input and\\noutput vectors via encoding and decoding processes that are external to the network.\\nThese processes may be based on an arbitrary or random assignment of vectors to words\\nor some form of more sophisticated embedding process, such as in [ 18].\\nThe focus here, however, is on analyzing the forward processing in the network\\nitself to show how it amounts to a hybrid symbol system. It should be possible to extend\\nsuch an analysis to encoding and decoding processes, as well as to learning in neuralnetworks, but this simple example is sufﬁcient to establish the precedent.\\nFig. 4. Designation relationships among input vector, word\\nvector, letters, and word meaning. Dotted lines reﬂect internaldesignation and dashed lines external designation.First consider the nodes\\nin the input layer of the\\nnetwork, now shown at thetop of Fig. 4as locations',\n",
       "  'within a vector of nomi-\\nnal activations. Such nodescan be seen as hybrid sym-\\nbols – symbolic nodes (i.e.,\\nlocations) with activationsas their quantitative meta-\\ndata – that exhibit a lim-\\nited form of composability\\nin yielding the hybrid sym-\\nbol structure that is the input vector. In contrast to the traditional interpretation of dis-tributed representations – where nodes are subsymbolic, or microfeatures, with symbols\\nonly arising as patterns over these elements – here the individual nodes are themselves\\nhybrid symbols that do not themselves designate, with patterns arising as structures ofthese hybrid symbols.\\nThis hybrid symbol structure does then internally designate a word structure that has\\none location per letter (middle of Fig. 4). The metadata in the word structure is not shown\\nas it is irrelevant to this analysis. What does matter is that declarative symbols in this\\nword structure externally designate particular letters of the alphabet, in this case making',\n",
       "  'up Allen Newell’s ﬁrst name (bottom of Fig. 4). The word as whole then externally\\ndesignates its meaning, iconiﬁed to the right of Fig. 4via an image of him.\\nKey to this all working is that it isn’t just the data aspect of hybrid symbols and\\nstructures that can designate, but the entirety of the hybrid symbols and structures –including their metadata – that can do so, just as is traditionally assumed for vectors in\\ndistributed representations [ 19]. The word itself is epiphenomenal to the feedforward\\nnetwork processing here – only the hybrid symbol structure at the top of Fig. 4, as yielded\\nby encoding, actually participates. As put in [ 20], “the node labels in a Connectionist\\nmachine are not part of the causal structure of the machine.”\\nThe internal nodes in the network are also hybrid symbols but without declarative\\ndesignations, ﬁtting the intuition that there are no ﬁxed meanings inside the network.',\n",
       "  'Instead, internal nodes – and links – procedurally designate ﬁxed processes. Considerlinkλin Fig. 3, which points from node ν\\n1to node ν2. This link is a hybrid symbol\\nstructure composed from these two hybrid symbols, with a weight as its metadata. It\\nprocedurally designates a process that multiplies the activation arriving from ν1by this',\n",
       "  '214 P . S. Rosenbloom\\nweight. Internal nodes such as ν2then procedurally designate processes that sum all of\\ntheir inputs – in this case, from ν1and any other nodes linked to it from the proceeding\\nlayer – and then nonlinearly transform the results.\\nThe last part of the analysis concerns the output nodes. Perhaps surprisingly, they\\ntoo do not declaratively designate anything here. Instead, they procedurally designate\\nthe same summation and transformation process as the internal nodes. It is not until\\npostprocessing – that is during decoding – that this reverse mapping occurs.\\nThis analysis demonstrates that a feedforward neural network is a hybrid symbol\\nsystem, as deﬁned here. As such, it makes the case that the shift from the PSSH to the\\nHSSH enables coopting neural-network successes as evidence for both the sufﬁciencyand necessity of hybrid symbol systems rather than as counterexamples to them.\\nBut what type of hybrid symbol system does this type of neural network yield? It',\n",
       "  'provides limited forms of declarative designation (at input nodes), procedural designa-tion (at all but input nodes), and composition (via vectors within a level and links across\\nlevels). Yet, other forms of neural networks do go beyond this. To name just two com-\\nmon examples, both convolutional networks (e.g., [ 21]) and transformers [ 22] include\\nadditional forms of composition. The ﬂexibility of composition seen in the output of\\ntransformer-based generative networks [ 23] is in fact quite compelling. Some forms of\\nneural networks are also known to support universal computation (see, e.g., [ 24]). Yet\\nno neural network to date has solved combinatorial board games without the dynamic\\ncomposition yielded by explicit state-space search (as seen, e.g., in both [ 25] and [ 26]).\\nSo, the overall story is complex, dependent on the exact nature of the neural networks\\nconsidered, and still not completely understood.\\n6 Conclusion',\n",
       "  'Leveraging essential analysis, symbols are (re)deﬁned as atoms or placeholders, and a\\nspace of variations is deﬁned for symbols, symbol structures, and symbol systems. Thisincludes the classical traits of compositionality and designation, plus hybridness and\\nadditional sub-traits under designation (such as interpretation). In response to lingering\\nchallenges to the Physical Symbol System Hypothesis (PSSH), two new hypotheseshave then been introduced that focus on the resulting hybrid symbol systems:\\nHybrid Symbol Systems Hypothesis (HSSH):\\nHybrid symbol systems are necessary and sufﬁcient for general intelligent action.\\nHybrid Cognitive Architectures Hypothesis (HCAH):\\nHybrid symbol systems are necessary and sufﬁcient for cognitive architectures.\\nThe HSSH is intended as a replacement for the PSSH, based on evidence accumulated\\nsince the latter was introduced as an empirical hypothesis a half-century ago. Given this',\n",
       "  'recent body of evidence, there is a sense in which the PSSH still holds, but it is a weaker\\nsense. The HSSH recaptures the originally intended strength while adding further to it\\nby reinterpreting neural networks as compatriots – that is, as hybrid symbol systems',\n",
       "  'Rethinking the Physical Symbol Systems Hypothesis 215\\nthemselves – rather than as competitors. The result also helps chip away in a rather\\nﬁne-grained manner at the overall divide between symbolic and neural systems.\\nThe HCAH is a more stringent claim than either the original PSSH or the HSSH in\\nthat it concerns cognitive architectures rather than general intelligent action. Evidenceaccumulated over the past decades has shown that traditional physical symbol systems\\nfail with respect to sufﬁciency for cognitive architectures due to the need for numeric\\nprocessing within the architectures themselves. The necessity of classical physical sym-bol systems for cognitive architectures remains an open question, as it is not yet clear\\nwhether neural networks – which although as argued here are hybrid symbol systems\\nbut which may not be classical symbol systems or even universal computationally – willprove to be a sufﬁcient alternative on their own for such architectures.',\n",
       "  'One potential chink in the armor of both of these new hypotheses is the possibility\\nof quantum aspects to intelligence that cannot be captured even by hybrid systems [ 27].\\nShould it prove necessary, some thought is already being put into what it would mean\\nto have quantum symbol systems (e.g., [ 28]).\\nAcknowledgements. I would like to think John Laird, Christian Lebiere, and Andrea Stocco for\\nhelpful comments and discussions on this general topic and this particular paper.\\nReferences\\n1. Newell, A., Simon, H.A.: Human Problem Solving. Prentice-Hall, Englewood Cliffs (1972)\\n2. Newell, A., Simon, H.A.: Computer science as empirical inquiry: symbols and search. Comm.\\nACM 19(3), 113–126 (1972)\\n3. Newell, A.: Physical symbol systems. Cog. Sci. 4(2), 135–183 (1980)\\n4. Laird, J.E., Lebiere, C., Rosenbloom, P .S.: A standard model of the mind: toward a common\\ncomputational framework across artiﬁcial Intelligence, cognitive science, neuroscience, and\\nrobotics. AI Mag. 38(4), 13–26 (2017)',\n",
       "  '5. Rosenbloom, P .S., Demski, A., Ustun, V .: The Sigma cognitive architecture and system:\\ntowards functionally elegant grand uniﬁcation. J. Artif. Gen. Intell. 7(1), 1–103 (2016)\\n6. Kurfess, F.J.: Integrating symbol-oriented and sub-symbolic reasoning methods into hybrid\\nsystems. In: Apolloni, B., Kurfess, F. (eds.) From Synapses to Rules: Disc. Sym. Rules fromNeural Proc. Data, pp. 275–292. Kluwer, New Y ork (2002)\\n7. Bader, S., Hitzler, P .: Dimensions of neural-symbolic integration—a structured survey. In:\\nArtëmov, S.N., Barringer, H., d’Avila Garcez, A.S., Lamb, L.C., Woods, J. (eds.) We Will\\nShow Them! Essays in Honour of Dov Gabbay, pp. 167–194. Coll. Pubs., Rickmansworth(2005)\\n8. Nilsson, N.J.: The physical symbol system hypothesis: status and prospects. In: Lungarella,\\nM., Iida, F., Bongard, J., Pfeifer, R. (eds.) 50 Years of Artiﬁcial Intelligence. LNCS (LNAI),\\nvol. 4850, pp. 9–17. Springer, Heidelberg (2007). https://doi.org/10.1007/978-3-540-772\\n96-5_2',\n",
       "  '9. Goodfellow, I.J., Bengio, Y ., Courville, A.: Deep Learning. MIT Press, Cambridge (2016)\\n10. Koller, D., Friedman, N.: Probabilistic Graphical Models: Principles and Techniques. MIT\\nPress, Cambridge (2009)\\n11. Kotseruba, I., Tsotsos, J.K.: 40 years of cognitive architectures: core cognitive abilities and\\npractical applications. Artif. Intell. Rev. 53(1), 17–94 (2018). https://doi.org/10.1007/s10462-\\n018-9646-y',\n",
       "  '216 P . S. Rosenbloom\\n12. Rosenbloom, P .S.: Thoughts on architecture. In: Goertzel, B., Iklé, M., Potapov, A., Pono-\\nmaryov, D. (eds.) Artiﬁcial General Intelligence (AGI 2022). LNCS, vol. 13539, pp. 364–373.\\nSpringer, Cham (2023). https://doi.org/10.1007/978-3-031-19907-3_35\\n13. de Raedt, L., Kersting, K., Natarajan, S., Poole, D.: Statistical relational artiﬁcial intelligence:\\nlogic, probability, and computation. Synth. Lect. Artif. Intell. Mach. Learn. 10(2), 1–189\\n(2016)\\n14. Rosenbloom, P .S.: On theories and their implications for cognitive architectures (In prep.)\\n15. Dictionary.com on symbol. https://www.dictionary.com/browse/symbol . Accessed 15 Feb\\n2023\\n16. McDermott, D.V .: Mind and Mechanism. MIT Press, Cambridge (2001)\\n17. Sun, R.: The CLARION cognitive architecture: towards a comprehensive theory of the mind.\\nIn: Chipman, S. (ed.) The Oxford Handbook of Cognitive Science, pp. 117–133. OxfordUniversity Press, New Y ork (2017)',\n",
       "  '18. Mikolov, T., Sutskever, I., Chen, K., Corrado, G.S., Dean, J.: Distributed representations of\\nwords and phrases and their compositionality. Adv. Neural Inf. Process. Syst. 26, 3111–3119\\n(2013)\\n19. Hinton, G.E., McClelland, J.L., Rumelhart, D.E.: Distributed representations. In: McClel-\\nland, J.L., Rumelhart, D.E. (eds.) Parallel Distributed Processing: Explorations in theMicrostructure of Cognition, vol. 1, pp. 77–109. MIT Press, Cambridge (1986)\\n20. Fodor, J.A., Pylyshyn, Z.W.: Connectionism and cognitive architecture: a critical analysis.\\nCognition 28(1–2), 3–71 (1988)\\n21. LeCun, Y ., Bengio, Y .: Convolutional networks for images, speech, and time series. In: Arbib,\\nM. (ed.) The Handbook of Brain Theory and Neural Nets. MIT Press, Cambridge (1995)\\n22. V aswani, A., et al.: Attention is all you need. In: Proc. of the 31\\nstAnnual Conf. on Neural\\nInfo. Proc. Sys., pp. 5998–6008 (2017)\\n23. Brown, T.B., et al.: Language models are few-shot learners. In: Proc. of the 34thConf. on',\n",
       "  'Neural Info. Proc. Sys., pp. 1877–1901 (2020)\\n24. Siegelmann, H.T., Sontag, E.D.: Turing computation with neural nets. Appl. Math. Lett. 4(6),\\n77–80 (1991)\\n25. Tesauro, G.: Temporal difference learning and TD-Gammon. Comm. ACM 38(3), 58–68\\n(1995)\\n26. Silver, D., et al.: A general reinforcement learning algorithm that masters chess, shogi, and\\nGo through self-play. Science 362(6419), 1140–1144 (2018)\\n27. Penrose, R.: The Emperor’s New Mind: Concerning Computers, Minds, and The Laws of\\nPhysics. Oxford University Press, Oxford (1989)\\n28. Laskey, K.B.: Quantum physical symbol systems. J. Log. Lang. Inf. 15(1–2), 109–154 (2006)',\n",
       "  'On Relation Between Facial Expressions\\nand Emotions\\nAlexei V . Samsonovich(B), Alexandr Sidorov, and Alexandr Inozemtsev\\nNational Research Nuclear University MEPhI, Moscow, Russia\\nalexei.samsonovich@gmail.com\\nAbstract. Human face is used to express affects and feelings, either involuntary\\nor deliberately. How many dimensions of emotional ﬂavors can be robustly dis-\\ntinguished in facial expressions, across individuals and cultures? Here we offer\\nan answer and develop a practical approach to generate synthetic emotional facial\\nexpressions. Results can be used in studies of synthetic emotions.\\nKeywords: machine learning ·emotion modeling ·Facial Action Coding\\nSystem (FACS) ·face attribute analysis ·DeepFace\\n1 Introduction\\nFor humans, facial expression is the main modality of nonverbal emotional communica-\\ntion during face-to-face social contact. Facial expression is used to communicate affectsand feelings either naturally, or subliminally, or deliberately. It may or may not reﬂect the',\n",
       "  'actual emotional state of the individual; yet its function is to express a certain emotional\\nstate. It is an interesting fundamental question – how facial expressions are related toﬂavors of emotional states, what ﬂavors are expressible on face, and what aspects of the\\nfacial conﬁguration are interpretable in terms of emotional semantics. This topic has a\\nlong history of research [ 1,2], yet certain details remain unresolved. Among them is\\nthe dimensionality of the space of emotional ﬂavors, robustly expressible on face across\\nindividuals.\\nHere we use a vector-space approach to emotion representation [ 3–5]. A key question\\nis the dimensionality of the affective space. Most frequently, three-dimensional models\\nare used [ 3,5,6], known as V AD (valence, arousal, dominance), PAD (pleasure, arousal,\\ndominance), etc. On the other hand, complex, or social emotions may require an intro-duction of extra dimensions to distinguish them from basic emotions [ 7]. An alternative',\n",
       "  'approach is to describe social emotions as pairs or combinations of simple emotions [ 8],\\nwhich also amounts to an expansion of the basic affective space.\\nFor example, overlapping in the V AD space may be representations of jealousy and\\nanger, gloat and sarcasm, sense of humor and happiness, compassion and sadness, etc.\\nDo these pairs have different representations on face? And vice versa, how many different\\nfacial expressions may represent one and the same emotional ﬂavor? Obviously, more\\nthan one facial conﬁguration can correspond to one emotional state: for example, witha closed or open mouth, with a look to the left or to the right. Then, what would be the\\nmaximal dimension of the set of unique face-emotion pairs?\\n© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023\\nP . Hammer et al. (Eds.): AGI 2023, LNAI 13921, pp. 217–221, 2023.https://doi.org/10.1007/978-3-031-33469-6\\n_22',\n",
       "  '218 A. et al.\\n2 Mathematical Statement of the Problem\\nLet E be the vector space of all possible emotional states, and F the vector space of all\\npossible facial conﬁgurations that can be interpreted as expressions of emotional states.Call a smooth mapping f:E→F admissible, if it assigns to each emotional state some\\nconﬁguration of the face, which expresses this emotional state. Similarly, call a smooth\\nmapping g:F→E is admissible, if it assigns to each face conﬁguration from F some\\nemotional state that can be considered expressed by this face conﬁguration.\\nLet us give the spaces E and F the structure of ﬁber bundles with some common\\nbase X, deﬁned by the projections π\\n1andπ2in such a way that the diagrams (Fig. 1)\\ncommute for the entire set of admissible functions { f,g} for ﬁxed π1,π2. The task is to\\ndetermine the maximal possible dimension of X.\\nFig. 1. Statement of the problem in terms of ﬁber bundles.\\nGenerally speaking, the answer may depend on how strict the admissibility require-',\n",
       "  'ment for f,gis. For example, it can be assumed that the question of admissibility\\nis decided by questioning the subjects, and in the experiment the faces of people ofdifferent nationalities, as well as avatars, are used.\\n3 Statistical Solution\\nTo simplify the analysis, one can consider vector spaces instead of ﬁber bundles. Then\\nthe answer is found by computing the canonical correlation. This analysis was done\\nin our recent work [ 9]. We studied electromyographic recordings from the facial mus-\\ncles together with computer analysis of emotional facial expressions based on video\\nrecordings of the face, using principal component analysis, canonical correlation, ridge\\nregression, random forest, and random forest with the choice of hyperparameters usingcross-validation. Based on this analysis, we found that the dimension of X is equal to\\nthree. In other words, the V AD model is sufﬁcient to distinguish among all emotionally\\ndifferent facial expressions.',\n",
       "  'If this is the case, then any emotional state can be expressed on the face of an avatar,\\ncapable of representing all points of the V AD space. The next question is how to do it\\naccurately and automatically.',\n",
       "  'On Relation Between Facial Expressions and Emotions 219\\n4 Practical Approach\\nIn this study, an anthropomorphic avatar embedded in a 3D virtual environment was\\nused. The implementation was based on Unreal Engine 4. Facial expressions of theavatar are controlled by 17 parameters – MorphTargets (Fig. 2).\\nFig. 2. MorphTargets of the avatar.\\nUsing the approach of [ 10], a neural network model has been developed that generates\\nthe MorphTargets parameters based on the V AD values received as input. The training\\nof the network was done using DeepFace [ 11]. Results are shown in Fig. 3.\\n5 Conclusions\\nAlgorithms for analyzing and synthesizing human emotional states are an important and\\nintegral part of software and hardware systems that model human intellectual activity\\n[12]. Human emotions are often represented using a three-dimensional space with three',\n",
       "  'basis vectors: dominance, valence, arousal. However, the main system for coding facialexpressions of a human face is the FACS system [ 1], which contains several dozen\\nparameters. It is known that it is impossible to uniquely establish a correspondence\\nbetween the space of emotional states and the FACS system. Conclusions of this studyare the following.\\n(1) It is argued that a three-dimensional space, such as V AD, is sufﬁcient for indexing\\nall emotionally distinct facial expressions.\\n(2) A method is proposed that makes it possible to generate realistic facial expres-\\nsions on an anthropomorphic avatar’s face, based on the values of the three basiccoordinates. The developed method consists in using a neural network model that\\ngenerates codes of facial conﬁguration changes.',\n",
       "  '220 A. et al.\\n(3) The described approach, while not directly related to Artiﬁcial General Intelligence,\\nis biologically inspired and can be useful in studies of synthetic emotions.\\nFig. 3. Generated synthetic faces and their V AD (valence, arousal, dominance) coordinates: (a)\\nanger=[−0.51, 0.59, 0.25], (b) joy =[0.81, 0.51, 0.46], (c) surprise =[0.40, 0.67, −0.13], (d)\\ndisgust =[−0.60, 0.35, 0.11], (e) fear =[−0.64, 0.60, 0.43], (f) sadness =[−0.63,−0.27,−\\n0.33].\\nAcknowledgments. The authors are grateful to Aleksey Kevroletin, Mark Karavashkin, Georgy\\nV ayntrub, V era Mironova, Zhanna Demidova, Ismail M. Gadzhiev, Mikhail Knyshenko, and Sergei\\nDolenko for their contribution to this project. This work was supported by the Russian Science\\nFoundation Grant #22–11-00213, https://rscf.ru/en/project/22-11-00213/ .\\nReferences\\n1. Ekman, P ., Friesen, W.: Facial Action Coding System: A Technique for the Measurement of\\nFacial Movement. Consulting Psychologists Press, Palo Alto (1978)',\n",
       "  '2. Ekman, P .: An argument for basic emotions. Cogn. Emot. 6(3–4), 169–200 (1992)\\n3. Russell, J., Mehrabian, A.: Evidence for a three-factor theory of emotions. J. Res. Per. 11,\\n273–294 (1977)\\n4. Plutchik, R.: A psychoevolutionary theory of emotions. Soc. Sci. Inf. 21, 529–553 (1982)\\n5. Lövheim, H.: A new three-dimensional model for emotions and monoamine neurotransmit-\\nters. Med. Hypotheses 78(2), 341–348 (2012)\\n6. Russell, J.: Core affect and the psychological construction of emotion. Psychol. Rev. 110(1),\\n145–172 (2003)\\n7. Moors, A., Ellsworth, P .C., Scherer, K.R., Frijda, N.H.: Appraisal theories of emotion: state\\nof the art and future development. Emot. Rev. 5(2), 119–124 (2013)\\n8. 8. Lieto, A., Pozzato, G.L., Striani, M., Zoia, S., Damiano, R.: DEGARI 2.0: a diversity-\\nseeking, explainable, and affective art recommender for social inclusion. Cogn. Syst. Res. 77,\\n1–17 (2023). https://doi.org/10.1016/j.cogsys.2022.10.001',\n",
       "  '9. Gadzhiev, I.M., Knyshenko, M.P ., Dolenko, S.A., Samsonovich, A.V .: Inherent dimension of\\nthe affective space: Analysis using electromyography and machine learning. Cogn. Syst. Res.\\n78, 96–105 (2023)\\n10. Li, R., et al.: Learning formation of physically-based face attributes. ArXiv, 2004.03458\\n(2020). https://doi.org/10.48550/ARXIV .2004.03458\\n11. Serengil, S.I., Ozpinar, A.: HyperExtended LightFace: a facial attribute analysis framework.\\nIn: 2021 International Conference on Engineering and Emerging Technologies (ICEET),\\nIstanbul, Turkey, pp. 1–4 (2021). https://doi.org/10.1109/ICEET53442.2021.9659697',\n",
       "  'On Relation Between Facial Expressions and Emotions 221\\n12. Marsella, S., Gratch, J., Petta, P .: Computational models of emotion. In: Scherer, K.R.,\\nBanziger, T., Roesch, E. (Eds.), A Blueprint for Affective Computing: A Sourcebook and\\nManual. Oxford University Press, Oxford (2010)',\n",
       "  'Evaluation of Pretrained Large Language\\nModels in Embodied Planning Tasks\\nChristina Sarkisyan1, Alexandr Korchemnyi1, Alexey K. Kovalev2(B),\\nand Aleksandr I. Panov2,3\\n1Moscow Institute of Physics and Technology, Dolgoprudny, Russia\\n2AIRI, Moscow, Russia\\n{kovalev,panov }@airi.net\\n3Federal Research Center “Computer Science and Control” of the Russian Academy\\nof Sciences, Moscow, Russia\\nAbstract. Modern pretrained large language models (LLMs) are\\nincreasingly being used in zero-shot or few-shot learning modes. Recentyears have seen increased interest in applying such models to embod-\\nied artiﬁcial intelligence and robotics tasks. When given in a natural\\nlanguage, the agent needs to build a plan based on this prompt. Thebest solutions use LLMs through APIs or models that are not publicly\\navailable, making it diﬃcult to reproduce the results. In this paper, we',\n",
       "  'use publicly available LLMs to build a plan for an embodied agent andevaluate them in three modes of operation: 1) the subtask evaluation\\nmode, 2) the full autoregressive plan generation, and 3) the step-by-step\\nautoregressive plan generation. We used two prompt settings: prompt-containing examples of one given task and a mixed prompt with examples\\nof diﬀerent tasks. Through extensive experiments, we have shown that\\nthe subtask evaluation mode, in most cases, outperforms others with atask-speciﬁc prompt, whereas the step-by-step autoregressive plan gen-\\neration posts better performance in the mixed prompt setting.\\nKeywords: Large language models\\n·Plan generation ·Planning for\\nembodied agents\\n1 Introduction\\nThe large language models (LLMs) pretrained on a huge corpus of texts are able\\nto solve problems that were not trained in the mode of few-shot [ 4] and zero-\\nshot [26] learning. Some modern LLMs demonstrate high eﬃciency on a variety of',\n",
       "  'diﬀerent tasks, including those whose examples were not included in the trainingdataset. Such models provide a good approximation to the linguistic world model\\nand serve as an important part of the AGI systems. In many ways, eﬀectiveness\\nin the multi-task setting is achieved by using such models as knowledge basesand generating the required model output by demonstrating examples in the\\nmodel’s input prompt. As a result, building the right prompt has turned into a\\nseparate area of applied data science, known as prompt engineering.\\nc⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2023\\nP. Hammer et al. (Eds.): AGI 2023, LNAI 13921, pp. 222–232, 2023.https://doi.org/10.1007/978-3-031-33469-6\\n_23',\n",
       "  'Evaluation of Pretrained Large Language Models 223\\nInitially, pretrained LLMs were mainly used in natural language processing\\ntasks. But recently, they have found application for tasks of embodied artiﬁcialintelligence (AI) and robotics [ 1,6,8,9,14,21,22,24]. LLMs are used in cases when\\nthe task for the agent is formed in a natural language, for example, “bring a cup\\nof coﬀee.” Then an agent has to generate a plan comprising subtasks leading tothe execution of the task [ 11]. In general, this plan can be expressed in a natural\\nlanguage, for example, “1. Find a cup. 2. Take a cup. 3. Put the cup into the coﬀee\\nmachine. 4. Switch on the coﬀee machine. 5. Bring a cup of coﬀee to the user,” andsubsequently mapped onto the actions available to the agent. Recent works show\\ngood results in building a plan by LLMs, using feedback from the environment [ 1,\\n21,22,24] and without it [ 9]. However, the best solutions use LLMs where access is',\n",
       "  'either limited by API tools (GPT-3 [ 4], ChatGPT [ 17]) or unavailable (PaLM [ 5],\\nFLAN [ 26]), which aﬀects the reproducibility of the result and the possibility of\\nfurther research in this direction. Typically, such models have more parameters\\nand are trained on more data than publicly available models.\\nOn the one hand, this allows us to achieve better results by improving the\\nlanguage model itself and through the emergence of some additional properties.\\nFor example, in [ 27], authors show that using a “chain of thought” prompt brings\\na better boost for models with over 100 billion parameters than for modelswith fewer parameters. On the other hand, when it comes to embodied AI and\\nrobotics, the use of such language models even in the inference mode on board\\nthe agent causes diﬃculties because of the required computing resources.\\nIn our work, we explore the use of publicly available pretrained LLMs to gen-',\n",
       "  'erate a plan from a language description of a task, in three modes of operation:\\n1) the subtask evaluation mode, 2) the full autoregressive plan generation, and 3)the step-by-step autoregressive plan generation. Through extensive experiments,\\nwe have shown that the use of LLMs, even with a relatively small number of\\nparameters, allows the agent to generate plans with high accuracy. In our exper-iments, the subtask evaluation mode, in most cases, outperforms others with\\na task-speciﬁc prompt, whereas the step-by-step autoregressive plan generation\\nposts better performance in the mixed prompt setting.\\n2 Metrics to Evaluate the Generated Plan\\nMeasuring the quality of generated plans is a challenging task due to the mul-timodal nature of the problem and the ambiguity of the natural language sincewe need to evaluate not only the agent’s natural language understanding but\\nthe ability to ground the steps of the plan with the admissible actions and',\n",
       "  'the visual information from the environment. It is common to evaluate plansin terms of their Executability and Correctness [ 8,9,21]. Executability implies\\nchecking that each step of the plan is syntactically correct, i.e., contains valid\\nactions and objects and can be executed by the agent in the environment. Cor-\\nrectness is an assessment of whether the execution of the plan’s steps leads to\\nthe achievement of the target environment state. As noted in [ 9], it is diﬃcult to\\njudge correctness based on a single gold standard measurement, since one task\\ncan have several correct plan solutions, so various metrics are used. Longest',\n",
       "  '224 C. Sarkisyan et al.\\nCommon Subsequence (LCS) [ 8,9] evaluates the intersection share of the gen-\\nerated plan with the ground truth (GT). In cases where the plan is directlyexecuted in the environment, Success Rate and Goal Conditions Recall metrics\\nare used [ 20,21], which is the full or partial achievement of all task-relevant goal-\\nconditions, respectively. In [ 12], the authors propose to evaluate plans step by\\nstep using CIDEr [ 23] and SPICE [ 2] scene graph-based metrics and design the\\nKeyActionScore (KAS) metric for the ALFRED [ 20] extract key actions for task\\ncompletion. Graph of the scene’s ﬁnal state where nodes represent objects withedges as relationships between them is used for correctness calculation in [ 8]. To\\ndate, there is no single generally accepted metric for this task. Human assessors\\ncan also be involved in the evaluation process, as in [ 1,9]. In our work, we use\\nthree metrics that evaluate the similarity of the generated plan and the GT plan',\n",
       "  'in terms of various criteria and present the results of human assessment to obtain\\na detailed analysis of the methods’ performance. A more detailed description of\\nthe evaluation process is provided in Sect. 3.4.\\n3M e t h o d\\n3.1 Problem Formulation\\nGiven a task description τ, the pretrained LLM is to build a task execution\\nplanˆS, which is a sequence of subtasks ˆS=(s1,...,s n). A subtask stis a pair\\n(at,ot),at∈A,ot∈O, where atis the action available for the agent to perform\\nand involves interacting with the target object ot.\\n3.2 Subtask Evaluation Mode\\nTo build a plan in subtask evaluation mode, we use the LLM as a scoring model,\\nas proposed in SayCan [ 1], which selects a potential text completion from a\\nlimited set of options rather than generating the text directly. The execution\\nplan is built iteratively: at each step, the LLM represents a distribution over the\\nset of subtask language descriptions L. The mapping φmatches each possible',\n",
       "  'subtask stwith a corresponding text description φ:A×O→L. Speciﬁcally, for\\neach action a∈Ain the robotic language, there is a template with its description\\nin the natural language into which the object o∈Ois inserted (e.g. subtask\\n“(PickupObject, {obj})”goes to the description “pick up the {obj}”). The LLM\\nscores all the natural language subtasks l∈L, computing their probabilities and\\nselecting the optimal subtask:\\nl∗\\nt= argmaxlt∈LP(lt|f(τ,l1,...,l t−1)),t=1,n. (1)\\nf(x) is a prompting function, that maps the input xto a task-speciﬁc template.\\nThe subtasks obtained in the previous steps of the algorithm are sequentially\\nattached to the input data of the prompt function, which adds them to the string\\nprompt according to the template. The resulting language subtask sequence(l\\n1,...,l n) can be further easily transformed into an executable plan using φ−1\\nback mapping: st=φ−1(lt).',\n",
       "  'Evaluation of Pretrained Large Language Models 225\\nThis approach ensures the executability of each subtask in the plan since we\\nconstrain the model output to speciﬁc tokens corresponding to the environmentalsubtasks. However, it requires multiple prompt passes through the model for\\neach subtask at each step of the algorithm, which lengthens the plan generation\\ntime and is resource-intensive. The computational complexity grows with thenumber of available actions and objects in the environment. To speed up the plan\\ngeneration procedure, we additionally reduce the spatial dimension of subtasks\\nusing a set of rules imposed by environmental constraints and common senseconsiderations. At each plan generation step, the set of subtasks Lis ﬁltered by\\nthe reducing function:\\nL\\n′=reduce (L) (2)\\nIn particular, the model is proposed to evaluate only the subtasks, meeting the\\nrules designed for the ALFRED [ 20] environment. For example, the rules take',\n",
       "  'into account the fact that the agent has only one manipulator and can not pickup two objects at once, so if the previously selected subtask has the “pick up”\\naction, other subtasks with the same action are discarded. Also, the subtasks\\nshould not contain objects that the agent in the scene does not observe.\\nWe found that discarding some unnecessary subtasks from the evaluation\\nprocess can signiﬁcantly speed up the LLM inference time, as well as slightly\\nboost its performance. Speciﬁcally, in our problem setting there are 159 uniquesubtasks. At each step of the plan generation, 159 prompts of length ∼1500\\ntokens must pass through the model to evaluate each subtask, while applying\\nthe rules reduces the number of prompts to an average of 50. Thus, instead of\\n∼250,000 tokens, the model receives ∼75,000 tokens.\\n3.3 Autoregressive Plan Generation\\nAn autoregressive language model (LM) is trained with a maximum likelihood',\n",
       "  'loss to model the probability of a sequence of tokens yconditioned on an input\\nsequence x, i.e.θ= argmax\\nθP(y|x;θ). The trained LM is then used for predic-\\ntionˆy= argmaxy∈SP(y|x;θ), where Sis the set of all text sequences.\\nLLMs are trained on large text collections and can perform in-context learn-\\ning using only contextual information. It is possible to use this approach toquery LLMs to generate action plans for high-level tasks by prepending some\\ntask-relevant context to the input sequence x(prompt). For a plan-generating\\ntask, a prompt usually consists of one or more parts: including a high-level prob-\\nlem statement, a description of the environment’s current state, and examples\\nof similar problems that have already been solved.\\nFull Plan Generation. To generate a complete plan, the language model\\ngenerates a text sequence ybased on the input sequence x=f\\nprompt (Y). When',\n",
       "  'generating a complete plan using a language model, it can be challenging tomatch the proposed commands with those available to the agent in the current\\nenvironment. Some possible issues include: 1) the action or its arguments are\\nrepresented by a synonym or alternative phrase (e.g. “pick up the cup” instead',\n",
       "  '226 C. Sarkisyan et al.\\nof “pick up the mug”), 2) the action or its arguments are lexically incorrect\\n(e.g. “cut the knife and heat it up”), 3) the plan is generated in a free-formrather than using a structured template (e.g. “the robot should pick up the\\nspoon and put it in the sink” instead of “pick up the spoon, then put it in\\nthe sink”), and 4) the sequence of actions is not logically connected (e.g., toretrieve an item from the fridge, the fridge must ﬁrst be opened). The quality of\\nthe plan generation improves with the increase in the number of examples in the\\nprompt. The solution to the grounding problem is proposed through step-by-stepgeneration.\\nStep-by-Step Plan Generation. In the case of iterative step-by-step plan\\ngeneration, the subtasks available in the environment are compared with the\\nsubtask generated at each step. The generated subtask s\\ngis compared with all\\npossible subtasks in the environment {se1,se2,...,s ek}and we taking the most\\nclosest ˆ se:\\nˆse= argmax',\n",
       "  'sei[C(femb(sg),femb(sei))] (3)\\nwhere fembis an embedding function and Cis a cosine distance function. In our\\nwork, we used Sentence-BERT [ 18] to obtain subtask embeddings.\\nThe generated step is then added to the prompt and the process continues\\niteratively until the stop sequence is generated. The advantage of this approachis the increased feasibility of the generated plan, as all object actions can be\\ngrounded to the environment. The disadvantage is the increase in computational\\ncomplexity, which grows exponentially with the number of available actions andobjects.\\n3.4 Plan Evaluation\\nTo measure the correctness of the plan generation, we use several metrics that\\nevaluate both the complete coincidence of plans and their partial similarity.\\nAccuracy (ACC ), i.e. the exact match of GT and model prediction subtask\\nsequences, is the most strict criterion that requires both actions and interactionobjects matching for each subtask in the plans.\\nACC(S,S\\n′)={\\n0,∃sk∈S,s′\\nk∈S′:ak̸=a′\\nk∨ok̸=o′\\nk',\n",
       "  '1,∀sk∈S,s′\\nk∈S′:ak=a′\\nk∧ok=o′\\nk(4)\\nWe designed the Actions Exact Match (AEM) metric to evaluate the\\nmethod’s ability to understand the task’s semantics, in particular, to classify\\nits type. AEM requires action matching but allows the model to confuse theobjects.\\nAEM (S,S\\n′)={\\n0,∃sk∈S,s′\\nk∈S′:ak̸=a′\\nk\\n1,∀sk∈S,s′\\nk∈S′:ak=a′\\nk(5)',\n",
       "  'Evaluation of Pretrained Large Language Models 227\\nWe also compute the LCS between two plans, normalized by the maximum\\nlength of the two, following [ 9]. However, there are cases when GT plans do not\\nallow us to determine the plan’s correctness. For one task, there may be several\\ncorrect plans (e.g., to complete the task “put a cup with a fork in it on the\\ncounter” the agent can perform actions in a diﬀerent order: put the cup on the\\ncounter, then put the fork in the cup, or vice versa), while according to the ACC,\\nonly the one ﬁxed order of subtasks is correct. Furthermore, tasks can often be\\nvague, so without the vision of the environment one can only make assump-tions about their purpose and the objects engaged (e.g., the task “to acquire an\\no d di t e ma sp l a c ei tw h e r ei ti sn o tu s e f u l ” ). Thus, we involve Human assess-\\nment (H) in the plan evaluation process. Three people were asked to evaluate',\n",
       "  'how many correct plans, in their opinion, the model generated, based only on\\nthe requirements of the text instructions. The GT plans were also available for\\nobservation. The ﬁnal metric value is obtained by dividing the number of correct\\nplans by the total number of tasks and averaging over the number of assessors.\\n4 Experiments\\nTo select an LLM, we studied publicly available pretrained models with a largenumber of parameters. In the robot’s behavior planning task GPT-3 model [ 4]\\nwith 175B parameters is frequently used. However, OpenAI API for GPT-3\\nallows one to generate/evaluate only a limited number of tokens, which is espe-\\ncially critical for the resource-intensive subtask evaluation mode. Therefore, weconsidered the models without restrictions on the number of input and output\\ntokens and also work fast enough with a limited number of computing resources.\\nAs LLMs, we use and compare three models: GPT-J-6B (GPT-J) [25],GPT-',\n",
       "  'NeoX-20B (GPT-NeoX) [3]a n d OPT-30B (OPT) [28]. GPT-J is a GPT-\\n3-like autoregressive language model, pretrained on the Pile dataset [ 7], contain-\\ning 825 GB of an English text corpus. GPT-J contains 6B trainable parameters.The architecture of the GPT-NeoX model almost repeats the GPT-J’s, and was\\ntrained on the same dataset, but has a larger number of parameters (20B). OPT\\nis also a decoder-only model from GPT-3 family and has the largest number ofparameters (30B) in our experiments. OPT was trained on several subsets of the\\nPile, and some other datasets (RoBERTa [ 13], CCNews [ 15], etc.), containing\\nEnglish and a small amount of non-English data. GPT-J and OPT models usethe BPE [ 19] tokenizer, like GPT-3, while GPT-NeoX uses a tokenizer specially\\ndesigned for this model.\\n4.1 ALFRED Environment\\nALFRED [ 20] is a benchmark for evaluating the ability of AI systems to under-',\n",
       "  'stand and act upon human language instructions in interactive visual environ-ments, such as a household setting. The benchmark includes expert demonstra-\\ntions of tasks, accompanied by natural language instructions, in 120 diﬀerent\\nindoor scenes in the AI2-THOR 2.0 [ 10] environment. These demonstrations',\n",
       "  '228 C. Sarkisyan et al.\\ninvolve partial observability, long action horizons, an underspeciﬁed natural lan-\\nguage, and irreversible actions. ALFRED includes a total of 25,743 English lan-guage directives describing 8,055 expert demonstrations, resulting in a total of\\n428,322 image-action pairs. In this benchmark, the agent is provided with ego-\\ncentric vision and must complete tasks by interacting with objects using a pixel-wise interaction mask. The benchmark is intended to facilitate the development\\nof AI systems that can translate human language into sequences of actions and\\ninteractions in a visually and physically realistic simulation environment.\\nTo create GT plans, we collected a dataset from task descriptions and cor-\\nresponding subtask sequences extracted from the expert demonstrations. There\\nare seven high-level actions in ALFRED: PickupObject, PutObject, SliceObject,\\nHeatObject, CoolObject, CleanObject, ToggleObject , most of which need to be',\n",
       "  'broken down into lower-level subactions (e.g., to heat the object, the agent should\\nopen the microwave, put the object in it, etc.).\\nThere are seven types of tasks in ALFRED, ranging in diﬃculty: Pick&Place,\\nLook in Light, Heat&Place, Cool&Place, Clean&Place, Pick Two&Place, andStack&Place. We divided them into three groups of increasing complexity and\\nplan length and chose one type from each group for experiments. Pick&Place\\n(PP) is the simplest type, in which one needs to pick up an object and put it inanother place. In Heat&Place (HP), the object needs to be additionally warmed\\nup before being put somewhere. In some tasks, the object must be sliced with\\na knife, which extends the plan’s length by one more action. The most complextask type is Stack&Place (SP), in which one needs to put an object in a movable\\ncontainer and put them in a speciﬁed location. The object can also be sliceable.\\n4.2 Prompt Engineering',\n",
       "  'To build a prompt, we use the template proposed in [ 1], which is a dialog between\\na user and a robot, and adapt it for the ALFRED. We ﬁrst add a preﬁx to theprompt describing the general problem statement. Then we preprocess ALFRED\\ndata by removing the punctuation from the task descriptions and converting\\nthem to lowercase. These descriptions are used as the user input in the dialog.Further, we convert GT plans: each subtask in the GT plan is mapped to a\\nnatural language subtask with the φ() function. For each queried task type, n\\nexamples of GT plans are randomly selected and concatenated to the prompt,\\nalternating by type. The imperative task description τis reformulated as a ques-\\ntion “Human: How would you {τ}?” and the sequence of subtasks l\\nkin GT plan\\nis formatted into a sequence “Robot: I would: 1. {l1},2.{l2},...,n. {ln}”. To avoid\\nmodel biases due to repetitive or ambiguous task examples, we additionally edit',\n",
       "  'prompts manually by replacing some examples with others and adding morediversity in object classes.\\nA prompt can contain examples of both diﬀerent types, and only one. One-\\ntype prompt problem setting is easier since the model “learns” on plans withsimilar sequences of actions and can lead to better performance. Such an app-\\nroach is possible if one can somehow classify task types before adding them to the\\nprompt, e.g., using a trained language model, as in [ 16], or using the k-neighbor',\n",
       "  'Evaluation of Pretrained Large Language Models 229\\nmethod to select in-context examples [ 22]. In our case, we consider the task type\\nknown in advance, given as GT.\\nWe conduct two series of experiments in subtask evaluation, full plan gen-\\neration, and step-by-step generation modes for Pick&Place, Heat&Place, and\\nStack&Place task types separately and compute the metrics values. In the ﬁrstblock of experiments, a one-type prompt is individually set for each task type,\\ncontaining examples of only this type. For the second block, prompts include\\nexamples of all 7 task types, to determine whether the model has common sensereasoning abilities and can classify the task type.\\n5 Results\\nTask-Speciﬁc Prompt. For the subtask evaluation mode, we validate GPT-\\nJ, GPT-NeoX, and OPT on 35 tasks per type, randomly selected from the\\nALFRED dataset. For each model, the experiments were run repeatedly with dif-',\n",
       "  'ferent prompt lengths (containing a diﬀerent number of examples), then the bestones were selected from the results obtained. The results are given in Table 1.\\nSince there are examples of only one type in the prompt, the sequence of actions\\nis successfully determined for each model, as evidenced by the high AEM metric.There is a signiﬁcant gap between ACC and H metrics because of the noisiness\\nof the ALFRED dataset. OPT outperforms the other two models in terms of\\nthe ACC on two task types but is lower in terms of H metrics. We found thatincreasing the number of model parameters (from GPT-J to OPT) does not\\nprovide a signiﬁcant increase in performance, especially in the case of one-type\\nprompts. In general, according to H, our approach with GPT-NeoX as the LLMwith a one-type prompt achieves the best quality of about 83%.\\nTable 1. Results for LLMs in three diﬀerent planning modes with task-speciﬁc prompt.\\nTask GPT-J (6B) GPT-NeoX (20B) OPT (30B)\\nACC AEM LCS H ACC AEM LCS H ACC AEM LCS H',\n",
       "  'Subtask Evaluation Mode\\nPP 0.57 1.00 0.85 0.89 ±0.000.63 1.00 0.87 0.97 ±0.000.66 1.00 0.87 0.85 ±0.02\\nHP 0.77 0.86 0.88 0.83 ±0.030.74 0.89 0.88 0.83 ±0.050.60 0.71 0.79 0.69 ±0.03\\nSP 0.31 0.91 0.74 0.69 ±0.130.26 1.00 0.70 0.70 ±0.010.34 1.00 0.69 0.64 ±0.02\\nFull Plan Generation\\nPP 0.14 0.91 0.52 0.85 ±0.010.32 1.00 0.59 0.91 ±0.050.48 1.00 0.76 0.97 ±0.03\\nHP 0.38 0.97 0.65 0.55 ±0.020.29 0.49 0.50 0.59 ±0.170.22 0.58 0.39 0.84 ±0.05\\nSP 0.20 0.57 0.48 0.66 ±0.050.20 0.94 0.52 0.66 ±0.050.17 0.39 0.50 0.60 ±0.03\\nStep by Step Plan Generation\\nPP 0.62 0.97 0.79 0.89 ±0.020.59 1.00 0.82 0.96 ±0.030.29 1.00 0.71 0.78 ±0.04\\nHP 0.40 0.60 0.60 0.47 ±0.030.34 0.51 0.57 0.40 ±0.030.29 0.74 0.61 0.60 ±0.08\\nSP 0.37 0.89 0.68 0.71 ±0.000.29 0.77 0.61 0.54 ±0.080.03 0.89 0.41 0.37 ±0.06',\n",
       "  '230 C. Sarkisyan et al.\\nA Prompt with Mixed Task Examples. In our experiments with mixed\\nprompts, the prompt contains three examples for each of the seven task types,with a total length of 21. The results are given in Table 2.\\nFor the subtask evaluation mode, the values of the metrics have deteriorated\\ncompared to the ﬁrst block of experiments. The most complex task type SP hasthe smallest ACC values; however, for the GPT-NeoX AEM is about 80%, which\\nmeans that the model understands the semantics of this task well, compared\\nto the smaller GPT-J. GPT-J outperforms the two other models on HP type.OPT, having the largest number of parameters, showed the worst results, having\\n“overﬁtted” for speciﬁc task types present in the prompt.\\nTable 2. Results for LLMs in three diﬀerent planning modes with a mixed prompt.\\nTask GPT-J (6B) GPT-NeoX (20B) OPT (30B)\\nACC AEM LCS H ACC AEM LCS H ACC AEM LCS H\\nSubtask Evaluation Mode',\n",
       "  'PP 0.43 0.60 0.71 0.56 ±0.090.46 0.54 0.68 0.54 ±0.030.03 0.03 0.49 0.26 ±0.39\\nHP 0.51 0.60 0.75 0.54 ±0.080.29 0.40 0.62 0.53 ±0.190.37 0.51 0.66 0.40 ±0.03\\nSP 0.06 0.26 0.59 0.17 ±0.030.11 0.80 0.61 0.42 ±0.110.03 0.74 0.57 0.19 ±0.06\\nFull Plan Generation\\nPP 0.12 0.91 0.32 0.77 ±0.060.18 1.00 0.50 1.00 ±0.000.48 1.00 0.76 0.86 ±0.03\\nHP 0.14 0.14 0.40 0.70 ±0.010.20 0.51 0.43 0.70 ±0.060.22 0.58 0.39 0.62 ±0.11\\nSP 0.00 0.40 0.15 0.39 ±0.050.00 0.91 0.21 0.60 ±0.080.17 0.39 0.50 0.60 ±0.17\\nStep by Step Plan Generation\\nPP 0.47 1.00 0.65 0.68 ±0.070.62 0.97 0.14 0.91 ±0.000.21 0.97 0.48 0.75 ±0.05\\nHP 0.37 0.54 0.61 0.48 ±0.020.43 0.46 0.64 0.55 ±0.050.00 0.00 0.16 0.41 ±0.08\\nSP 0.09 0.49 0.53 0.39 ±0.050.14 0.71 0.52 0.53 ±0.040.03 0.36 0.19 0.43 ±0.15\\n6 Conclusion\\nIn our work, we explored the application of publicly available LLMs to the plan\\ngeneration for an embodied agent. We considered three modes of model oper-',\n",
       "  'ation: 1) subtask evaluation mode, 2) full autoregressive plan generation, and\\n3) step-by-step autoregressive plan generation. In our studies, we used modelswith a diﬀerent number of parameters, while there is no signiﬁcant increase in\\nthe metrics values with an increase in the number of parameters, and in some\\ncases, a decrease is observed.\\nIn general, the subtask evaluation mode performs better than the other two\\nwith a task-speciﬁc prompt. This mode is the most resource-intensive since it\\nrequires a parallel evaluation of all subtasks available to the agent. The mode offull autoregressive plan generation is the worst among others. The main problem\\nwith this mode is that the subtasks obtained in this way may not be executable\\nby the agent. The step-by-step autoregressive plan generation mode occupies anintermediate position with a task-speciﬁc prompt setting, but outperforms others',\n",
       "  'Evaluation of Pretrained Large Language Models 231\\nwith a mixed prompt. Although it requires a mapping of the received subtasks\\nto the space of agent subtasks, this procedure is not as resource intensive asthe parallel evaluation of subtasks and can be implemented with pre-computed\\nembeddings of subtasks. With an increase in the number of actions and objects,\\nthe number of subtasks also increases combinatorially. Although the mappingof generated subtasks to agent subtasks can be considered as implicit feedback\\nfrom the environment, in our work, we have focused on plan generation without\\nfeedback, and have taken this as a direction for further work.\\nReferences\\n1. Ahn, M., Brohan, A., Brown, N., Chebotar, Y., et al.: Do as i can and not as i say:\\ngrounding language in robotic aﬀordances (2022)\\n2. Anderson, P., Fernando, B., Johnson, M., Gould, S.: SPICE: semantic propositional',\n",
       "  'image caption evaluation. In: Leibe, B., Matas, J., Sebe, N., Welling, M. (eds.)ECCV 2016. LNCS, vol. 9909, pp. 382–398. Springer, Cham (2016). https://doi.\\norg/10.1007/978-3-319-46454-1\\n24\\n3. Black, S., Biderman, S., Hallahan, E., Anthony, Q., et al.: GPT-NeoX-20B: an\\nopen-source autoregressive language model (2022)\\n4. Brown, T., et al.: Language models are few-shot learners. In: NeurIPS (2020)\\n5. Chowdhery, A., Narang, S., Devlin, J., Bosma, M., et al.: PaLM: scaling language\\nmodeling with pathways (2022)\\n6. Driess, D., Xia, F., Sajjadi, M.S.M., Lynch, C., et al.: PaLM-E: an embodied\\nmultimodal language model (2023)\\n7. Gao, L., Biderman, S., Black, S., Golding, L., et al.: The pile: an 800GB dataset\\nof diverse text for language modeling (2020)\\n8. Gramopadhye, M., Szaﬁr, D.: Generating executable action plans with\\nenvironmentally-aware language models (2022)\\n9. Huang, W., Abbeel, P., Pathak, D., Mordatch, I.: Language models as zero-shot',\n",
       "  'planners: extracting actionable knowledge for embodied agents. In: ICML (2022)\\n10. Kolve, E., Mottaghi, R., Han, W., VanderBilt, E., et al.: AI2-THOR: an interactive\\n3D environment for visual AI (2017)\\n11. Kovalev, A.K., Panov, A.I.: Application of pretrained large language models in\\nembodied artiﬁcial intelligence. Doklady Math. 106(S1), S85–S90 (2022). https://\\ndoi.org/10.1134/S1064562422060138\\n12. Lin, B.Y., Huang, C., Liu, Q., Gu, W., Sommerer, S., Ren, X.: On grounded\\nplanning for embodied tasks with language models (2022)\\n13. Liu, Y., Ott, M., Goyal, N., Du, J., et al.: RoBERTa: a robustly optimized BERT\\npretraining approach (2019)\\n14. Logeswaran, L., Fu, Y., Lee, M., Lee, H.: Few-shot subgoal planning with language\\nmodels (2022)\\n15. Mackenzie, J., Benham, R., Petri, M., Trippas, J.R., et al.: CC-News-En: a large\\nEnglish news corpus. In: CIKM (2020)\\n16. Min, S.Y., Chaplot, D.S., Ravikumar, P., Bisk, Y., Salakhutdinov, R.: FILM: fol-',\n",
       "  'lowing instructions in language with modular methods (2021)\\n17. OpenAI: Introducing ChatGPT (2022). https://openai.com/blog/chatgpt\\n18. Reimers, N., Gurevych, I.: Sentence-BERT: sentence embeddings using Siamese\\nBERT-networks. In: Proceedings of the 2019 Conference on Empirical Methods in\\nNatural Language Processing. Association for Computational Linguistics (2019).\\nhttps://arxiv.org/abs/1908.10084',\n",
       "  '232 C. Sarkisyan et al.\\n19. Shibata, Y., Kida, T., Fukamachi, S., Takeda, M., et al.: Byte pair encoding: a text\\ncompression scheme that accelerates pattern matching (1999)\\n20. Shridhar, M., Thomason, J., Gordon, D., Bisk, Y., et al.: ALFRED: a benchmark\\nfor interpreting grounded instructions for everyday tasks. In: CVPR (2020)\\n21. Singh, I., Blukis, V., Mousavian, A., Goyal, A., et al.: ProgPrompt: generating\\nsituated robot task plans using large language models (2022)\\n22. Song, C.H., Wu, J., Washington, C., Sadler, B.M., et al.: LLM-planner: few-shot\\ngrounded planning for embodied agents with large language models (2022)\\n23. Vedantam, R., Lawrence Zitnick, C., Parikh, D.: CIDEr: consensus-based image\\ndescription evaluation. In: CVPR (2015)\\n24. Vemprala, S., Bonatti, R., Bucker, A., Kapoor, A.: ChatGPT for robotics: design\\nprinciples and model abilities. Tech. rep., Microsoft (2023)\\n25. Wang, B., Komatsuzaki, A.: GPT-J-6B: a 6 billion parameter autoregressive lan-',\n",
       "  'guage model (2021). https://github.com/kingoﬂolz/mesh-transformer-jax\\n26. Wei, J., et al.: Finetuned language models are zero-shot learners (2021)\\n27. Wei, J., et al.: Chain of thought prompting elicits reasoning in large language\\nmodels (2022)\\n28. Zhang, S., Roller, S., Goyal, N., Artetxe, M., et al.: OPT: open pre-trained trans-\\nformer language models (2022)',\n",
       "  'Alien Versus Natural-Like Artiﬁcial General\\nIntelligences\\nHoward Schneider1(B)and Piotr Bołtu´ c2\\n1Sheppard Clinic North, Vaughan, ON, Canada\\nhschneidermd@alum.mit.edu\\n2University of Illinois, Springﬁeld, IL, USA\\npbolt1@uis.edu\\nAbstract. A natural-like artiﬁcial general intelligence (AGI) is deﬁned to be an\\nAGI that includes mammalian-like mechanisms such as core usage of navigation\\nmaps, spatial and temporal binding, predictive coding, lifelong learning, and innateknowledge procedures. If it includes core mechanisms which allow full causal and\\nanalogical processing, then it is also considered to be a human-like AGI. An AGI\\nwhich is not a natural-like AGI is termed an alien AGI. We consider (for sakeof example) as a natural-like AGI a largely conceptual cognitive architecture (the\\nCausal Cognitive Architecture 5) inspired by the mammalian brain. We consider',\n",
       "  '(for sake of example) as an alien AGI the large language model ChatGPT. We showfor a non-numeric simple example, that the natural-like AGI is able to solve the\\nproblem by automatic core mechanisms, but an alien AGI has difﬁculty arriving\\nat a solution. It may be, that alien AGIs’ understanding of the world is so differentfrom a human understanding that to allow alien AGIs to do tasks done originally\\nby humans, is to eventually invite strange failures in the tasks.\\nKeywords: artiﬁcial general intelligence ·large language model ·ChatGPT ·\\ncognitive architecture ·analogies ·causality\\n1 Introduction – The Need to Consider What Type of System is\\nProducing the Intelligent Behavior\\nA large body of work exists attempting to deﬁne artiﬁcial intelligence (AI) and to a lesser\\nextent artiﬁcial general intelligence (AGI) [ 1–10]. However, as we show below, in most\\nof these deﬁnitions there is the lack of consideration whether an AI/AGI is based on a',\n",
       "  'natural-like mechanism or is what we term here an “alien-like” AI/AGI.\\nAt the time of this writing, large language models have improved to the point where\\ntheir users may at times consider them to be AI/AGI systems performing at a human\\nlevel. For example, Kung and colleagues demonstrated that the large language model\\ncalled ChatGPT was able to essentially achieve passing marks on the United States\\nmedical licensing exams without any specialized training ahead of time [ 11].\\nWe make no claims of AGI existing at the time of writing, but in the following\\nsections, we will attempt to consider AGI systems in particular, and go on to deﬁne\\n© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023\\nP. Hammer et al. (Eds.): AGI 2023, LNAI 13921, pp. 233–243, 2023.https://doi.org/10.1007/978-3-031-33469-6\\n_24',\n",
       "  '234 H. Schneider and P. Bołtu´ c\\nnatural-like AGI, and by exclusion, alien-like AGI (or “alien AGI”), by the mechanisms\\nthey use. Since alien AGIs will arrive at decisions in ways very different than natural-like\\nAGIs would, what effect will this have on the utility and the safety of their decisions?\\n2 Deﬁnitions of Artiﬁcial Intelligence (AI) and Artiﬁcial General\\nIntelligence (AGI)\\nRussell and Norvig [ 1] deﬁne artiﬁcial intelligence in terms of replicating human per-\\nformance versus rational (i.e., doing the “right thing”) thought, and in terms of inter-\\nnal thought processes versus external intelligent behavior. Legg and Hutter [ 2]p r o -\\nvide a mathematical formulation of a measure of machine intelligence in terms of an\\nagent’s “ability to achieve goals in a wide range of environments.” Chollet [ 3] proposes\\nthe machine-suitable Abstraction and Reasoning Corpus, with tasks similar to Raven’sProgressive Matrices.',\n",
       "  'Many other approaches with regard to classifying AI systems exist. For example,\\nWang [ 4] attempts to carefully deﬁne what is an artiﬁcial intelligence, and proposes\\n“adaptation with insufﬁcient knowledge and resources.” Rosenbloom and colleagues\\n[5] describe characterizing AI systems and cognitive architectures in terms of basic\\ndichotomies, i.e., is the system symbolic versus sub-symbolic, symmetric versus asym-metric, and combinatory versus non-combinatory. The ﬁeld of biologically inspired\\ncognitive architectures (BICA) describes cognitive architectures inspired by the human\\nbrain [ 6]. Such BICA systems can serve to give insight into brain function as well as\\nto create systems acting as AI systems and perhaps in the future as the basis of AGI\\nsystems.\\nThe term “artiﬁcial general intelligence” was brieﬂy used in 1997 by Gubrud [ 7]\\nbut used again more extensively by Goertzel and Legg in 2002 [ 8]. Goertzel and Pen-',\n",
       "  'nachin [ 9] noted that unlike “narrow AI… artiﬁcial general intelligence can solve a\\nvariety of complex problems in a variety of different domains, and that controls itself\\nautonomously…” There have since been other attempts to characterize what deﬁnes\\nartiﬁcial general intelligence. For example, Adams and colleagues [ 10] describe a large\\nvariety of characteristics for AGI environments, tasks, agents, and architectures.\\nAgain, no claims of AGI existing at the time of writing are made, but given that we\\nare concerned largely by more capable systems that produce intelligence in keeping withthe various deﬁnitions of AGI, in this paper, we will talk about AGI rather than AI. As\\nnoted earlier, none of these deﬁnitions of artiﬁcial general intelligence really consider\\nwhether an AGI is based on a natural-like mechanism or not.\\n3 A Deﬁnition of a Natural-Like Artiﬁcial General Intelligence',\n",
       "  'While the underlying integrative mechanisms of a mammalian brain (let alone any animalbrain) are still not well understood, certain high-level mechanisms are recognized, and the\\ndeﬁnition of a natural-like artiﬁcial general intelligence (AGI) thus requires a number\\nof these pertinent high-level mechanisms. As will be seen in the sections below, therationale for distinguishing an alien AGI from a natural-like AGI is because alien AGIs',\n",
       "  'Alien Versus Natural-Like Artiﬁcial General Intelligences 235\\nmay arrive at answers to problems in ways very different than a human or a natural-like\\nAGI would, and this can have an important (negative or positive) impact on the utility\\nand safety of their decisions.\\nA natural-like artiﬁcial general intelligence (AGI) is deﬁned as follows:\\n\\x81A natural-like AGI uses high-level mechanisms to produce artiﬁcial general intel-\\nligence similar to the high-level mechanisms used by the mammalian-like animalbrain.\\nA. High-level mechanisms required in all natural-like AGIs:\\n1. Given the existence of spatial maps in hippocampi, and given hippocampi in\\nall mammals [ 12–14], we postulate [ 15,16] and require the use of navigation\\nmaps (described in more detail in the next section) not just for navigation, but\\ninvolved in the core mechanisms of the AGI.\\n2. Spatial as well as temporal binding (described in more detail below) of sensory',\n",
       "  'inputs with the navigation maps or other internal data structures [ 16–18].\\n3. Given that it is well known that higher-level brain or thought levels can inﬂuence\\nthe perception of information in a sensory scene, the AGI’s core mechanism mustuse some form of predictive coding, i.e., errors between what the AGI thought\\nit would be seeing or sensing (or internally concluding) and between the actual\\nsensory inputs, are propagated to higher levels [ 19].\\n4. Given that incremental, lifelong learning can occur in mammalian brains, it\\nshould also occur in a natural-like AGI.\\n5. Natural-like AGI does not start off as a tabula rasa, but contains a number of\\ninnate knowledge procedures concerning objects, physics, agents, numbers, and\\nsocial group members, similar to those found in mammals and humans. The work\\nof Spelke and others [ 20,21] shows that such knowledge procedures occur in\\nhuman infants as well as in some other young mammals.',\n",
       "  'B. In a natural-like AGI which is also “human-like” the following additional high-level\\nmechanisms are required to be used by the natural-like/human-like AGI:\\n6. Predominant causal (i.e., essentially considering cause and effect) processing of\\nsensory and stored data can occur by re-operating on intermediate results. Whilenon-human mammals including chimpanzees ( Pan troglodytes ) and bonobos\\n(Pan paniscus ) do not demonstrate full causal behavior, for the most part, humans\\ndo [22].\\n7. Analogical processing is integrated in the core mechanisms of the natural-\\nlike/human-like AGI. While most mammals, including chimpanzees, do not\\ndemonstrate full analogical abilities, humans, of course, largely, do [ 16,23].\\nPsychological evidence indicates that analogical processing is actually a core\\nmechanism in the human brain [ 24].\\n“High-level mechanism” refers to the mechanisms employed above the level of\\nthe local neural circuits. “Mammalian-like” means similarities between the AGI and',\n",
       "  '236 H. Schneider and P. Bołtu´ c\\na mammalian animal brain. Would a natural-like AGI modeled on the crow’s brain\\n(i.e., genus Corvus ) be acceptable as a natural-like AGI under the above deﬁnition?\\nYes. Non-mammalian vertebrates have navigation systems including homologues to\\nthe mammalian brain’s navigation-related structures [ 25]. While most non-mammalian\\nvertebrates may not meet the main criterion of the deﬁnition above of producing artiﬁcial\\ngeneral intelligence-like behavior, some birds may [ 26]. (However, a natural-like AGI\\nmodeled on the octopus’s brain (e.g., Amphioctopus marginatus ) would not—it would\\nbe considered as an alien AGI under the above deﬁnition. While octopuses may be able\\nto solve problems and use tools [ 27], their brain architecture is very different from the\\nmammalian brain.)\\nKralik gives a large list of uniquely human cognitive abilities [ 28]. However, above in\\nour deﬁnition we choose two pertinent human-like high-level mechanisms which are not',\n",
       "  'found in other mammals, and which are more mechanically suitable for considerationof inclusion in an AGI. Although beyond the scope of this paper, if the two human-level\\ncriteria of the deﬁnition above are present, then it is likely that most of the other of what\\nare considered uniquely human cognitive abilities will also co-exist or emerge [ 15,16,\\n18].\\n4 An Example of a Natural-Like Artiﬁcial General Intelligence\\nThe Causal Cognitive Architecture 5 (CCA5) is a biologically inspired cognitive archi-\\ntecture (BICA) loosely inspired by the mammalian brain, conceptually meeting the\\ncriteria above for a natural-like AGI, and if full feedback is used [ 16], also meeting the\\ncriteria above for a human-like AGI. The navigation maps in the simulated architec-\\nture are arrays with 6x6x6 spatial dimensions (as well as other dimensions for object\\nsegmentation, associated procedures and metadata). Thousands to billions of such nav-',\n",
       "  'igation maps can exist within the architecture and prove useful for the overall storage\\nand representational needs of the architecture, i.e., each navigation map storing asso-ciated features of the environment along with associated procedures and links to other\\nnavigation maps [ 15].\\nAn overview of the architecture of the CCA5 is shown in Fig. 1. The architecture\\ntakes as an input the set of sensory features streaming in from different perceptual\\nsensors. Objects detected in this stream of sensory features are segmented, and visual,\\nauditory, and other sensory features of each segmented object are spatially mapped ontonavigation maps dedicated to one sensory modality. This represents the ﬁrst step in spa-\\ntial object binding. These single-sensory navigation maps are then mapped onto a best\\nmatching multi-sensory navigation map taken from the Causal Memory Module (Fig. 1)\\nand operated on in the Navigation Module. This represents the second step in spatial',\n",
       "  'object binding. As well, a parallel sensory stream has gone through the Sequential/Error\\nCorrecting Module (Fig. 1) which detects changes with time, and is then converted\\nto a vector value which is also bound along with the spatial features onto the same\\nnavigation maps, effectively representing temporal binding [ 17,18]. Instinctive prim-\\nitives and learned primitives, essentially small rules or productions, themselves using\\nmodiﬁed navigation maps, are then applied onto the navigation map in the Navigation\\nModule, producing a signal to the Output Vector Association Module (Fig. 1) and then to',\n",
       "  'Alien Versus Natural-Like Artiﬁcial General Intelligences 237\\nthe external embodiment. Instinctive primitives are innate knowledge procedures con-\\ncerning objects, agents, numbers, and social group members. Learned primitives are\\nprocedures which are learned by the architecture.\\nThere are extensive feedback pathways throughout the architecture—states of a\\ndownstream module can inﬂuence the recognition and processing of more upstream\\nsensory inputs. The differences between the expected sensory input and the actual sen-\\nsory input are computed and fed forward, and inﬂuence the binding of the sensory inputsonto local sensory navigation maps in the Input Sensory Vectors Association Modules\\n(Fig. 1) as well as the ﬁnal binding of the local navigation maps onto a multisensory\\nnavigation map which becomes the working navigation map in the Navigation Module(Fig. 1). This is reﬂected in the equations describing the architecture in [ 16].',\n",
       "  'Existing navigation maps in the Input Sensory Vectors Association Modules (Fig. 1)\\nas well as the Sequential/Error Correcting Module (Fig. 1) and the Causal Memory\\nModule (Fig. 1) are updated with changes as sensory inputs stream in, and as well new\\nnavigation maps are created. This can occur constantly, as long as there is a sufﬁcient\\nsupply of empty navigation maps and links between navigation maps. There is no catas-trophic forgetting–navigation maps are updated or created, links are updated or created,\\nwith little effect on other navigation maps (i.e., lifelong learning occurs).\\nFig. 1. Overview of the Causal Cognitive Architecture 5 (CCA5)',\n",
       "  '238 H. Schneider and P. Bołtu´ c\\nIn the Causal Cognitive Architecture 5, the feedback pathways between the Navi-\\ngation Module/ Object Segmentation Gateway Module and the Input Sensory Vectors\\nAssociation Modules (Fig. 1) are enhanced allowing intermediate results from the Nav-\\nigation Module to be stored in the Input Sensory Vectors Association Modules. If so, inthe next cognitive cycle (i.e., cycles of passing input sensory vectors into and through\\nthe architecture), these intermediate results will automatically be considered as the input\\nsensory information and propagated to the Navigation Module and operated on again.By feeding back and re-operating on the intermediate results, the Causal Cognitive\\nArchitecture is able to formulate and explore possible cause and effect of actions, i.e.,\\ngenerate causal behavior [ 16–18]. As Schneider shows, a consequence of this enhance-\\nment in feedback processing of the intermediate results of the architecture is not only the',\n",
       "  'ability to generate causal behavior but that the architecture now readily uses analogical\\nreasoning as a central and core mechanism of action [ 16,18,29].\\nThe references [ 15–18,29] give the functioning of the Causal Cognitive Architecture\\n5 (CCA5) in much more detail, but we note here that it conceptually meets the above\\ndeﬁnition of being both a natural-like AGI and a human-like AGI:\\n“An AGI” – While no claims are made for the CCA5 as being a functioning AGI,\\nit conceptually could meet the deﬁnitions [ 9] above for an AGI: “…artiﬁcial general\\nintelligence can solve a variety of complex problems in a variety of different domains,and that controls itself autonomously…”\\n1.The use of navigation maps – The CCA5 makes extensive use of navigation maps\\nfor representational storage and operations.\\n2.Spatial as well as temporal binding – As described above, via the binding of',\n",
       "  'sensory inputs onto local navigation maps and then onto a multi-sensory navigationmap, spatial binding occurs, and temporal binding occurs via the Sequential/Error\\nCorrecting Module (Fig. 1).\\n3.Predictive coding – The errors between what the AGI thought it would be sensing\\nand between the actual sensory inputs are propagated to higher levels as described\\nabove.\\n4.Lifelong learning – Existing navigation maps in the Input Sensory Vectors Associ-\\nation Modules (Fig. 1) as well as the Sequential/Error Correcting Module (Fig. 1)\\nand the Causal Memory Module (Fig. 1) are updated with changes as sensory inputs\\nstream in, and as well new navigation maps are created. As noted above, this occursconstantly, and there is no catastrophic forgetting.\\n5.Innate knowledge procedures – As noted above, instinctive primitives, which are\\nessentially small rules or productions concerning objects, numbers, agents and so\\non, are built into the architecture.',\n",
       "  '6.Predominant causal processing – As noted above, by allowing feedback from the\\nintermediate results of the Navigation Module (Fig. 1) to be temporarily stored and\\nre-processed in the next cycle, the architecture is able to formulate and explorepossible cause and effect.\\n7.Analogical processing – As noted above, a consequence of the feedback of the\\nintermediate results from the Navigation Module (Fig. 1) to be temporarily stored,\\nis that analogical processing readily emerges as part of the core mechanisms of thearchitecture.',\n",
       "  'Alien Versus Natural-Like Artiﬁcial General Intelligences 239\\n5 Alien-Like AGI Versus Natural-Like AGI\\nThe rationale for distinguishing an alien AGI from a natural-like AGI is because alien\\nAGIs may arrive at answers to problems in ways very different than humans would,the latter which by anecdotal evidence [ 16] we assume is better represented by natural-\\nlike/human-like AGIs. This can have an important effect on the utility and safety of\\ndecisions arrived at.\\nAbove we presented the Causal Cognitive Architecture 5 (CCA5). While no claims\\nare made for the CCA5 as being a functioning AGI, conceptually it meets the above\\ndeﬁnitions for an AGI [ 9]. As well, it meets the criteria above of the deﬁnition of a\\nnatural-like AGI including being a human-like AGI. Thus, for the sake of example, we\\nconsider the CCA5 as a natural-like AGI and a human-like AGI.\\nAbove we discussed how a large language model (utilizes an underlying transformer',\n",
       "  'architecture [ 30]) called ChatGPT was able to essentially achieve passing marks on\\nthe United States medical licensing exams [ 11]. In actual performance, ChatGPT meets\\nmany of the requirements of an AGI in being “able to solve a variety of complex problemsin a variety of different domains”, and it could easily be embedded into an agent structure\\nand “control itself autonomously” [ 9]. While there is disagreement whether architectures\\nsuch as ChatGPT are close to representing true AGIs [ 31], for the sake of example, we\\nconsider ChatGPT as an AGI. Since it does not meet many of the criteria above of a\\nnatural-like AGI, we thus consider it an alien AGI.\\nIn Schneider [ 16] a robot controlled by a CCA5 architecture (for convenience we\\nwill term the combination of the CCA5 and the robot as the “CCA5”) wants to cross a\\ndeep river ﬁlled with ﬂoating leaves. It has no experience with leaves (and actually doesnot even know their names). However, by analogy (which occurs automatically as part',\n",
       "  'of the core mechanisms of the architecture in conjunction with its very basic intuitive\\nobject and physics knowledge) with stepping on similarly thin sheets of newspaper in apuddle at an earlier time, the CCA5 arrives at the conclusion that stepping on the leaves\\nwould result in it falling into the water of the river. Thus, without any prior detailed\\nunderstanding about leaves or what a leaf really is, it makes the correct decision not tocross the river.\\nChatGPT has an enormous innate storage (orders of magnitude greater than a single\\nhuman could have) of written information about the world (although it may be lackingwith regard to numeric computations, which we thus avoid in our examples). If it is asked\\nif it safe to cross a river walking on “leaves,” it will respond that it is too dangerous to\\ncross, as shown in Fig. 2.\\nHowever, consider if we ask this question in a way similar to the CCA5 example in',\n",
       "  'Schneider [ 16] where the AGI has never seen an object such as leaves before. Imagine\\nthat the alien AGI (i.e., ChatGPT) had not seen before the leaves on the river other than\\nseeing them in a catalog somewhere as “solid08”. The alien AGI has the same limited\\ninformation about sheets of a newspaper as being “solid22”. The alien AGI is giventhe same previous experience about stepping on an object “solid22” (i.e., pages of a\\nnewspaper) in a puddle.\\nFrom Fig. 3we can see that even if the alien AGI (i.e., ChatGPT) is told a second\\ntime what happened with “solid22” (i.e., pages of a newspaper in a puddle) it still was',\n",
       "  '240 H. Schneider and P. Bołtu´ c\\nFig. 2. Conversation with ChatGPT about crossing a river ﬁlled with leaves. (The swirling symbol\\nrepresents answers from ChatGPT. Retrieved: Jan 30, 2023.)\\nnot able to readily come to the conclusion that the river should not be crossed (or to give\\na creative solution that a human would not normally have thought of).\\nFig. 3. Conversation with ChatGPT about crossing a river ﬁlled with “solid08”. (The swirling\\nsymbol represents answers from ChatGPT. Retrieved: Jan 30, 2023.)\\nFrom this overall conversation (i.e., Figs. 2plus 3) experience ChatGPT may possibly\\nnow know better what to do the next time it encounters “solid08” and “solid22”. However,\\nthere are a myriad of other such examples, where the examples may not be in the massive\\ncorpus which ChatGPT has trained on. In these examples, the alien AGI such as ChatGPT\\nmay be unable to make correct decisions where there are such unknowns, while a natural-',\n",
       "  'like and human-like AGI (e.g., CCA5) despite its much more modest world knowledge,could in theory handle these situations easily.\\nThis example really applies where the natural AGI and alien AGI are at a level aiming\\nfor human-level but not there yet. If an alien AGI is at a level many times that of a human',\n",
       "  'Alien Versus Natural-Like Artiﬁcial General Intelligences 241\\n(deﬁning “level” per Chollet [ 3]) then these kinds of examples may not hold, and further\\nwork is required to investigate natural-like AGIs versus alien AGIs.\\n6 Discussion\\nAbove we deﬁned a natural-like AGI, including those AGIs that are also human-likeAGIs. If an AGI is not a natural-like AGI, then we term it an alien AGI.\\nThe rationale for distinguishing an alien AGI from a natural-like AGI is because in\\na complex world where the decisions taken or the solutions arrived at are not straight-\\nforward, the underlying mechanisms used by the AGI do matter. As justiﬁed above,\\nfor the sake of example, we considered the large language model ChatGPT as an alienAGI. Similarly for the sake of example, we considered the largely conceptual Causal\\nCognitive Architecture 5 (CCA5) as a natural-like AGI and human-like AGI.\\nWe considered the example which had already been examined in [ 16] whereby the',\n",
       "  'CCA5 controlling a robot needs to cross a deep river. It sees a river ﬁlled with leaves\\n(which it has no previous understanding or knowledge of other than to recognize them,\\nand indeed considers them “solid08” objects). Through the use of key features of anatural-like AGI (causality, analogy, innate physics, and so on) it is able to automatically\\nreason that it would be dangerous to cross the river.\\nWe then considered the same example with respect to ChatGPT, which we consider\\nas an alien AGI. If we tell ChatGPT that the river is ﬁlled with “leaves,” due to its\\nsuper-human knowledge of much of what has ever been written, it immediately says not\\nto cross the river. However, if we specify that the river is ﬁlled with “solid08” objects(i.e., so that this problem represents something ChatGPT has not previously read about)\\nand give it information about analogous “solid22” objects similar to what the CCA5\\nreceived, it is not able to make a decision about crossing the river.',\n",
       "  'This is an extremely simple example. The simplicity was required in [ 16] because of\\nthe extremely modest simulation of the CCA5. On the other hand, ChatGPT representsan impressive engineering achievement, and yet, it was not able to make a decision on this\\nsimple example. Imagine a similar problem in an analogous situation where arriving at a\\ndecision held more importance, e.g., prescribing or not prescribing a certain medicationfor a patient, and so on. The safety and utility of the decision provided by an alien\\nAGI, despite its apparent super-human abilities, may be seriously and unpredictably\\nimpaired at times. Given the current and near-future technology, it may be that alienAGIs’ understanding of the world is so different from a human understanding that to\\nallow alien AGIs to do tasks done originally by humans, is to eventually invite strange',\n",
       "  'failures in the tasks. An alien AGI is not necessarily less powerful in cognition than ahuman, but different [ 32]. A need exists to distinguish future AGIs as being natural-like\\nAGIs versus alien AGIs.\\nReferences\\n1. Russel, S.J., Norvig, P.: “What is AI?” in Artiﬁcial Intelligence: A Modern Approach, 4th\\nedn, pp. 1–5. Pearson, New Jersey (2021)',\n",
       "  '242 H. Schneider and P. Bołtu´ c\\n2. Legg, S., Hutter, M.: Universal intelligence: a deﬁnition of machine intelligence. arXiv:0712.\\n3329 (2007)\\n3. Chollet, F.: On the measure of intelligence. arXiv:1911.01547 (2019)\\n4. Wang, P.: On deﬁning artiﬁcial intelligence. J. Artif. Gen. Intell. 10(2), 1–37 (2019). https://\\ndoi.org/10.2478/jagi-2019-0002\\n5. Rosenbloom, P., Joshi, H., Ustun, V .: (Sub)Symbolic x (A)Symmetric x (Non)Combinatory.\\nIn: Cox, M.T. (ed.) Proceedings of the 7th Annual Conference on Advances in Cognitive\\nSystems (2019)\\n6. Bołtu´ c, P., Boltuc, M.: BICA for AGI. Cogn. Syst. Res. 62, 57–67 (2020)\\n7. Gubrud, M.A.: Nanotechnology and international security. In: Fifth Foresight Conference on\\nMolecular Technology (1997)\\n8. Goertzel, B.: Who coined the term “AGI”? https://goertzel.org/who-coined-the-term-agi/ .\\nAccessed 26 Jan 2023\\n9. Goertzel, B., Pennachin, C. (eds.): Artiﬁcial General Intelligence. Springer, New York (2007)',\n",
       "  '10. Adams, S., et al.: Mapping the landscape of human-level artiﬁcial general intelligence. AI\\nMag. 33, 25–42 (2012). https://doi.org/10.1609/aimag.v33i1.2322\\n11. Kung, T.H., et al.: Performance of ChatGPT on USMLE. medRxiv (2022). https://doi.org/10.\\n1101/2022.12.19.22283643\\n12. Wernle, T., Waaga, T., Mørreaunet, M., Treves, A., Moser, M.B., Moser, E.I.: Integration of\\ngrid maps in merged environments. Nat. Neurosci. 21(1), 92–101 (2018)\\n13. Alme, C.B., Miao, C., Jezek, K., et al.: Place cells in the hippocampus. Proc. Natl. Acad. Sci.\\nU.S.A. 111(52), 18428–18435 (2014)\\n14. Schafer, M., Schiller, D.: Navigating social space. Neuron 100(2), 476–489 (2018)\\n15. Schneider, H.: Navigation map-based artiﬁcial intelligence. AI 3(2), 434–464 (2022)\\n16. Schneider, H.: An inductive analogical solution to the grounding problem. Cogn. Syst. Res.\\n77, 174–216 (2023). https://doi.org/10.1016/j.cogsys.2022.10.005',\n",
       "  '17. Schneider, H.: Causal cognitive architecture 2: a solution to the binding problem. In: Klimov,\\nV .V ., Kelley, D.J. (eds.) BICA 2021. SCI, vol. 1032, pp. 472–485. Springer, Cham (2022).https://doi.org/10.1007/978-3-030-96993-6_52\\n18. Schneider, H.: Causal cognitive architecture 3: a solution to the binding problem. Cogn. Syst.\\nRes. 72, 88–115 (2022)\\n19. Rao, R.P.N., Ballard, D.H.: Predictive coding in the visual cortex. Nat. Neurosci. 2(1), 79–87\\n(1999). https://doi.org/10.1038/4580\\n20. Spelke, E.S.: Initial knowledge: six suggestions. Cognition 50, 431–445 (1994)\\n21. Kinzler, K.D., Spelke, E.S.: Core systems in human cognition, chap 14. In: von Hofsten, C.,\\nRosander, K. (eds.) Progress in Brain Research, vol. 164 (2007)\\n22. Martin-Ordas, G., Call, J., Colmenares, F.: Tubes, tables and traps: great apes solve two\\nfunctionally equivalent trap tasks but show no evidence of transfer across tasks. Anim. Cogn.\\n11(3), 423–430 (2008). https://doi.org/10.1007/s10071-007-0132-1',\n",
       "  '23. Penn, D.C., Holyoak, K.J., Povinelli, D.J.: Darwin’s mistake: explaining the discontinuity\\nbetween human and nonhuman minds. Behav. Brain Sci. 31(2), 109–130 (2008)\\n24. Hofstadter, D.R.: Analogy as the core of cognition. In: Gentner, D., Holyoak, K.J., Kokinov,\\nB.N. (eds.) The Analogical Mind, pp. 499–538. MIT Press (2001)\\n25. Rodríguez, F., Quintero, B., Amores, L., Madrid, D., Salas-Peña, C., Salas, C.: Spatial\\ncognition in teleost ﬁsh: strategies and mechanisms. Animals 11(8), 2271 (2021)\\n26. Taylor, A.H., Knaebe, B., Gray, R.D.: An end to insight? New Caledonian crows can sponta-\\nneously solve problems without planning their actions. Proc. Biol. Sci. 279(1749), 4977–4981\\n(2012). https://doi.org/10.1098/rspb.2012.1998\\n27. Finn, J.K., Tregenza, T., Norman, M.D.: Defensive tool use in a coconut-carrying octopus.\\nCurr Biol. 19(23), R1069–R1070 (2009). https://doi.org/10.1016/j.cub.2009.10.052 .P M I D :\\n20064403',\n",
       "  'Alien Versus Natural-Like Artiﬁcial General Intelligences 243\\n28. Kralik, J.D.: What can nonhuman animals, children, and g tell us about human-level general\\nintelligence (AGI)? In: Goertzel, B., Iklé, M., Potapov, A., Ponomaryov, D. (eds.) AGI 2022,\\npp. 282–292. Springer, Cham (2023). https://doi.org/10.1007/978-3-031-19907-3_26\\n29. Schneider, H.: Analogical problem solving in the causal cognitive architecture. In: Goertzel,\\nB., Iklé, M., Potapov, A., Ponomaryov, D. (eds.) AGI 2022, pp. 100–112. Springer, Cham\\n(2023). https://doi.org/10.1007/978-3-031-19907-3_10\\n30. Vaswani, A., Shazeer, N., Parmar, N., et al.: Attention is all you need. In: Advances in Neural\\nInformation Processing Systems, vol. 30 (2017)\\n31. Gozalo-Brizuela, R., Garrido-Merchan, E.C.: ChatGPT is not all you need. A state of the art\\nreview of large generative AI models (2023). arXiv:2301.04655\\n32. Bołtu´ c, P.: Strong semantic computing. Procedia Comput. Sci. 1(123), 98–103 (2018)',\n",
       "  'Computing with Categories in Machine\\nLearning\\nEli Sennesh1(B),T o mX u2, and Yoshihiro Maruyama2\\n1Northeastern University, Boston, USA\\nsennesh.e@northeastern.edu\\n2Australian National University, Canberra, Australia\\n{tom.xu,yoshihiro.maruyama }@anu.edu.au\\nAbstract. Category theory has been successfully applied in various\\ndomains of science, shedding light on universal principles unifying diversephenomena and thereby enabling knowledge transfer between them.\\nApplications to machine learning have been pursued recently, and yet\\nthere is still a gap between abstract mathematical foundations and con-crete applications to machine learning tasks. In this paper we introduce\\nDisCoPyro as a categorical structure learning framework, which com-\\nbines categorical structures (such as symmetric monoidal categories andoperads) with amortized variational inference, and can be applied, e.g.,\\nin program learning for variational autoencoders. We provide both math-',\n",
       "  'ematical foundations and concrete applications together with compari-son of experimental performance with other models (e.g., neuro-symbolic\\nmodels). We speculate that DisCoPyro could ultimately contribute to the\\ndevelopment of artiﬁcial general intelligence.\\nKeywords: Structure learning\\n·Program learning ·Symmetric\\nmonoidal category ·Operad ·Amortized variational Bayesian inference\\n1 Introduction\\nCategory theory has been applied in various domains of mathematical science,\\nallowing us to discover universal principles unifying diverse mathematical phe-nomena and thereby enabling knowledge transfer between them [ 7]. Applications\\nto machine learning have been pursued recently [ 21]; however there is still a large\\ngap between foundational mathematics and applicability in concrete machinelearning tasks. This work begins ﬁlling the gap. We introduce the categorical\\nstructure learning framework DisCoPyro, a probabilistic generative model with',\n",
       "  'amortized variational inference. We both provide mathematical foundations andcompare with other neurosymbolic models on an example application.\\nHere we describe why we believe that DisCoPyro could contribute, in the long\\nrun, to developing human-level artiﬁcial general intelligence. Human intelligencesupports graded statistical reasoning [ 15], and evolved to represent spatial (geo-\\nmetric) domains before we applied it to symbolic (algebraic) domains. Symmet-\\nric monoidal categories provide a mathematical framework for constructing both\\nSupported by the JST Moonshot Programme on AI Robotics (JPMJMS2033-02).\\nc⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2023\\nP. Hammer et al. (Eds.): AGI 2023, LNAI 13921, pp. 244–254, 2023.https://doi.org/10.1007/978-3-031-33469-6\\n_25',\n",
       "  'Computing with Categories in Machine Learning 245\\nsymbolic computations (as in this paper) and geometrical spaces (e.g. [ 17]). We\\ntake Lake [ 15]’s suggestion to represent graded statistical reasoning via probabil-\\nity theory, integrating neural networks into variational inference for tractability.\\nIn terms of applications, we get competitive performance (see Subsect. 3.2below)\\nby variational Bayes, without resorting to reinforcement learning of structure aswith modular neural networks [ 13,20].\\nThe rest of the paper is organized as follows. In Sect. 2, we ﬁrst introduce\\nmathematical foundations of DisCoPyro (Subsects. 2and 2.1). In Sect. 3we then\\nexplain how to train DisCoPyro on a task (Subsect. 2.1) and provide experimen-\\ntal results and performance comparisons (Subsect. 3.2). Figure 1demonstrates\\nthe ﬂow of execution during the training procedure for the example task. Weconclude and discuss further applications in Sect. 4. We provide an example',\n",
       "  'implementation at https://github.com/neu-pml/discopyro with experiments at\\nhttps://github.com/esennesh/categorical\\nbpl. DisCoPyro builds upon Pyro [ 2]\\n(a deep universal probabilistic programming language), DisCoCat [ 4]( ad i s -\\ntributional compositional model for natural language processing [ 4]), and the\\nDisCoPy [ 6] library for computing with categories.\\n1.1 Notation\\nThis paper takes symmetric monoidal categories (SMCs) Cand their correspond-\\ning operads Oas its mathematical setting. The reader is welcome to see Fong [ 7]\\nfor an introduction to these. SMCs are built from objects Ob(C) and sets of\\nD1\\n2\\n3A\\nBC1.Graph skeleton of a free operad. \\nRandom walks on this graph induce a free operad prior.\\n2.Wiring diagram for decoder .\\n3.A decoder  and its inverse .\\nA.Sample edges from the free operad \\nprior to \\nll the wiring diagram.\\nB.Invert the decoder  to produce E \\nand predict the data.\\nC.Evaluate the ELBO and perform a \\ngradient update on the decoder parameters  and inference',\n",
       "  'parameters .D\\nDE\\nD\\nθ\\nϕℝ2ℝ4\\nℝ6ℝ4×ℝ8ℝ64\\nFig. 1. Example experiment. In each epoch of training, DisCoPyro learns variational\\nautoencoder structures by sampling them from its skeleton according to a wiring dia-gram, then learning their faithful inverses as approximate posteriors.',\n",
       "  '246 E. Sennesh et al.\\nmorphisms C(τ1,τ2) between objects τ1,τ2∈Ob(C). Operads are built from\\ntypes Ty(O) and sets of morphisms O(τ1,τ2) between types τ1,τ2∈Ty(O). An\\nSMC is usually written ( C,⊗,I) with a product operation ⊗over objects and\\nmorphisms and a unit Iof⊗. In both settings, every object/type τhas a unique\\nidentity morphism idτ. Categories support composition g◦fon morphisms, and\\noperads support indexed composition g◦if(fori∈N) on morphisms.\\n2 Foundations of DisCoPyro\\nIn essence, Deﬁnition 1below exposes a ﬁnite number of building blocks (genera-\\ntors) from an SMC, and the morphisms constructed by composing those genera-\\ntors with ◦and⊗. For example, in categories of executable programs, a monoidal\\nsignature [ 6] speciﬁes a domain-speciﬁc programming language.\\nDeﬁnition 1 (Monoidal signature in an SMC). Given a symmetric\\nmonoidal category (SMC) Cwith the objects denoted by Ob(C),amonoidal sig-\\nnature1S=(O,M )in that SMC consists of\\n– A ﬁnite set O⊆Ob(C);a n d',\n",
       "  '– A ﬁnite set Mconsisting of elements m:C(τ1,τ2)for some τ1,τ2∈O,s u c h\\nthat∀τ∈O,m̸=idτ.\\nThe following free operad over a monoidal signature represents the space of\\nall possible programs synthesized from the above building blocks (generators\\nspeciﬁed by the monoidal signature). Employing an operad rather than just\\na category allows us to reason about composition as nesting rather than justtransitive combination; employing an operad rather than just a grammar allows\\nus to reason about both the inputs and outputs of operations rather than just\\ntheir outputs.\\nDeﬁnition 2 (Free operad over a signature). The free operad O\\nSover a\\nsignature S=(O,M )consists of\\n– A set of types (representations) Ty(OS)={I}∪O⊗;\\n– For every n∈N+a set of operations (mappings) OS(τ0,...,τn−1;τn)con-\\nsisting of all trees with ﬁnitely many branches and leaves, in which each vertex\\nvwithn−1children is labeled by a generator m(v)∈Msuch that dom(m(v))\\nhas product length n−1;',\n",
       "  '– An identity operation idτ:OS(τ;τ)for every τ∈Ty(OS);a n d\\n– A substitution operator ◦ideﬁned by nesting a syntax tree OS(σ1,...,σm;τi)\\ninside another OS(τ1,...,τn−1;τn)when i∈[1...n−1]to produce a syntax\\ntreeOS(τ1,...,τi−1,σ1,...,σm,τi+1,...τn−1;τn).\\nIntuitively, free operads share a lot in common with context-free grammars,\\nand in fact Hermida [ 10] proved that they share a representation as directed\\nacyclic hypergraphs. The deﬁnition of a signature in an SMC already hints atthe structure of the appropriate hypergraph, but Algorithm 1will make it explicit',\n",
       "  'Computing with Categories in Machine Learning 247\\nInput: signature S=(O,M)\\nOutput: hypergraph H=(V,E), recursion sites R\\nV←O;\\nE←map(λm.(dom( m),cod(m)),M);\\nR←∅;\\nstack ←{v∈V||v|>1};\\nwhile stack ̸=∅do\\nty←pop(stack );\\ninhabitants ←map(λc.{(dom( e),c)|e∈E,cod(e)=c},chunks( ty,V));\\nforeach ((d1,c1),...,(dk,c k))∈⨂inhabitants do\\nifnotsublist(⨂\\ni∈[1..k]di,ty)then\\nR←R∪{ ⊗[(d1,c1),...,(dk,c k)]}\\nE←E∪{(⨂\\ni∈[1..k]di,⨂\\ni∈[1...k ]ci)};\\nifd/∈Vthen\\npush( stack,d);\\nV←V∪{⨂\\ni∈[1..k]di};\\nend\\nend\\nend\\nend\\nreturn (V,E),R\\nAlgorithm 1: Algorithm to represent a free operad as a hypergraph. The\\nfunction chunks partitions tyinto sublists, each an element of the set V.\\nand add edges to the hypergraph corresponding to nesting separate operations\\nin parallel (or equivalently, to monoidal products in the original SMC). In the\\nhypergraph produced by Algorithm 1, each vertex corresponds to a non-product',\n",
       "  'type and each hyperedge has a list of vertices as its domain and codomain.Each such hypergraph admits a representation as a graph as well, in which the\\nhyperedges serve as nodes and the lists in their domains and codomains serve as\\nedges. We will use this graph representation G≃Hto reason about morphisms\\nas paths between their domain and codomain.\\nWe will derive a probabilistic generative model over morphisms in the free\\noperad from this graph representation’s directed adjacency matrix A\\nG.\\nDeﬁnition 3 (Transition distance in a directed graph). The “transition\\ndistance” between two indexed vertices vi,vjis the negative logarithm of the i,j\\nentry in the exponentiated adjacency/transition matrix\\nd(vi,vj)=−log([\\neAG]\\ni,j)\\n, (1)\\nwhere the matrix exponential is deﬁned by the series\\neAG=∞∑\\nn=1(AG)n\\nn!.\\n1Also called a “hypersignature”.',\n",
       "  '248 E. Sennesh et al.\\nData: hypergraph ( V,E)\\nfunction Path( τ−,τ+,β,w)\\ni←1;\\nτi←τ−;\\nf←id ty−;\\nwhile τi̸=τ+do\\nei∼π(e∈E|τi,τ+β);\\nτi←cod(ei);\\nf←f/fatsemiGenerator( ei,β,w);\\ni←i+1 ;\\nend\\nreturn f\\nend\\nAlgorithm 2: The Markov chain constructing paths between types\\nA soft-minimization distribution over this transition distance will, in expec-\\ntation and holding the indexed target vertex constant, deﬁne an probabilistic\\ngenerative model over paths through the hypergraph.\\nDeﬁnition 4 (Free operad prior). Consider a signature S=(O,M )and\\nits resulting graph representation G=(V,E )and recursion sites R, and then\\ncondition upon a domain and codomain τ−,τ+∈Ty(OS)represented by vertices\\nin the graph. The free operad prior assigns a probability density to all ﬁnite paths\\ne=(e1,e2,...,e n)with dom( e)=τ−and cod( e)=τ+by means of an absorbing\\nMarkov chain. First the model samples a “precision” βand a set of “weights” w\\nβ∼γ(1,1) w∼Dirichlet(\\n⃗1(|M|+|R|))\\n.',\n",
       "  'Then it samples a path (from the absorbing Markov chain in Algorithm 2)b y\\nsoft minimization (biased towards shorter paths by β) of the transition distance\\nπ(e∈E|τ1,τ2;β): =exp(\\n−1\\nβd(cod(e),τ2))\\n∑\\ne′∈E:dom( e′)=τ1exp(\\n−1\\nβd(cod(e′),τ2)). (2)\\nEquation 2will induce a transition operator Twhich, by Theorem 2.5.3 in\\nLatouche and Ramaswami [ 16], will almost-surely reach its absorbing state corre-\\nsponding to τ2. This path can then be ﬁlled in according to Algorithm 3. The pre-\\ncision βincreases at each recursion to terminate with shorter paths. We denote\\nthe induced joint distribution as\\np(f,w,β;τ−,τ+)=p(f|β,w;τ−,τ+)p(w)p(β). (3)\\nHaving a probabilistic generative model over operations in the free operad\\nover a signature, we now need a way to specify a structure learning problem.\\nDeﬁnition 5provides this by specifying what paths to sample (each box speciﬁes\\na call to Algorithm 2) and how to compose them.',\n",
       "  'Computing with Categories in Machine Learning 249\\nData: hypergraph ( V,E), generators M, recursion sites R\\nfunction Generator( e, β, w)\\ngs←{m∈M|(dom( m),cod(m)) = (dom( e),cod(e))};\\ngs←gs∪{ ⊗[(d1,c1),...,(dk,c k)]∈R|⨂\\ni∈[1...k ]di=\\ndom(e)∧⨂\\ni∈[1...k ]ci=c o d ( e)};\\nforeach j∈{1,...,|gs|}do\\nifgs j=⊗(...)then\\nwj←wj/β;\\nend\\nend\\nwe←[wn|g∈gs,g∈M,n=index (g,M)];\\nwe←we+[w|M|+n|g∈gs,g∈R,n=index (g,R)];\\nj∼Discrete( we);\\nifgs j=⊗[(d1,c1),...,(dk,c k)]then\\nreturn⨂k\\nl=1Path( dl,c l,β+1,w)\\nelse\\nreturn gs j\\nend\\nend\\nAlgorithm 3: Filling in an edge in the path with a morphism\\nDeﬁnition 5 (Wiring diagram). An acyclic, O-typed wiring diagram [7,\\n22] is a map from a series of internal boxes , each one deﬁned by its domain\\nand codomain pair (τ−\\ni,τ+\\ni)to an outer box deﬁned by domain and codomain\\n(τ−\\nn,τ+\\nn)\\nΦ:OS(τ−\\n1,τ+\\n1)×...×O S(τ−\\nn−1,τ+\\nn−1)→O S(τ−\\nn,τ+\\nn).\\nAcyclicity requires that connections (“wires”) can extend only from the outer',\n",
       "  'box’s domain to the domains of inner boxes, from the inner boxes codomains\\nto the outer box’s codomain, and between internal boxes such that no cycles areformed in the directed graph of connections between inner boxes.\\nGiven a user-speciﬁed wiring diagram Φ, we can then wite the complete prior\\ndistribution over all latent variables in our generative model.\\np(f,w,β;Φ,S)=p(β)p(w)∏\\n(τ−\\ni,τ+\\ni)∈Φp(fi|β,w;τ−\\ni,τ+\\ni). (4)\\nIf a user provides a likelihood pθ(x,z|f) relating the learned structure fto\\ndata x(via latents z) we will have a joint density\\np(x,z,f,w,β;Φ,S)=p(x|f)p(f,w,β;Φ,S), (5)\\nand Eq. 5then admits inference from the data xby Bayesian inversion\\np(z,f,w,β|x;Φ,S)=p(x,z,f,w,β;Φ,S)\\npθ(x;Φ,S). (6)',\n",
       "  '250 E. Sennesh et al.\\nSection 2.1will explain how to approximate Eq. 6by stochastic gradient-based\\noptimization, yielding a maximum-likelihood estimate of θand an optimal\\napproximation for the parametric family φto the true Bayesian inverse.\\n2.1 Model Learning and Variational Bayesian Inference\\nBayesian inversion relies on evaluating the model evidence pθ(x;Φ,S), which\\ntypically has no closed form solution. However, we can transform the high-dimensional integral over the joint density into an expectation\\np\\nθ(x;Φ,S)=∫\\npθ(x,z,fθ,w,β;Φ,S)dzdfθdwdβ\\n=Ep(fθ,w,β;Φ,S)[pθ(x,z,fθ,w,β;Φ,S)],\\nand then rewrite that expectation into one over the proposal\\nEp(fθ,w,β;Φ,S)[pθ(x,z,fθ,w,β;Φ,S)] =\\nEqφ(z,fθ,w,β|x;Φ,S)[pθ(x,z,fθ,w,β;Φ,S)\\nqφ(z,fθ,w,β|x;Φ,S)]\\n.\\nFor constructing this expectation, DisCoPyro provides both the functorial inver-\\nsion described in Sect. 3.1and an amortized form of Automatic Structured Vari-\\national Inference [ 1] suitable for any universal probabilistic program.',\n",
       "  'Jensen’s Inequality says that expectation of the log density ratio will lower-\\nbound the log expected density ratio\\nEqφ(z,fθ,w,β|x;Φ,S)[\\nlogpθ(x,z,fθ,w,β;Φ,S)\\nqφ(z,fθ,w,β|x;Φ,S)]\\n≤\\nlog Ep(fθ,w,β;Φ,S)[pθ(x,z,fθ,w,β;Φ,S)],\\nso that the left-hand side provides a lower bound to the true model evidence\\nL(θ,φ)= Eqφ(z,fθ,w,β|x;Φ,S)[\\nlogpθ(x,z,fθ,w,β;Φ,S)\\nqφ(z,fθ,w,β|x;Φ,S)]\\n≤logpθ(x;Φ,S).\\nMaximizing this evidence lower bound (ELBO) by Monte Carlo estimation of its\\nvalues and gradients (using Pyro’s built-in gradient estimators) will estimate the\\nmodel parameters θby maximum likelihood and train the proposal parameters\\nφto approximate the Bayesian inverse (Eq. 6)[12].\\n3 Example Application and Training\\nThe framework of connecting a morphism to data via a likelihood with inter-\\nmediate latent random variables allows for a broad variety of applications. This',\n",
       "  'section will demonstrate the resulting capabilities of the DisCoPyro framework.Section 3.1will describe an example application of the framework to deep proba-\\nbilistic program learning for generative modeling. Section 3.2that describe appli-\\ncation’s performance as a generative model.',\n",
       "  'Computing with Categories in Machine Learning 251\\nTable 1. Average log-evidence on the Omniglot evaluation set across models. Our free\\noperad model obtains the highest higher log-evidence per data dimension.\\nModel Image Size Learns Structure log-ˆZ/dim\\nSequential Attention [ 19] 28×28 ✗ −0.1218\\nVariational Homoencoder [ 11] (PixelCNN) 28×28 ✗ −0.0780\\nGraph VAE [ 9] 28×28 ✓ −0.1334\\nGenerative Neurosymbolic [ 5] 105×105 ✓ −0.0348\\nFree Operad DGM (ours) 28×28 ✓ −0.0148\\n3.1 Deep Probabilistic Program Learning with DisCoPyro\\nAs a demonstrative experiment, we constructed an operad Owhose generators\\nimplemented Pyro building blocks for deep generative models fθ(taken from\\nwork on structured variational autoencoders [ 12,19,24]) with parameters θ.W e\\nthen speciﬁed the one-box wiring diagram Φ:(I,R28×28)→(I,R28×28)t o\\nparameterize the DisCoPyro generative model. We trained the resulting freeoperad model on MNIST just to check if it worked, and on the downsampled',\n",
       "  '(28×28) Omniglot dataset for few-shot learning [ 14] as a challenge. Since the\\ndata x∈R\\n28×28, our experimental setup induces the joint likelihood\\npθ(x|z,fθ)=N(μθ(z,fθ),Iτ)\\npθ(x,z|fθ)=pθ(x|z,fθ)pθ(z|fθ).\\nDisCoPyro provides amortized variational inference over its own random vari-\\nables via neural proposals qφfor the “conﬁdence” β∼qφ(β|x) and the “prefer-\\nences” over generators w∼qφ(w|x). Running the core DisCoPyro generative\\nmodel over structures fθthen gives a proposal over morphisms in the free operad,\\nproviding a generic proposal for DisCoPyro’s latent variables\\nqφ(fθ,w,β|x;Φ,S)=p(fθ|w,β;Φ,S)qφ(β|x)qφ(w|x).\\nSince the morphisms in our example application are components of deep gener-\\native models, each generating morphism can be simply “ﬂipped on its head” to\\nget a corresponding neural network design for a proposal. We specify that pro-posal as q\\nφ(z|x,fθ); it constructs a faithful inverse [ 23] compositionally via a',\n",
       "  'dagger functor (for further description of Bayesian inversion as a dagger functor,\\nplease see Fritz [ 8]). Our application then has a complete proposal density\\nqφ(z,fθ,w,β|x;Φ,S)=qφ(z|fθ,x)qφ(fθ,w,β|x;Φ,S). (7)\\n3.2 Experimental Results and Performance Comparison\\nTable 1compares our free operad model’s performance to other structured deep\\ngenerative models. We report the estimated log model evidence. Our free operad',\n",
       "  '252 E. Sennesh et al.\\n(a) Omniglot characters (above) and\\ntheir reconstructions (below)\\n(b) A string diagram sampled from the\\nfree operad model’s Bayesian inverse.\\nFig. 2. Reconstructions (left) generated by inference in the diagrammatic generative\\nmodel (right) on handwritten characters in the Omniglot evaluation set. The string\\ndiagram shows a model that generates a glimpse, decodes it into an image canvas via\\na variational ladder decoder, and then performs a simpler process to generate anotherglimpse and insert it into the canvas.\\nprior over deep generative models achieves the best log-evidence per data dimen-\\nsion, although standard deviations for the baselines do not appear to be availablefor comparison. Some of the older baselines, such as the sequential attention\\nmodel and the variational homoencoder, ﬁx a composition structure ahead of\\ntime instead of learning it from data as we do. Figure 2shows samples from',\n",
       "  'the trained model’s posterior distribution, including reconstruction of evalua-\\ntion data (Fig. 2a) and an example structure for that data (Fig. 2b).\\nHistorically, Lake [ 14] proposed the Omniglot dataset to challenge the\\nmachine learning community to achieve human-like concept learning by learn-\\ning a single generative model from very few examples; the Omniglot challenge\\nrequires that a model be usable for classiﬁcation, latent feature recognition, con-cept generation from a type, and exemplar generation of a concept. The deep\\ngenerative models research community has focused on producing models capa-\\nble of few-shot reconstruction of unseen characters. [ 11,19] ﬁxed as constant the\\nmodel architecture, attempting to account for the compositional structure in\\nthe data with static dimensionality. In contrast, [ 5,9] performed joint structure\\nlearning, latent variable inference, and data reconstruction as we did.\\n4 Discussion',\n",
       "  'This paper described the DisCoPyro system for generative Bayesian structurelearning, along with its variational inference training procedures and an exampleapplication. Section 2described DisCoPyro’s mathematical foundations in cate-\\ngory theory, operad theory, and variational Bayesian inference. Section 3showed\\nDisCoPyro to be competitive against other models on a challenge dataset.\\nAs Lake [ 15] suggested, (deep) probabilistic programs can model human\\nintelligence across more domains than handwritten characters. Beyond pro-\\ngrams, neural network architectures, or triangulable manifolds, investigators',\n",
       "  'Computing with Categories in Machine Learning 253\\nhave applied operads and SMCs to chemical reaction networks, natural lan-\\nguage processing, and the systematicity of human intelligence [ 3,18]. This broad\\nvariety of applications motivates our interest in representing the problems a gen-\\nerally intelligent agent must solve in terms of operadic structures, and learning\\nthose structures jointly with their contents from data.\\nAcknowledgements. The authors would like to thank the anonymous reviewers for\\ntheir constructive feedback and encouragement. This work was supported by the Japan\\nScience and Technology Agency (JST JPMJMS2033) and National Science Foundation\\nof the United States of America (NSF 2047253).\\nReferences\\n1. Ambrogioni, L., et al.: Automatic structured variational inference. In: Proc. AIS-\\nTATS, pp. 676–684 (2021)\\n2. Bingham, E., et al.: Pyro: deep universal probabilistic programming. JMLR 20,\\n973–978 (2019)',\n",
       "  '3. Bradley, T.D.: What is applied category theory? arXiv preprint arXiv:1809.05923\\n(2018)\\n4. Coecke, B., Sadrzadeh, M., Clark, S.: Mathematical foundations for a composi-\\ntional distributional model of meaning. arXiv preprint arXiv:1003.4394 (2010)\\n5. Feinman, R., Lake, B.M.: Learning task-general representations with generative\\nneuro-symbolic modeling. In: Proc. ICLR (2021)\\n6. de Felice, G., Toumi, A., Coecke, B.: DisCoPy: monoidal categories in python. In:\\nApplied Category Theory Conference, pp. 1–20. EPTCS (2020). http://arxiv.org/\\nabs/2005.02975 arXiv: 2005.02975\\n7. Fong, B., Spivak, D.I.: Seven Sketches in Compositionality: An Invitation to\\nApplied Category Theory. Cambridge University Press, Cambridge (2019)\\n8. Fritz, T.: A synthetic approach to Markov kernels, conditional independence and\\ntheorems on suﬃcient statistics. Adv. Math. 370, 107239 (2020)\\n9. He, J., et al.: Variational autoencoders with jointly optimized latent dependency',\n",
       "  'structure. In: Proc. ICLR 2019, pp. 1–16 (2019)\\n10. Hermida, C., Makkaiy, M., Power, J.: Higher dimensional multigraphs. In: Pro-\\nceedings - Symposium on Logic in Computer Science, pp. 199–206 (1998)\\n11. Hewitt, L.B., et al.: The variational homoencoder: learning to learn high capacity\\ngenerative models from few examples. Proc. UAI 2018, 988–997 (2018)\\n12. Kingma, D.P., Welling, M.: Auto-encoding variational bayes. arXiv preprint\\narXiv:1312.6114 (2013)\\n13. Kirsch, L., Kunze, J., Barber, D.: Modular networks: learning to decompose neural\\ncomputation. In: Proc. NeurIPS 2018, pp. 2408–2418 (2018)\\n14. Lake, B.M., Salakhutdinov, R., Tenenbaum, J.B.: The Omniglot challenge: a 3-year\\nprogress report. Curr. Opin. Behav. Sci. 29, 97–104 (2019)\\n15. Lake, B.M., Ullman, T.D., Tenenbaum, J.B., Gershman, S.J.: Building machines\\nthat learn and think like people. Behav. Brain Sci. 40, e253 (2017)\\n16. Latouche, G., Ramaswami, V.: Introduction to matrix analytic methods in stochas-',\n",
       "  'tic modeling. In: Society for Industrial and Applied Mathematics (1999)\\n17. Milnor, J.: On spaces having the homotopy type of CW-complex. Trans. Am.\\nMath. Soc. 90, 272–280 (1959)',\n",
       "  '254 E. Sennesh et al.\\n18. Phillips, S., Wilson, W.H.: Categorial compositionality: a category theory expla-\\nnation for the systematicity of human cognition. PLoS Comput. Biol. 6, e1000858\\n(2010)\\n19. Rezende, D.J., et al.: One-shot generalization in deep generative models. In: Inter-\\nnational Conference on Machine Learning, vol. 48 (2016)\\n20. Rosenbaum, C., Klinger, T., Riemer, M.: Routing networks: adaptive selection of\\nnon-linear functions for multi-task learning. In: Proc. ICLR 2018, pp. 1–10 (2018)\\n21. Shiebler, D., Gavranovi´ c, B., Wilson, P.: Category theory in machine learning. In:\\nProc. ACT 2021 (2021)\\n22. Spivak, D.I.: The operad of wiring diagrams: formalizing a graphical language for\\ndatabases, recursion, and plug-and-play circuits (2013). arxiv: 1305.0297\\n23. Webb, S., et al.: Faithful inversion of generative models for eﬀective amortized\\ninference. In: Proc. NIPS 2018, pp. 3074–3084 (2018)',\n",
       "  '24. Zhao, S., Song, J., Ermon, S.: Learning hierarchical features from generative mod-\\nels. In: International Conference on Machine Learning (2017)',\n",
       "  'ADAM: A Prototype of Hierarchical\\nNeuro-Symbolic AGI\\nSergey Shumsky(B)and Oleg Baskov\\nMoscow Institute of Physics and Technology,\\nDolgoprudny 141701, Russian Federation\\nserge.shumsky@gmail.com\\nhttps://mipt.ru/\\nAbstract. Intelligent agents are characterized primarily by their far-\\nsighted expedient behavior. We present a working prototype of an intelli-\\ngent agent (ADAM) based on a novel hierarchical neuro-symbolic archi-\\ntecture (Deep Control) for deep reinforcement learning with a potentiallyunlimited planning horizon. The control parameters form a hierarchy\\nof formal languages, where higher-level alphabets contain the semantic\\nmeanings of lower-level vocabularies.\\nKeywords: Artiﬁcial General Intelligence\\n·Hierarchical reinforcement\\nlearning ·Neuro-symbolic architecture\\n1 Introduction\\nArtiﬁcial General Intelligence (AGI) aims to create intelligent agents capable\\nof planning expedient behavior. The larger the planning horizon, the stronger',\n",
       "  'the intelligence of agents. We present an early AGI prototype with a new hie-rarchical neuro-symbolic architecture in which the planning horizon increases\\nexponentially with the number of levels. The article is structured as follows.\\nSection 2introduces our neuro-symbolic approach and formulates reinforce-\\nment learning (RL) as a search for the best sequences of discrete cognitive states\\nthat maximize reward over some planning horizon. We extend the original alpha-\\nbet of cognitive states with such sequences, called mental states , thus forming\\na vocabulary of words of some formal mental language , deﬁned by its grammar\\nrules. The semantic meaning of these words is determined by the correspondingMarkov matrix learned from experience. Such a language allows an intelligent\\nagent to plan its behavior several steps ahead.\\nTo achieve a potentially unlimited planning horizon without a combinatorial\\nexplosion, we propose a hierarchical neuro-symbolic architecture Deep Control,',\n",
       "  'in which the cognitive states of the upper levels represent the semantic meaning\\nof the mental states of the lower ones.\\nSection 3presents an early AGI prototype ADAM, based on the proposed\\narchitecture. It takes a closer look at the proposed architecture, describing its\\nmain components and how they interact with each other.\\nc⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2023\\nP. Hammer et al. (Eds.): AGI 2023, LNAI 13921, pp. 255–264, 2023.https://doi.org/10.1007/978-3-031-33469-6\\n_26',\n",
       "  '256 S. Shumsky and O. Baskov\\nSection 4compares the current work with alternate approaches to AGI and\\nthe ﬁnal Sect. 5concludes the paper.\\n2 Deep Control: The Architecture of Intelligence\\nThis section shows that strong AGI with unlimited planning horizon implies hie-\\nrarchical RL and proposes a Deep Control architecture as a stack of elementary\\ncontrollers similar to the stack of elementary mappings in deep neural networks.\\n2.1 Reinforcement Learning as a Mental Language\\nIn RL the goal of an agent is to maximize the total reward it can receive for\\na given planning horizon T. Let the agent’s mind be characterized by a set\\nof discrete mental states {m}that depend on its sensors and actuators. (For\\nexample: “turn right to see the park”, “turn left to see the grocery store”, etc.)\\nMaximal total reward starting from the given mental state mtwith the expected\\nreward R(mt) learned from experience and following the optimal strategy πis\\ndeﬁned by the value function :\\nQ(mt)=R(mt) + max\\nπT−1∑\\nτ=1∑',\n",
       "  'mt+τR(mt+τ)pπ(mt+τ|mt). (1)\\nHere we constrain our agent’s mind to a Markov Decision Process (MDP), where:\\npπ(mt+τ|mt)=∑\\nmt+τ−1pπ(mt+τ|mt+τ−1)pπ(mt+τ−1|mt). (2)\\nThe optimal deterministic strategy π∗of such an agent is:\\n˜mt= arg max\\nmtQ(mt)pπ∗(mt|mt−1). (3)\\nSince the actual mental state mtdoes not in general coincide with the expected\\none˜mt, but deﬁnitely depends on it, the transition probabilities pπ(mt|mt−1)\\ndepend on the decision process and thus – on the value function. Value function\\n(1) in its turn depends on the transition probabilities. This feedback makes it\\ndiﬃcult to calculate the value function explicitly. Fortunately one can learn itfrom experience making use of a Bellman equation following from ( 1,2):\\nQ(m\\nt)=R(mt)+γmax\\nmt+1pπ∗(mt+1|mt)Q(mt+1), (4)\\nwith a discount parameter γ=(T−1)/T. For a true Q-function, the left and\\nright sides of ( 4) must be equal. Thus, the solution can be found using the\\nfollowing iterative procedure:\\nQ(mt)←Q(mt)+α(t)[\\nR(mt)+γmax',\n",
       "  'mt+1pπ(mt+1|mt)Q(mt+1)−Q(mt)]\\n(5)',\n",
       "  'ADAM: A Prototype of Hierarchical Neuro-Symbolic AGI 257\\nwith (gradually reducing) learning rate α(t)≪1. The reward and transition\\nmodels are also updated iteratively based on real data.\\nSuch Q-learning is attractive because of its simplicity. However, it is\\nextremely ineﬃcient for large planning horizons. The optimal Q-learning algo-\\nrithm is proved to converge in O(T3) iterations [ 23]. This cubic dependence is\\na signiﬁcant limitation. It takes millions of iterations to learn how to achieve a\\ngoal on a horizon of 100 time steps.\\nThat is why we limit the planning horizon to a modest T≲10 to ﬁnd Q(m)\\nin just a few hundred iterations, but allow the formation of new mental states as\\nthe most frequent combinations of already known ones. These combinations are\\nimplicitly encoded in the transition matrix pπ(m′|m). To ﬁnd them explicitly,\\nwe expand the original mental alphabet {m0}≡{ s}along with the appropriate\\nexpansion of the transition matrix.',\n",
       "  'Namely, transition probabilities are deﬁned by the number of observations of\\nsuccessive state pairs Cmm′:\\npπ(m′|m)=Cmm′/∑\\nm′Cmm′. (6)\\nMental states are found recursively during Q-learning by merging the most fre-\\nquently occurring pairs of existing ones: when Cm′m′′exceeds a certain threshold,\\na new mental state is deﬁned as the concatenation of the corresponding pair:\\nm←m′m′′,C m′m′′>C 0. (7)\\nThese rules deﬁne a formal language – the set of all valid strings of cognitive\\nstates s(characters of that language): {m}={s1...sτ}. We’ll call this set\\nmental language .\\nThe rules of mental language ( 7) allow the agent to represent the input\\nsequence of cognitive states as a shorter sequence of their typical combinations\\nby construction of a binary tree of merging ( parsing ). Such an agent is capable\\nof perceiving and predicting temporal structures. Indeed, the optimal strategy(3) can now choose sequences of action-states. This allows the agent to plan its',\n",
       "  'behavior several steps ahead even in the current MDP setting. (For example:\\n“turn right to see the park, then go for a morning jog, then head back home”.)\\nThe view of reinforcement learning described above relates the latter to tra-\\nditional logic-based AI. In fact, mental language ( 7), together with the decision-\\nmaking process ( 3), endows the agent with formal mental logic – the ability to\\ndeduce new mental states as logical consequences of previous ones. Reinforce-\\nment learning improves the predictive and logical abilities of an agent.\\nLogical thinking is closely related to the semantics of the language: both are\\ndetermined by transition probabilities. Indeed the semantic meaning of words\\nin the language is determined by the context in which they appear – recall\\nWittgenstein’s famous “meaning is use”. In MDP setting semantics depends on\\nthe transition probabilities p\\nπ(m′|m)a n d p′\\nπ(m|m′), deﬁned by normalized rows',\n",
       "  'and columns of the same empirical matrix Cmm′. That is, each mental state has\\na corresponding semantic vector xm=pπ(...|m)p′\\nπ(...|m) – concatenation of\\nright and left context probabilities.',\n",
       "  '258 S. Shumsky and O. Baskov\\n‘Synonymic’ mental states with similar semantic vectors appear interchange-\\nably in similar situations and can be considered as diﬀerent implementations ofthe same step of some higher level plan, formulated in a more abstract meta-\\nlanguage , which can also be learned as described above. In fact, the agent can\\nlearn the hierarchy of metalanguages, thus paving the way for AGI. We calledthis architecture Deep Control [ 20].\\n2.2 Deep Control: Scale-Free Thinking\\nIn the so-called Deep RL, the function Q(a|s), the value of the action ain state\\ns, is represented by a deep neural network. That is, in Deep RL, the hierarchy\\nof neural representations is used to determine only the agent’s next step. In\\norder to plan behavior a few steps ahead, one needs to generate a bunch of fairly\\ngood trajectories and choose the best one, for example, using a Monte Carlo\\nTree Search, as in AlphaZero, MuZero and the like. This leads to combinatorial',\n",
       "  'explosion, that limits the planning horizon of modern Deep RL, as shown inFig.1(left).\\nFig. 1. Deep RL (left) uses neural networks for individual decision making, augmented\\nby Monte Carlo Tree Search. Deep Control (right) maintains a hierarchy of plans\\ni n s c r i b e di ne a c ho t h e r .\\nOn the contrary, in Deep Control mental states form a hierarchy of nested\\nshort-term plans ( ml→˜ml), where each step of the higher-level plan corre-\\nsponds to a set of its possible realizations one level lower. Each hierarchical level\\nlooks only one mental state ahead using its Markov probability matrix p(m′\\nl|ml),\\nthus avoiding combinatorial problems.\\nA hierarchy of mental states and their supposed successors form a hierarchy of\\nplans inscribed in each other as illustrated in Fig. 1(right). As soon as enough\\ntraining data accumulates at the top level, the next level is generated, whichcontrols the behavior on an even larger time scale. With a top-level planning',\n",
       "  'horizon growing exponentially with the number of levels, Deep Control can create\\nextremely far-sighted plans without a combinatorial explosion.',\n",
       "  'ADAM: A Prototype of Hierarchical Neuro-Symbolic AGI 259\\n3 ADAM: Implementing Intelligence\\nThis section discusses Deep Control in more detail and presents an early AGI\\nprototype ADAM (Adaptive Deep Autonomous Machine) solving the hierarchi-\\ncal RL problem with a potentially unlimited planning horizon.\\n3.1 ADAM’s Design\\nThe Deep Control architecture implements a stack of simple controllers that\\ncontrol behavior across diﬀerent timescales. All layers are similar, taking as an\\ninput analogue representation of current mental state from the previous layer and\\nproviding a prediction of the next mental states, corresponding to the proposedplan of behavior as illustrated in Fig. 2.\\nFig. 2. Hierarchical neuro-symbolic architecture of ADAM.\\nThe ﬁrst layer interacts with the world. It operates with sensorimotor vectors,\\nits output being the proposed action and the predicted next sensory value. Asis customary in RL, the sensorimotor vector contains a special reinforcement',\n",
       "  'component r, signaling how well ADAM is doing its job. Each layer operates\\nwith its own discrete set of mental states representing the most valuable chainsof discrete symbols from the layer’s alphabet: m\\nl=s1\\nl...sτ\\nl(τ≤T). In the\\nrest of this section, we describe ADAM’s modules and the interaction between\\nits hierarchical levels.',\n",
       "  '260 S. Shumsky and O. Baskov\\nCentral to Deep Control is the Semantic memory Cmm′counting obser-\\nvations of consecutive pairs of mental states. Semantic memory allows to: (i)predict the next mental state to plan behavior: m→˜m, and (ii) encode the\\ncurrent mental state as input to the next layer: m→x\\nm.\\nThe semantic encoding of mental states is similar to the semantic represen-\\ntation of words in NLP [ 12]. Namely, the semantic vector of a mental state m\\ndetermines in what contexts the latter appears. Semantic vector xmis composed\\nof corresponding normalized row and column of the Semantic memory matrix.\\nThe Encoder maps the analog input vector xto discrete multidimensional\\nsymbol srepresenting cognitive state: x→s=(s1,...,s H), using Hdiﬀerent\\nclusterings of the input space, each of which divides the input space into K\\ndiﬀerent clusters. The number of unique discrete codes labeling diﬀerent regions\\nof the input space is KH. We say that such an Encoder have Hheads, the latter',\n",
       "  'playing the same role, as in modern Transformer neural networks – each head\\npays attention to a diﬀerent aspect of the data.\\nThe stream of symbols from the Encoder is fed onto the ﬁxed-length stack\\nof the Parser , where they are combined into chunks (mental states) using Byte\\nPair Encoding algorithm [ 6], recursively merging the most frequently occurring\\npairs of mental states in the Parser stack. Merging two mental states frees upspace in the stack for the next symbol. The resulting mental states m=s\\n1...sτ\\nrepresent typical ADAM behavior patterns.\\nEach head of the Encoder maps input vector into a character from its own\\nalphabet . The Parser combines them into ever-growing dictionary of the most fre-\\nquently occurring character strings. The Semantic memory Ch\\nmhm′\\nhincreases its\\nvalue each time the strings mhandm′\\nhin the head hbelong to the winning pair in\\nthe Parser stack with the strongest connection c(m,m′)=∑\\nhln(\\n1+Ch\\nmhm′\\nh)\\n:\\nCh\\nmhm′\\nh←Ch\\nmhm′\\nh+1,(h=1,...,H ). (8)',\n",
       "  'When some Ch\\nmm′exceeds the speciﬁed threshold, a new combined string appears\\nin the head’s dictionary: mnew←mm′. Gradually, the Parser learns to identify\\never larger strings by recursively combining the most frequent pairs of shorter\\nstrings, starting with single characters.\\nThe Parser recursively attempt to merge the winner pair in the Parser stack,\\nupdating statistics of the winning pair according to ( 8). In the case of a merge,\\nthe rewards of merged mental state are summed up and Parser is ready to processthe next symbol. If the merge is not possible because some of the combined\\nstrings do not yet exist in the heads dictionaries, the leftmost mental state\\nmoves from the Parser stack to the Encoder of the next layer, thereby freeingup space for processing the next symbol.\\nEach act of merging is accompanied by reinforcement learning: updating\\nobservation counter n\\nh(mh), average reward Rh(mh) and value function Qh(mh)',\n",
       "  'ADAM: A Prototype of Hierarchical Neuro-Symbolic AGI 261\\nof the newly formed mental state mand all others to the left of it. Using SARSA\\nalgorithm for a pair of consecutive mental states ( m,m′), we get:\\nn(m)←n(m)+1,\\nR(m)←R(m)+[r(m)−R(m)]/n(m),\\nQ(m)←Q(m)+α[r(m)+γQ(m′)−Q(m)],\\nwith vector notation for parallel updates in all heads.\\nThe Planner aims to propose the optimal course of action in the given\\ncircumstances, i.e. predict the optimal next mental state m→˜m. Since cir-\\ncumstances are constantly changing, previous plans are also subject to constant\\nadjustment.\\nThe development and correction of plans proceeds from top to bottom – from\\nthe most general intentions to more and more detailed plans. The main problem\\nis to harmonize the plans of diﬀerent levels. To this end Planners of all levels\\nuse the same algorithm. Namely, each time the Parser updates its stack, thePlanner:\\n– replenishes its stack of plans (if needed);\\n– validates these plans against the new state of the Parser stack;',\n",
       "  '– selects valid plan with the highest expected value for execution.\\nThe top-level Planner replenishes its stack with new plans when a new men-\\ntal state mappears in its Parser stack. Lower level Planners receive plans for\\nexecution through higher level Decoders.\\nSince a new symbol appears in the Parser stack at each step of its work, all\\nprevious plans that do not correspond to this symbol lose their relevance, as\\nthey do not correspond to the current state of aﬀairs. Accordingly, these plans\\nare eliminated during the validation process, and only plans that are relevant inthe current situation survive.\\nFinally, the actual plan with the highest value is selected, which is passed\\nfor execution to the lower level. This way of hierarchical planning combines theability to plan for a (very) long period of time and adapt such far-reaching plans\\nto constantly changing conditions.\\nThe Decoder translates the plan selected by the Planner to a lower layer for',\n",
       "  'execution. Fast decoding uses a look-up memory of all low-level implementations\\nfor all known mental states of the current level, being diﬀerent realizations of the\\nsame plan. If fast decoding fails (in an unknown situation), the decoder choosesthe plan for which the maximum number of heads voted.\\nThe Episodic memory is used by the Decoder. At the top level it is also\\nused to build the next layer when it has collected enough data for the clusteringalgorithm.\\n3.2 ADAM’s Prospects\\nThe ADAM project started in early 2020. To date, the preliminary version of\\nADAM has been developed. The source code is in fast and easy-to-develop',\n",
       "  '262 S. Shumsky and O. Baskov\\nJulia language. The single layer version has been tested on classic RL prob-\\nlems (MountainCar, CartPole, etc.). We are now testing multi-layered versionby training language models, since we can interpret the corresponding language\\nstructures: the ﬁrst layer learns words as chains of characters, the second layer\\nlearns phrases, the third layer – sentences, and so on. The results will be pre-sented in a separate article.\\nHowever it is clear that our language models are much cheaper, than that\\nbased on deep neural networks. Namely, ADAM on a single CPU learns languageat a rate of about 1 GB/hour regardless of the number of layers. Indeed, learning\\neach next layer is several times faster, than the previous one, since the size of\\ntraining set shrinks with each hierarchical level. (The number of words in a dataset is several times less than the number of characters, etc.). Thus, ADAM can',\n",
       "  'master Wikipedia in just one day and the GPT-3 training set of 1,2 TB – in 50\\ndays without any supercomputing power, due to its neuro-symbolic architecture\\n(manipulating symbols, rather than multidimensional vectors).\\nMoreover, having a value function at the heart of its architecture, ADAM\\ncan easily learn conversational behavior, unlike ChatGPT and its ilk, which use\\nreinforcement learning based on human feedback to ﬁne-tune pretrained large\\nlanguage models.\\n4 Related Work\\nHierarchical architecture has been widely discussed as a proposed brain model byKarl Friston with co-authors [ 5,15,16]. Their model assumes that each cognitive\\nstate of a higher hierarchical level originates a trajectory of cognitive states of a\\nlower level, generated by the corresponding Markov matrix. Our approach diﬀersin that we use a transition matrix between mental states rather than cognitive\\nstates. That is, we learn transitions between trajectories, and not between indi-',\n",
       "  'vidual states. We are also more focused on ﬁnding practical learning algorithmsfor AGI.\\nSuch a practical approach is typical for Deep RL, in particular, for Hierar-\\nchical RL. The problem here is that deep neural networks only solve part ofthe problem, namely Q-function approximation. On the contrary, Deep Control\\nsolves the problem of predictive control in its entirety with the same basic app-\\nroach: layers of weak learners. The diﬀerence is that deep neural networks learn ahierarchy of increasingly abstract representations for data approximation, while\\nDeep Control learns a hierarchy of increasingly abstract control parameters for\\npredictive control. The resulting multi-layer controller can handle increasingly\\ncomplex behavior as the number of layers increases.\\nAlthough Hierarchical RL has not yet been solved, many such attempts have\\nbeen made, including [ 1–4,22] to name just a few. See [ 14] for comprehensive',\n",
       "  'review. However, in practical terms, the vast majority of work is limited to\\nonly two levels of hierarchy, since even in two-level systems there are a lot ofdiﬃculties. Simultaneous training of layers turns out to be unstable, and it takes\\na lot of eﬀort to eliminate these instabilities [ 11,13,24].',\n",
       "  'ADAM: A Prototype of Hierarchical Neuro-Symbolic AGI 263\\nThe closest neural network analogue of Deep Control seems to be the Feyn-\\nman machine [ 10]. The latter also solves the predictive control problem using\\nhierarchical design with bidirectional information ﬂow and local learning rules.\\nDeep Control diﬀers in that it uses a neuro-symbolic approach rather than a\\npurely analog one. It allows Deep Control to model symbolic thinking and lan-guage acquisition in addition to behavior control [ 19,21].\\nThe proposed symbolic thinking is easy to interpret: at any given moment,\\neach layer keeps track of the current context m\\nland executes a certain plan ˜ml,\\nrepresented by the corresponding mental state from its dictionary. The mental\\nstates mlmapped to the same cognitive state of the higher level sl+1are diﬀerent\\nrealizations of a certain step of the next layer’s plan. In this respect, Deep Con-trol resembles traditional rule-based cognitive architectures that mimic symbolic',\n",
       "  'thinking [ 7–9,17]. But the latter are incapable of learning hierarchical planning.\\nAccording to Stuart Russell: “At present all existing methods for hierarchical\\nplanning rely on a human-generated hierarchy of abstract and concrete actions.\\nWe do not yet understand how such hierarchies can be learned from experience”[18]. Deep Control oﬀers just that – to learn such hierarchies from experience.\\n5 Conclusion\\nThis paper presents a prototype of a novel hierarchical neuro-symbolic AGI\\narchitecture.\\nWe consider reinforcement learning as the construction of a formal language\\nwhose rules deﬁne useful behavior as a combination of the basic symbols of the\\nlanguage. The semantics of this language makes it possible to deﬁne a metalan-\\nguage and, thus, a whole hierarchy of metalanguages that provide the hierar-chical control of complex adaptive behavior. Such a hierarchical neuro-symbolic\\narchitecture (Deep Control) models the symbolic thinking of rational intelligent',\n",
       "  'agents, the hallmark of AGI.\\nWe present an early prototype of neuro-symbolic AGI (ADAM) that learns\\nto plan and control expedient behavior with an ever-increasing number of hie-\\nrarchical levels and an exponentially growing planning horizon.\\nReferences\\n1. Bakker, B., Schmidhuber, J., et al.: Hierarchical reinforcement learning based on\\nsubgoal discovery and subpolicy specialization. In: Proceedings of the 8-th Confer-ence on Intelligent Autonomous Systems, pp. 438–445 (2004)\\n2. Barto, A.G., Mahadevan, S.: Recent advances in hierarchical reinforcement learn-\\ning. Discret. Event Dyn. Syst. 13(1–2), 41–77 (2003)\\n3. Botvinick, M.M.: Hierarchical reinforcement learning and decision making. Curr.\\nOpin. Neurobiol. 22(6), 956–962 (2012)\\n4. Dietterich, T.G., et al.: The MAXQ method for hierarchical reinforcement learning.\\nIn: ICML, vol. 98, pp. 118–126 (1998)',\n",
       "  '264 S. Shumsky and O. Baskov\\n5. Friston, K.J., Parr, T., Yuﬁk, Y., Sajid, N., Price, C.J., Holmes, E.: Generative\\nmodels, linguistic communication and active inference. Neurosci. Biobehav. Rev.\\n118, 42–64 (2020)\\n6. Gage, P.: A new algorithm for data compression. C Users J. 12(2), 23–38 (1994)\\n7. Kotseruba, I., Tsotsos, J.K.: 40 years of cognitive architectures: core cognitive\\nabilities and practical applications. Artif. Intell. Rev. 53(1), 17–94 (2020)\\n8. Laird, J.E., Lebiere, C., Rosenbloom, P.S.: A standard model of the mind: toward\\na common computational framework across artiﬁcial intelligence, cognitive science,\\nneuroscience, and robotics. AI Mag. 38(4), 13–26 (2017)\\n9. Langley, P., Laird, J.E., Rogers, S.: Cognitive architectures: research issues and\\nchallenges. Cogn. Syst. Res. 10(2), 141–160 (2009)\\n10. Laukien, E., Crowder, R., Byrne, F.: Feynman machine: the universal dynamical\\nsystems computer (2016). arXiv preprint arXiv:1609.03971',\n",
       "  '11. Levy, A., Platt, R., Saenko, K.: Hierarchical reinforcement learning with hindsight\\n(2018). arXiv preprint arXiv:1805.08180\\n12. Levy, O., Goldberg, Y.: Neural word embedding as implicit matrix factorization.\\nIn: Advances in Neural Information Processing Systems, pp. 2177–2185 (2014)\\n13. Nachum, O., Gu, S.S., Lee, H., Levine, S.: Data-eﬃcient hierarchical reinforcement\\nlearning. Adv. Neural Inf. Process. Syst. 31, 3307–3317 (2018)\\n14. Pateria, S., Subagdja, B., Tan, A.H., Quek, C.: Hierarchical reinforcement learning:\\na comprehensive survey. ACM Comput. Surv. (CSUR) 54(5), 1–35 (2021)\\n15. Pezzulo, G., Parr, T., Friston, K.: The evolution of brain architectures for predictive\\ncoding and active inference. Philos. Trans. R. Soc. B 377(1844), 20200531 (2022)\\n16. Pezzulo, G., Rigoli, F., Friston, K.J.: Hierarchical active inference: a theory of\\nmotivated control. Trends Cogn. Sci. 22(4), 294–306 (2018)\\n17. Ritter, F.E., Tehranchi, F., Oury, J.D.: ACT-R: a cognitive architecture for mod-',\n",
       "  'eling cognition. Wiley Interdiscip. Rev. Cogn. Sci. 10(3), e1488 (2019)\\n18. Russell, S.: Human compatible: artiﬁcial intelligence and the problem of control.\\nPenguin (2019)\\n19. Shumskii, S.: ADAM: a model of artiﬁcial psyche. Autom. Remote Control 83(6),\\n847–856 (2022). https://doi.org/10.1134/S0005117922060030\\n20. Shumsky, S.: Machine Intelligence. Essays on the theory of machine learning and\\nartiﬁcial intelligence. RIOR (2019). (in Russian). https://doi.org/10.29039/02011-\\n1\\n21. Shumsky, S.: Scalable natural language understanding: from scratch, on the ﬂy. In:\\n2018 International Conference on Artiﬁcial Intelligence Applications and Innova-\\ntions (IC-AIAI), pp. 73–74. IEEE (2018). https://doi.org/10.1109/IC-AIAI.2018.\\n8674432\\n22. Vezhnevets, A.S., et al.: Feudal networks for hierarchical reinforcement learning.\\nIn: International Conference on Machine Learning, pp. 3540–3549. PMLR (2017)\\n23. Wainwright, M.J.: Variance-reduced q-learning is minimax optimal (2019). arXiv',\n",
       "  'preprint arXiv:1906.04697\\n24. Wang, R., Yu, R., An, B., Rabinovich, Z.: I2HRL: interactive inﬂuence-based hie-\\nrarchical reinforcement learning. In: Proceedings of the Twenty-Ninth International\\nConference on International Joint Conferences on Artiﬁcial Intelligence, pp. 3131–\\n3138 (2021)',\n",
       "  'Electronic Education Machine AGI-EEdu\\nNihad Subasic(B)\\nDepartment of Machine Design, Mechatronics, KTH Royal Institute of Technology,\\nThe School of Industrial Technology and Management, Stockholm, Sweden\\nsubasic@kth.se\\nhttps://www.kth.se/profile/subasic/\\nAbstract. This project is part of the digitization of engineering edu-\\ncation at the Royal Institute of Technology (KTH). The digitizationprocess of the observed undergraduate course in electrical engineering so\\nfar allows students to use the Learning Management System (LMS) to\\nﬁnd course literature and notes, recorded lectures, theoretical exercisesin the form of quizzes, and online interactive calculation exercises with\\nbuilt-in feedback for each question. What remains to be solved is the\\nhuman-type contact that can answer a student’s question, for example:“How am I doing in the course?”. Usually, an experienced teacher would\\nbe able to answer that question with his own predictions.',\n",
       "  'The purpose of this project is to use AGI technology to construct a\\nsoftware solution that, by adapting to the environment, i.e. the existing\\nLMS, with insuﬃcient knowledge and resources about the future, can\\nprovide a qualiﬁed prediction about the student’s future performance.\\nThat solution, here with the working name “AGI-EEdu”, would get to\\nknow each student in the course and (with the help of data from stu-dents’ activities in the LMS, as well as the other data based on aggregated\\nstatistical data from previous exam) could provide unique answers and\\nadvice to each student in real time throughout the course.\\nKeywords: Artiﬁcial General Intelligence (AGI)\\n·Non-Axiomatic\\nReasoning System (NARS) ·Narsese language ·M¨obius Courseware\\n(MCW)\\n1 Introduction\\nResearch on Artiﬁcial General Intelligence (AGI) is one of the latest scientiﬁc',\n",
       "  'ﬁelds. The most developed model of intelligence is the human brain, and it is nat-ural that many psychologists were interested in research on intelligence from the\\nvery beginning. Several authors (among others [ 2]) refer to a 1956 summer confer-\\nence at Dartmouth College in Hanover, New Hampshire, USA, as the start of thenew science [ 8], where one of organizers, John McCarthy, deﬁned Artiﬁcial Intel-\\nligence (AI) as “the science and engineering of making intelligent machines” [7].\\nThis project is about constructing an intelligent machine based on AGI tech-\\nnology to be applied in undergraduate electro courses. The machine (called here\\nc⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2023\\nP. Hammer et al. (Eds.): AGI 2023, LNAI 13921, pp. 265–275, 2023.https://doi.org/10.1007/978-3-031-33469-6\\n_27',\n",
       "  '266 N. Subasic\\n“Electronic Education Machine AGI-EEdu”) would be similar to a normal chat-\\nbot but would go a step further and integrate with the existing Learning Man-agement System (LMS) Canvas and M¨ obius Courseware (MCW). AGI-EEdu\\nwould get to know each student, follow their progress and be ready to answer\\nall their questions, even those about future individual results.\\n2 Theoretical Background\\nScientists have always been fascinated by intelligence, the human brain, and theway it works. This question is complex and it is natural that the research area and\\nthe prevailing culture inﬂuences the researchers’ way of thinking. The summer\\nconference at Dartmouth College collected leading scientists from diﬀerent ﬁelds,\\nand they attempted to deﬁne the term Artiﬁcial Intelligence. [ 8]\\n2.1 A New Science Has Been Born\\nAlan Turing is considered to be the ﬁrst pioneer in modern times who deﬁned\\nthe theoretical framework for using computers (actually, calculating machines) to',\n",
       "  'construct a machine that could function in the same way as human intelligence.\\n[1] Inspired by his work, Allen Newel Global and Simon Herbert [ 9]t r i e dt o\\nprogram a General Problem Solver (GPS) and the new ﬁeld of science named by\\nJohn McCarthy as Artiﬁcial Intelligence was established. From the very begin-\\nning, attempts were made to simulate the human brain (itself not well explored\\nat that time) with a calculator. This attempt required competence from diﬀer-\\nent scientiﬁc ﬁelds: psychologists were expected to describe how humans think,neuro-scientists how a human brain works, and hardware and software engineers\\nto describe a digital machine that could imitate a human brain.\\n2.2 About Theory of Intelligence\\nMany researchers from diﬀerent ﬁelds work on AGI and one continuing problem\\nis a diﬃculty to agree on a deﬁnition of the theory of AGI [ 14]. On the other\\nhand a “thinking machine” could be constructed only by hardware and software',\n",
       "  'engineers in collaboration. AI was from the beginning an interdisciplinary chal-\\nlenge and to deﬁne a theory of intelligence was not at all an easy task. Two main\\ngroup of researchers have been more or less established. The ﬁrst see AI as acopy of the human brain, the second see AI as a programmable digital machine\\ncapable to store and manage data in proper software.\\nWang compares this situation with Rudolf Carnap who had a similar prob-\\nlem when trying to clarify the concept of probability in the middle of the last\\ncentury. [ 12] A long time after that, we still have certain challenges regarding the\\ndeﬁnition of probability theory, with several diﬀerent deﬁnitions or views on def-initions accepted nowadays (Bayesian, Frequentist interpretation etc.), whereby\\nthere exists the commonly accepted formalization of probability spaces in math-\\nematics, with its well-known axioms.',\n",
       "  'Electronic Education Machine AGI-EEdu 267\\nSeveral synonyms like “strong AI”, “hard AI”, “real AI” have established\\nthemselves in published scientiﬁc articles due to the researchers’ diﬀerent back-grounds and diﬀerent views on the issue. McCarthy wrote in [ 7] about human-\\nlevel AI. Despite diﬀerent designations of AGI, diﬀerent researchers do agree\\non the common description of the basic principles of the theory of AI in theliterature, with Wang describing it as “The common thesis behind these terms is\\nthe believe that intelligence is a uniﬁed mechanism that should be described and\\ndeveloped as a whole, independent of any application domain.” [12]\\n3 Research Questions\\nLike other technical solutions, diﬀerent AI-based solutions from Google, chat-\\nbots and even specialized multi-agent systems are spreading within higher educa-\\ntion with diﬀerent tasks and aims. [ 4] I am aware of risks and suspicion towards\\neverything new [ 3] and in this investigation, I will focus on the following ques-',\n",
       "  'tions:\\nRQ 1. How could AGI be used in the development process of undergraduate\\nelectrical courses?\\nRQ 2. How could a continuous prediction of student performance during the\\ncourse be deﬁned by using Narsese language?\\n4 Project - “AGI-EEdu” Machine\\nWang’s deﬁnition of intelligence, which has been deﬁned already in his PhD the-\\nsis in the 90s, is that “Intelligence is the principle of adapting to the environment\\nwhile working with insuﬃcient knowledge and resources.” [13] In this project, the\\nreliable facts from an ongoing course have been used to predict knowledge that\\nis not known but that could contribute to more students passing the ongoing\\ncourse.\\n4.1 AGI-EEdu’s Main Task\\nAGI-EEdu machine will ﬁrst learn some well-known facts about the ongoing\\ncourse and students activities in the course’s Canvas room. Secondly the AGI-\\nEEdu will get information from “the insuﬃcient knowledge ﬁeld” and build a\\npattern of student activities inside the course’s Canvas room.',\n",
       "  'The idea for the project is based on a prediction machine which could\\nhelp students to predict their course grade in undergraduate electro-techniquecourses. Students sometimes feel that exam questions are “harder” than exercise\\nquestions during the course. Teachers sometimes feel that the previous genera-\\ntion of students was much better. All students are keen to know how things aregoing for them during the course and how “hard” the ﬁnal exam will be.\\nNeither the students nor the teachers have a tool to measure their feelings or\\na way to know anything about the future. There is only statistics data based on',\n",
       "  '268 N. Subasic\\nthe results of the previous generations of students, which is not always reliable\\nfor the prediction of future students’ results in the ﬁnal exam, but it can providegood indicators.\\n4.2 Statistical Data from M¨ obius MCW\\nThe course material in the observed electro-courses is completely digitized and\\nis available through LMS Canvas and the embedded software M¨ obius Course-\\nware (MCW) in which the online interactive exercises and examination are con-\\nstructed. [ 11]\\nA “Question” is the smallest part which can be constructed in M¨ obius MCW.\\nThe constructed questions can be combined in two modes - an exercise modeallowing students to receive hints and teacher feedback, and an examination\\nmode with no help available. How M¨ obius MCW has been used in the actual\\ncourses has been explained in this interview [ 10].\\nEach question constructed in M¨ obius MCW is equipped by detailed statistical\\ndata, how many previous students answered the question correctly (partly or',\n",
       "  'fully), The statistical data is automatically updated for each new use of thequestion, Fig. 1. This data can be interpreted as how “hard” each question is.\\nFig. 1. Statistics for one question in M¨ obius MCW\\nEach exercise and exam consists of several questions, with respective statis-\\ntical data generated for each exercise and exam. This data can be learned by the\\nmachine as facts and the respective variables can be connected to the machineand update the machine with new facts in real time. Both practice exercises and\\nthe ﬁnal exam are constructed from several M¨ obius MCW questions, and the\\nstatistical data at practice level or exam level will be grounds for AGI-EEdumachine’s predictions about each individual student’s result.\\nSince the observed electro courses are digitized and all exercises are con-\\nstructed in the same environment as the ﬁnal exam, students’ performance asshown in Fig. 2can be used as input in real time.',\n",
       "  'Fig. 2. Result of one student in a M¨ obius MCW exercise “VIL22 3V¨ axelstr¨ om”',\n",
       "  'Electronic Education Machine AGI-EEdu 269\\nThe interactive exercises of digitized course material consist of three optional\\nand two compulsory interactive tasks, all constructed in M¨ obius MCW. In LMS\\nCanvas, students can ﬁnd pdf ﬁles with a book of theory and a book of calculating\\nstudy cases, several videos and quizzes, all shown in Fig. 3. The digital course\\nmaterial is multifaceted and each form has at least one educational goal to fulﬁl.All course materials can be accessed from any electronic gadget Fig. 4but we do\\nnot know which of these formats will help students the most Fig. 5.\\nStatistical data generated by M¨ obius MCW is reliable high-quality data\\nbecause it provides a picture of student activities that directly aﬀects students’\\nknowledge or the course grades. Less reliable data is statistical data based on\\nthe activity of other students in LMS Canvas.\\n4.3 Statistical Data from LMS Canvas\\nAll these known facts will be learned by AGI-EEdu machine and the machine',\n",
       "  'will be able to predict a student’s ﬁnal result. M¨ obius MCW based exercises are\\navailable for students during the course and the collected statistics in M¨ obius\\nCourseware (MCW) change in real-time based on the results from all students.All student activity during the course, be it in the LMS based digital course\\nmaterial or the M¨ obius MCW based interactive exercises, has an impact on\\nthe statistics of each question, and even on the predictions made by AGI-EEdu\\nmachine. More correct answers will impact the prediction of higher scores on\\nFig. 3. AGI-EEdu machine will get input from all students activities\\nthe ﬁnal exam. The same technology could be used to ﬁnd out about students’\\nﬁnal grades. And since the input data comes directly from the current course,each student can inﬂuence their future results by increasing their activity in the\\ncourse - thus the AGI-EEdu machine learns that the student in question has\\nincreased their activities.',\n",
       "  'AGI-EEdu machine and its prediction is expected to oﬀer students a clearer\\npicture of how hard the course is, and how their activities during the course can\\nimpact their result on the ﬁnal exam. AGI-EEdu machine will be updated by',\n",
       "  '270 N. Subasic\\nstatistical data describing both the whole class’s activities and each student’s\\nactivities in real time. The machine’s prediction should be based on the actualresults of all students who attend the same course and on each student’s own\\nperformance. By using actual data we can impact students’ activities in the\\ncourse and students can see (even on smartphones or I-Pads Fig. 4) that they\\ncan impact the machine’s predictions.\\n4.4 Methodology of the Project\\nAs mentioned before, in M¨ obius MCW each question has quite detailed statistical\\ndata that shows how many students pass and how many of the partial questions\\nasked were answered correctly. Each M¨ obius MCW exam question has four sub-\\nquestions and this statistical data is based on our use and collection of M¨ obius\\nMCW data since the 2019–2020 academic year.\\nThe input data will be taken from the databases of M¨ obius MCW as shown\\nin Fig. 3and the students will be able to impact predictions of the AGI-EEdu',\n",
       "  'machine by their own activity in M¨ obius MCW exercises. AGI-EEdu machine\\nwill be embedded in LMS Canvas as well as M¨ obius MCW, and this technical\\nsolution will allow the machine to be constructed as an agent which behaves as\\nif certain relevant events happened, as described in [ 5]. This tool will be built\\nby using Non-Axiomatic Reasoning System (NARS).\\nFig. 4. AGI-EEdu machine will be updated in real time',\n",
       "  'Electronic Education Machine AGI-EEdu 271\\n4.5 Non-Axiomatic Reasoning System (NARS)\\nA Non-Axiomatic Reasoning System (NARS) consists of three parts:\\n– Language (Narsese)\\n– Non-Axiomatic Logic (NAL)\\n– Control mechanism\\n4.6 Narsese Language\\nNarsese is the representation language of NARS and it is a very useful tool\\nto describe a task and related knowledge. Xiang at.al. presented an interesting\\nproject about an emotions model in NARS at the conference [ 6]. A similar task\\nhas been done in this AGI-EEdu project, where the prediction of students’ ﬁnalresults is based on the results of the previous generation who took the same\\ncourse, and the current students own results during the course.\\nIn this project the constructed application will be able to continuously pre-\\ndict the students’ results in real time during the course. The statistics data of\\nthe questions in Learning Management System (LMS) Canvas and Courseware',\n",
       "  'Mobius (MCW) is based on the whole result of students’ answers on all questionswhich students have done (or answered) as optional or mandatory assignments.\\nThe knowledge of the system is based on beliefs which are based on statistical\\nfacts on students’ tasks, and desires which will be a student’s wish to know the\\nfuture courses-grade.\\nA descriptive theory starts with certain observations in the ﬁeld [ 14]a n dw e\\nwill use the three types of sentences in Narsese to describe the system in our\\nLMS Canvas. The three types of sentences are following:\\n– Judgment - consisting of deﬁnitions of problems\\n– Goal - describing the wishes (wanted outcomes)\\n– Question - deﬁning the tasks which are to be answered or solved\\nNAL-0: Binary Inheritance. During the 90s NAL-0 was called “Inheritance\\nLogic” [ 13] and its idealised version of logic is based on simple deductive logic.\\nFirst, we have to deﬁne the semantics of NAL and the smallest unit of Narsese\\nis a “term”.',\n",
       "  'NAL-1: Basic Syntax and Semantics. NAL-1 is the simplest non-axiomatic\\nlogic where we deﬁne and measure evidence for a statement. In this project we\\ndeﬁne that testbelongs to the course and student belongs to the course, and by\\nusing the NAL semantic it would be written like this:\\n<tentamen →course>\\n<student →course>',\n",
       "  '272 N. Subasic\\nFig. 5. In an AGI-EEdu machine all students have relation to all tasks\\nIn the NAL-1 the Atomic terms will be deﬁned and in this project will be useful\\nto connect NAL-1 to the list of students who registered themselves to a spe-\\nciﬁc course. In this level we use ﬁrst-order reasoning described in [ 14]. A course\\nincludes three optional interactive M¨ obius MCW exercises (VIL, VIRS and VAI)\\nand two compulsory tests (INL and TEN) constructed in the same tool. (Note,\\nthese abbreviations are based on the Swedish names, the details of which are\\nnot relevant here.) M¨ obius Courseware (MCW) has the statistics of the tasks\\nincluded in the mentioned exercises and tests. This statistical data gives us a\\npicture of how diﬃcult it was to solve such tasks for previous students, and thatdata belongs to the course in the same way that the student belongs to the\\ncourse.\\n<ovnV IL →course>\\n<ovnV IRS →course>\\n<ovnV AI →course>\\n<ovnIN L →course>\\n<oldT EN →course>',\n",
       "  'Students can also learn by watching videos or reading theory or solution pro-\\nposals in pdf ﬁles, or by answering quizzes, but LMS Canvas does not oﬀer a\\nmeasurable value that could be assigned to the system. Here we trust that the\\nsystem learns how long students spend on a screen showing mp4 or pdf ﬁles.Here we only allocate space in memory for any information about that time.\\n<timeP DF →course>\\n<timeM P 4→course>\\n<timeQuiz →course>',\n",
       "  'Electronic Education Machine AGI-EEdu 273\\nNAL-2: Derivative Copulas. We use the ﬁrst layer of NARS, called NAL-1\\nto set formal inference rules between statements in NARS, and by using a formallanguage we represent our knowledge of the system. Here, in the second layer,\\nNAL-2, we add grammar and inference rules. Description in each layer starts\\nwith an idealized form and we extend the model in each next step.\\nIn NAL-2 to NAL-4 we declare compound terms:\\n<[(student )∗(timeP DF )]→read > .f p =0 .2...(coming from LMS)\\n<[(student )∗(timeM P 4)]→watched > .f v =0 .4...(coming from LMS)\\n<[(student )∗(timeQuiz )]→succed > .f q =0 .6...(coming from LMS)\\nThe values of functions fp, fv, fq will be extracted from the LMS Canvas in a\\nsimilar way as for the value of statistical data for exercises constructed in M¨ obius\\nCourseware (MCW). These functions describe students’ activities based on pdfﬁles, videos and quizzes which do not necessarily lead to development of students’',\n",
       "  'knowledge. For example, a pdf ﬁle can be shown on the screen or downloaded\\nwithout the student actually reading it, and such activities do not contributeto the student’s development. On the other hand, the description of optional\\n(VIL, VIRS and VAI) and mandatory exercises (INL and TEN) constructed in\\nM¨obius MCW are of higher value because it is based on students’ calculations\\nof real study cases. This statistical data will be extracted from M¨ obius MCW\\ninto NARS, and the following relationship established:\\n<[(student) *(ovnV IL )]→read > .fvil =(coming from MCW)\\n<[(student) *(ovnV IRS )]→watched > .f virs =(coming from MCW)\\n<[(student) *(ovnV AI )]→succed > .f vai =(coming from MCW)\\n4.7 NAL5 and NAL6 - Higher Order Terms\\nEvery activity that a student does in the course must be noticed by AGI-EEdu\\nmachine in such a way that the system learns about it in real time.\\nStudent activities like: “Student read a pdf.” or “Student watched a video',\n",
       "  'lesson.” or “Student solved a quiz.” can be deﬁned in the following way:\\n<(*,{student },p d f)→read>.\\n<(*,{student },m p4)→watch>.\\n<(*,{student },q u i z)→solveQ>.\\n4.8 Minimal Narsese Course Success Reasoning Example\\nThe system watches all course interactions as events. The following is an example\\nof how interactions in M¨ obius MCW can be represented as events in Narsese,\\nwhich allows it to reason about how students can succeed in certain exams or\\nquizzes:',\n",
       "  '274 N. Subasic\\n//Student1 is a student\\n<{student1} --> student>.//Lecture1 is a lecture\\n<{lecture1} --> lecture>.\\n//Quiz1 is a quiz<{quiz1} --> quiz>.\\n//Student1 watched lecture 1\\n<({student1} * {lecture1}) --> watch>. :|://Student 1 succeeded in quiz1 with a success rate of 60%\\n<({student1} * {quiz1}) --> succeed>. :|: %0.6%\\n20\\n//How can someone succeed in quiz1?\\n<?1 =/> <({$1} * {quiz1}) --> succeed>>?\\n//By watching lecture1://Answer: <<({$1} * {lecture1}) --> watch> =/>\\n// <({$1} * {quiz1}) --> succeed>>.\\n// Truth: frequency=0.600000, confidence=0.282230\\n//Does watching lectures contribute to succeeding in quizzes?\\n<<({$1} * lecture) --> watch> =/> <({$1} * quiz) --> succeed>>?//Yes, there is some evidence for that:\\n//Answer: <<({$1} * lecture) --> watch> =/>\\n// <({$1} * quiz) --> succeed>>.// Truth: frequency=0.600000, confidence=0.12133\\nAcknowledgement. The author would like to acknowledge and give warmest thanks',\n",
       "  'to Dr. Robert Johansson, Dr. Pei Wang and Dr. Patrick Hammer for providing us the\\nwonderful opportunity to study Artiﬁcial General Intelligence in the course organisedby Digital Futures at KTH in the fall of 2022.\\nThe author would like to express special thanks to Dr. Patrick Hammer for gener-\\nously sharing his knowledge of Narsese even after the course.\\nReferences\\n1. Turing, A.: His Work and Impact. Elsevier (2013). https://doi.org/10.1016/C2010-\\n0-66380-2 ,https://linkinghub.elsevier.com/retrieve/pii/C20100663802\\n2. Anne H˚ akansson, Ronald Lee Hartung: Artiﬁcial Intelligence, Concepts, areas,\\ntechniques and applications. Studentlitterature, 1. edn. (2020). https://www.\\nstudentlitteratur.se/kurslitteratur/teknik-datorer-it-och-bygg/data-allmant/artiﬁcial-intelligence/\\n3. Brin, D.: Essential (mostly neglected) questions and answers about artiﬁcial intel-\\nligence. Neohelicon, December 2022. https://doi.org/10.1007/s11059-022-00656-8 ,',\n",
       "  'https://link.springer.com/10.1007/s11059-022-00656-8',\n",
       "  'Electronic Education Machine AGI-EEdu 275\\n4. Churi, P.P., Joshi, S., Elhoseny, M., Omrane, A.: Artiﬁcial Intelligence in\\nHigher Education: A Practical Approach, 1 edn. CRC Press, Boca Raton,\\nJuly 2022. https://doi.org/10.1201/9781003184157 ,https://www.taylorfrancis.\\ncom/books/9781003184157\\n5. Everitt, T., Lea, G., Hutter, M.: AGI Safety Literature Review, May 2018.\\nhttps://doi.org/10.48550/ARXIV.1805.01109 ,https://arxiv.org/abs/1805.01109 ,\\npublisher: arXiv Version Number: 2\\n6. Li, X., Hammer, P., Wang, P., Xie, H.: Functionalist emotion model in NARS. In:\\nIkl´e, M., Franz, A., Rzepka, R., Goertzel, B. (eds.) AGI 2018. LNCS, vol. 10999, pp.\\n119–129. Springer, Cham (2018). https://doi.org/10.1007/978-3-319-97676-1 12\\n7. McCarthy, J.: From here to human-level AI. Artif. Intell. 171(18), 1174–1182\\n(2007). https://doi.org/10.1016/j.artint.2007.10.009 ,https://linkinghub.elsevier.\\ncom/retrieve/pii/S0004370207001476',\n",
       "  '8. McCarthy, J., Minsky, M.L., Rochester, N., Shannon, C.E.: A Proposal for the\\nDartmouth Summer Research Project on Artiﬁcial Intelligence, 31 August 1955,\\nvol. 27, December 2006. https://ojs.aaai.org/index.php/aimagazine/article/view/\\n1904\\n9. Newell, A., Simon, H.A.: Human Problem Solving. Prentice-Hall, Englewood Cliﬀs,\\nN.J (1972)\\n10. Subasic, N.: Designing an Eﬀective STEM Course, June 2021. https://digitaled.\\ncom/resources/blog/designing-an-eﬀective-stem-course-part1\\n11. Subasic, N., Johansson, H.: Interactive Assignments in Electro Courses at MDA.\\nStockholm, March 2021. http://kth.diva-portal.org/smash/get/diva2:1588335/\\nFULLTEXT02.pdf\\n12. Wang, P., Hahm, C., Hammer, P.: A model of uniﬁed perception and cognition.\\nFront. Artif. Intell. 5, 806403 (2022). https://doi.org/10.3389/frai.2022.806403 ,\\nhttps://www.frontiersin.org/articles/10.3389/frai.2022.806403/full\\n13. Wang, Pei: Non-Axiomatic Reasoning System: Exploring the Essence of Intel-',\n",
       "  'ligence. Ph.D. thesis, Indiana University., USA (1995). https://cis.temple.edu/\\n∼wangp/Publication/thesis.pdf\\n14. Wang, P.: Rigid Flexibility. Springer, Netherlands (2006). https://doi.org/10.1007/\\n1-4020-5045-3 ,http://link.springer.com/10.1007/1-4020-5045-3',\n",
       "  'Can Language Models Be Used in\\nMultistep Commonsense Planning\\nDomains?\\nZhisheng Tang and Mayank Kejriwal(B)\\nInformation Sciences Institute, USC Viterbi School of Engineering,\\n4676 Admiralty Way 1001, Marina Del Rey, CA 90292, USA\\n{zhisheng,kejriwal }@isi.edu\\nAbstract. Transformer-based language models have recently been the\\nfocus of much attention, due to their impressive performance on myriad\\nnatural language processing (NLP) tasks. One criticism when evaluat-\\ning such models on problems such as commonsense reasoning is thatthe benchmarking datasets may not be challenging or global enough.\\nIn response, task environments involving some kind of multistep plan-\\nning, have emerged as a more stringent, and useful, evaluation paradigm.ScienceWorld is one such environment that has weaker dependence on\\nlanguage itself (compared to core commonsense reasoning). In the origi-',\n",
       "  'nal publication, ScienceWorld problems proved diﬃcult to solve even fora reasonably advanced language model. This paper demonstrates that,\\nwhile true for the hardest version of the problem, even ﬁrst-generation\\nmodels like BERT can achieve good performance on many interestingintermediate problems within ScienceWorld. Our results, in addition to\\nproposing a more practical methodology and metrics for evaluating lan-\\nguage models on multistep planning domains involving commonsensereasoning, also suggest that language models are still likely to be an\\nessential component of (rather than completely orthogonal to) a more\\ncomprehensive approach.\\nKeywords: Language Models\\n·Multistep Planning ·Commonsense\\nReasoning\\n1 Introduction\\nLarge language models, such as ChatGPT, are transformer-based deep neural\\nnetworks that have captured public attention lately for their capability to per-',\n",
       "  'form a wide range of tasks, from text generation to natural language under-standing and reasoning. Such models have achieved human-level performance\\non many benchmarks [ 2]. However, transformer-based models are shown to be\\nchallenged when facing complex problems, such as decision making under uncer-tainty [ 8] and multistep commonsense planning in some text environments, an\\nexample being the ScienceWorld benchmark [ 10]. Such problems require a sys-\\ntem to reason about commonsense concepts and plan for future events and goals,\\nc⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2023\\nP. Hammer et al. (Eds.): AGI 2023, LNAI 13921, pp. 276–285, 2023.https://doi.org/10.1007/978-3-031-33469-6\\n_28',\n",
       "  'Can Language Models Be Used in Multistep Commonsense 277\\nwhich is essential for achieving Artiﬁcial General Intelligence (AGI). Without\\nsuch abilities, an intelligent system cannot be robust to potential obstacles thatare common in everyday life (when pursuing myriad goals within constrained\\nenvironments) and plan accordingly to achieve success. Multistep commonsense\\nplanning can help AGI systems adapt to changing environments and handleuncertainty in a consistent manner.\\nThe ScienceWorld benchmark contains elementary science curriculum tasks\\nthat depend upon multistep planning and commonsense reasoning abilities. Thebenchmark claims to require the same level of commonsense as an elementary\\nschool student. The environment itself is an abstract, interactive text environ-\\nment with 25 diﬀerent high-level verbs (e.g. go to, pick up), 200 types of ele-ments (e.g. plants, electrical components), and 10 interconnected locations (e.g.',\n",
       "  'kitchen, hallway). Agents are expected to solve tasks using natural language\\ninputs, such as ‘pick up pot 7’ and ‘go to hallway’.\\nIn the original work, the ScienceWorld benchmark is evaluated using ﬁve\\ndiﬀerent agents, two of which are transformer-based (CALM [ 11] and Decision\\nTransformer [ 3]). The highest score achieved by these agents is only 0.17. One\\nof the reasons is the evaluation metric because the original benchmark evalu-\\nates each solution in an ‘all or nothing’ fashion: either an agent successfullyaccomplishes the task or fails it. However, such a metric fails to take incremen-\\ntal progress into account. We posit that, while this metric is ultimately very\\nimportant, designing auxiliary metrics to evaluate the incremental performanceof agents can serve a useful role in assessing their progress. Additionally, the Sci-\\nenceWorld benchmark was designed primarily for reinforcement learning agents\\nrather than language models.',\n",
       "  'We propose a novel benchmark called QAScienceWorld that is derived from\\nthe original ScienceWorld benchmark and is composed of a set of independent\\nand identically distributed (i.i.d) Multiple Choice Question Answering (MCQA)instances. The benchmark is meant to address both of the issues noted above. It\\ncan be used to evaluate discriminative language models. To measure incremental\\nprogress, QAScienceWorld uses four metrics for evaluating responses at diﬀerent\\nlevels of granularity. We demonstrate the utility of the benchmark by applying\\nthe Bidirectional Encoder Representations from Transformers (BERT) model[4], and showing that it is able to achieve high accuracy on some metrics, and\\nlower accuracy on the metric best aligned with the ‘all or nothing’ paradigm\\nmentioned earlier. Hence, the benchmark shows that language models can serveas an important component in solving such multistep commonsense planning\\nproblems, without negating previous suggestions that they are unlikely to yield',\n",
       "  'a complete solution to these problems.\\n2 Benchmark Construction\\nThe original ScienceWorld benchmark is seeded with 30 diﬀerent task types,ranging from measuring temperature to object classiﬁcation. Each type of task\\nhas 10 to 1400 task variations, where each task variation changes some important',\n",
       "  '278 Z. Tang and M. Kejriwal\\ntask objects. For example, for the task of ﬁnding a plant, the ﬁnal objective of\\none variation is to put the plant into an orange box. But, in other variations ofthe same task, the orange box is changed to a yellow box or some other container.\\nEach task variation is provided with a ground-truth or ‘gold-standard’ trajec-\\ntory, which begins from the starting state S\\n1to the ending state Sn+1(where the\\ntask is successfully completed). Such a gold-standard trajectory can be expressed\\nas a sequence of nactions ( a1,a2, ..., a n), where an agent selects the ﬁrst action\\na1in the starting state S1to go to state S2, then selects the second action a2\\nin the second state S2to go to state S3and so on, to ﬁnally select the nth\\ngold-standardaction anin the nth state Snto go to the ending (goal-achieving)\\nstate Sn+1. The framework is very similar to traditional planning. Each action\\naican be further decomposed into a verb viand one or two elements ei, where',\n",
       "  'the verb is ‘applied’ to the element(s). Note that the gold-standard trajectories\\nonly represent canonical or standard solutions (e.g. use a thermometer to mea-\\nsure temperature). We recognize that other solutions exist, but we only focus\\non the gold-standard trajectories at present, as these are the only solutions thatare provided within the ScienceWorld benchmark for evaluation purposes.\\nIn the original ScienceWorld benchmark, an agent is expected to select a\\nsequence of actions to accomplish a task. With 25 verbs and many more elementchoices, the agent needs to search a huge solution space to ﬁnd even one correct\\naction. The diﬃculty grows exponentially as the number of actions required to\\nsolve a task increases because each action is dependent on the previous actions.Additionally, transformer-based language models are not designed for problems\\nthat involve planning a sequence of actions. To decrease the diﬃculty of the',\n",
       "  'task and make the problem more amenable to language models, we propose theQAScienceWorld benchmark, which frames each action as an i.i.d MCQA prob-\\nlem. Namely, instead of being dependent on previous actions, in our benchmark,\\neach action is independent of other actions and only depends on the current envi-ronment. Also, agents select from a small and ﬁxed number of choices as their\\nprediction for the current environment. This formulation greatly reduces the\\ncomplexity of the original problem. As explained earlier, we only use the actions\\nfrom the gold-standard trajectory of each task variation of all task types. We\\ndescribe the benchmark construction below, with a visual example provided inFig.1:\\n1. We assume that, in order to select the correct action, the agent needs to know\\nthree important pieces of information: the task description, the observation\\nof the current environment, and the current inventory status . Speciﬁcally,',\n",
       "  'the task description oﬀers information about the ultimate objective, while\\nthe observation of the current environment gives descriptions of the currentlocation, items available to use (and so on), and the current inventory tells the\\nagent about what items can be used besides those in the current environment.\\nFor each gold-standard action a\\ni, we concatenate the above three pieces of\\ninformation from the state Sito form the basic prompt of the MCQA. Because\\neach action can be decomposed into a verb part and an element part, we create\\ntwo MCQA instances (one for each part) for each action. We concatenate the',\n",
       "  'Can Language Models Be Used in Multistep Commonsense 279\\nFig. 1. A partial gold-standard trajectory (last 4 actions) for a variation of the task\\ntype: ﬁnd a(n) plant. The last action is converted to a verbMCQA instance and an ele-\\nment MCQA instance. The prompts are the concatenation of the task description ,t h e\\nenvironment observation ,the inventory ,a n dt h e verbprompt or the element prompt.\\nThe four possible choices consist of one ground-truth item and three (randomly sam-\\npled) wrong choices. In the case of an element MCQA where two elements have to\\nbe simultaneously selected for a verb, the three wrong choices consist of two wrongchoices where only one element is randomly sampled and a third wrong choice where\\nboth elements are randomly sampled.\\nbasic prompt with a verb prompt (i.e., ‘Your next action should be:’) to form\\nthe ﬁnal prompt for the verb MCQA instance. Similarly, we concatenate thebasic prompt with an element prompt (i.e., ‘Your next object should be:’) to',\n",
       "  'form the ﬁnal prompt for the element MCQA instance.\\n2. Each MCQA instance has four choices, one of which is the ground-truth\\nchoice. To construct the remaining three wrong choices, we randomly sample\\nfrom the state S\\ni, given the gold-standard action ai. The original Science-\\nWorld benchmark provides a convenient API for sampling available verbs\\nand elements. For a verb MCQA instance, we randomly sample three wrong\\nverbs as the three wrong choices. For an element MCQA instance, if thegold-standard action a\\nionly has one element, we again randomly sample\\nthree wrong elements as the three wrong choices. For cases where the gold-\\nstandard action aicontains two elements (e.g., {correct element 1, correct\\nelement 2 }), we randomly sample two wrong elements to replace either one\\nof the ground-truth element as two of the wrong choices (e.g. {correct ele-\\nment 1, wrong element 1 }and{wrong element 2, correct element 2 })a n d',\n",
       "  'randomly sample two wrong elements to replace both ground-truth elements\\nas the third wrong choice (e.g., {wrong element 3, wrong element 4 }). More\\nadvanced sampling techniques, such as adversarially sampling choices thatcan be more challenging for an agent, are left for future work.\\n3. In the original ScienceWorld benchmark, for each task type, all task varia-\\ntions are split into 50% training, 25% development, and 25% test sets. In our',\n",
       "  '280 Z. Tang and M. Kejriwal\\nbenchmark, we follow the same partition. The training set verb and element\\nMCQA instances are created using the actions of the gold-standard trajec-tory from the training set task variations in the original benchmark.\\n1The\\nsame goes for the development set and the test set. Details concerning the\\nbenchmark are tabulated in Table 1.\\n4. In the original ScienceWorld benchmark, there are 25 diﬀerent verbs. How-\\never, we exclude any verb that has ‘look’ in it, because this verbs does not\\nchange the environment in any way, only providing information that is alreadyprovided by the observation of the current environment and will cause confu-\\nsion to the agents. Hence, we have 22 diﬀerent verbs. We refer the reader to\\nthe original ScienceWorld paper for details about the action space. Addition-ally, following the original work, we set the environment to the ‘easy’ mode\\nwhen we extract the environment information. On average, the optimal ran-',\n",
       "  'dom performance, whether it entails selecting one of four choices at random\\nor selecting the mode of the correct choices in the training partition, is 40%\\nand 25% for verb and element MCQA instances, respectively.\\n3 Metrics\\nIn most text-based interactive environments, such as the Jericho [ 5], a kitchen\\ncleanup game [ 7], and the TextWorld Commonsense [ 6], the performance of an\\nagent is evaluated using a score-based metric, where the raw score is obtained\\nwhen the agent tries to solve a task through a sequence of actions and the ﬁnalnormalized score is calculated by dividing the raw score with the maximum\\nscore possible if the task is solved. This metric evaluates agents’ performances\\nonly at the level of the task as the ﬁnal score is normalized using the maximumscore obtained when the task is ﬁnished. Additionally, this metric evaluates the\\nagent’s performance under the assumption that, at each action, the problem is',\n",
       "  'dependent on all previous actions. Hence, such a score-based metric is limitedif we try to investigate the more detailed behaviors of the agent. We argue that\\nmetrics with ﬁner granularity are needed to fully evaluate the performance and\\nthe behavior of an agent. Hence, we present four diﬀerent metrics that are atdiﬀerent levels of granularity: Verb accuracy, Element accuracy, Action accuracy ,\\nand Task accuracy .\\nRecall that, for a gold-standard trajectory and i∈[1,n], the ith gold-\\nstandard action a\\nican be decomposed into a verb part viand an element part\\nei. An agent predicts the ith verb v′\\niand the ith element e′\\ni,a n d a′\\niis the agent’s\\nprediction for the ith action, where a′\\niis the combination of v′\\niand e′\\ni.\\nVerb accuracy, element accuracy, and action accuracy evaluate the agent’s\\nperformance under the assumption that, for each action, the problem is inde-\\npendent of all previous actions, but only depends on the current environment.',\n",
       "  '1Because the gold-standard trajectory for the task type ‘measuring the boiling point\\nof an unknown substance’ is missing from the original ScienceWorld repository, we\\nonly have 29 task types here.',\n",
       "  'Can Language Models Be Used in Multistep Commonsense 281\\nTable 1. The number of task variations of each task type and the number of actions\\ncreated using all task variations of each task type, divided into the training set, thedevelopment set, and the test set. The number of MCQA instances is twice the number\\nof actions because each action is split into a verb MCQA instance and an element\\nMCQA instance.\\n# of Variations # of Actions\\nTopic Task Type Train Dev Test Train Dev Test\\nMatter Changes of State (Boiling) 14 7 9 471 400 699\\nMatter Changes of State (Any) 14 7 9 282 292 381\\nMatter Changes of State (Freezing) 14 7 9 391 402 484\\nMatter Changes of State (Melting) 14 7 9 362 290 381\\nMeasurement Measuring Boiling Point (known) 218 109 109 5916 4401 3093\\nMeasurement Use Thermometer 270 135 135 2949 1480 1413\\nElectricity Create a circuit 10 5 5 70 39 35\\nElectricity Renewable vs Non-renewable Energy 10 5 5 132 75 64\\nElectricity Test Conductivity (known) 450 225 225 8934 4450 4647',\n",
       "  'Electricity Test Conductivity (unknown) 300 150 150 5023 2482 2540\\nClassiﬁcation Find an animal 150 75 75 1103 543 557\\nClassiﬁcation Find a living thing 150 75 75 1103 543 557\\nClassiﬁcation Find a non-living thing 150 75 75 605 312 288\\nClassiﬁcation Find a plant 150 75 75 1029 517 511\\nBiology Grow a fruit 62 31 33 3699 1838 1941\\nBiology Grow a plant 62 31 33 1609 815 856\\nChemistry Mixing (generic) 16 8 8 297 135 179\\nChemistry Mixing paints (secondary colours) 18 9 9 154 79 78\\nChemistry Mixing paints (tertiary colours) 18 9 9 282 139 138\\nBiology Identify longest-lived animal 62 31 32 180 95 92\\nBiology Identify longest-then-shortest-lived animal 62 31 32 242 126 124\\nBiology Identify shortest-lived animal 62 31 32 180 95 92\\nBiology Identify life stages (animal) 6 3 5 159 91 159\\nBiology Identify life stages (plant) 4 2 4 25 16 30\\nForces Inclined Planes (determine angle) 84 42 42 1006 508 512\\nForces Friction (known surfaces) 692 346 348 8316 4182 4214',\n",
       "  'Forces Friction (unknown surfaces) 80 40 42 956 484 512\\nBiology Mendelian Genetics (known plants) 60 30 30 5037 2524 2546\\nBiology Mendelian Genetics (unknown plants) 240 120 120 20219 9990 10128\\nSum 3442 1721 1744 70731 37343 37251\\nSpeciﬁcally, a verb prediction v′\\nior an element prediction e′\\niis correct if it is\\nidentical to the ith gold-standard verb vior the ith gold-standard element ei.\\nVerb accuracy is deﬁned as the percentage of correct verb predictions and is\\ncalculated by dividing the number of correct verb predictions by the total num-\\nber of gold-standard actions in the trajectory. Element accuracy is deﬁned in asimilar manner where it is the percentage of correct element prediction and is\\ncalculated by dividing the number of correct element predictions by the total',\n",
       "  '282 Z. Tang and M. Kejriwal\\nnumber of gold-standard actions in the trajectory. Similarly, the ith action pre-\\ndiction a′\\niis considered correct if both v′\\niis identical to viand e′\\niis identical to\\nei. Action accuracy is deﬁned as the percentage of correct action predictions and\\nis calculated by dividing the number of correct action predictions by the total\\nnumber of gold-standard actions in the trajectory.\\nTask accuracy shares the same assumption as the score-based metrics in that\\nit evaluates agents’ performances at the level of the task. Hence, for one task\\nvariation, it is considered correct if all action predictions of that task variationare correct. Formally, if ( a\\n′\\n1,a′2, ..., a′\\nn) is identical to ( a1,a2, ..., a n), we consider\\nthe prediction for this task variation as correct. Task accuracy is calculated by\\ndividing the number of correct task variation predictions by the total number ofvariations of a task type.\\n4 Experiments',\n",
       "  'We use BERT to demonstrate the utility of our QAScienceWorld benchmark\\nand to showcase the diﬀerent granularity of our novel metrics. BERT repre-\\nsents a paradigm shift from the previous neural models (e.g. CNN and RNN) totransformer-based models. Moving forward, transformer-based models continue\\nto push the performance to near or above human performances on many natural\\nlanguage understanding and computer vision tasks [ 9]. Hence, BERT is a repre-\\nsentation of other transformer-based models and can reﬂect the base diﬃculty\\non a variety of tasks, including our benchmark.\\nFollowing the original paper, we use the BERT base model, which has 12\\nlayers, a hidden size of 768, and 12 attention heads and has a pre-trained weight\\navailable in the Hugging Face repository [ 1]. We ﬁne-tuned this pre-trained\\nBERT model using all verb MCQA instances and element MCQA instances\\nfrom the training set. For one MCQA instance, the input to the BERT model',\n",
       "  'is a prompt-choice string constructed by concatenating the prompt and one ofthe four choices. Because we have four choices, the input is four such strings\\nper MCQA instance. The label for this MCQA instance is the index of the\\nprompt-choice string that contains the correct choice. During inference, for anMCQA instance, the ﬁne-tuned BERT model will return four scores (one for\\neach prompt-choice string) and the prompt-choice string with the highest score\\nis considered the prediction. We ﬁne-tuned the pre-trained BERT model for oneepoch, using the default learning rate of 5e-5 and a batch size of 16. We report the\\nﬁne-tuned BERT results on the test set evaluated using verb accuracy, element\\naccuracy, action accuracy, and task accuracy in Table 2.\\nThe results illustrate that the ﬁne-tuned BERT model can achieve over 90%\\nverb, element, and action accuracy. But accuracy in the 70% to 80% range isalso observed, such as in the case of the task type: Renewable vs Non-renewable',\n",
       "  'Energy, where the action accuracy is only 76%. These results suggest that the\\ntransformer-based models can achieve human-level performance when the prob-lems are presented in the format of i.i.d MCQA and the solution space is rela-\\ntively limited. The diﬃculty arises when we evaluate these models’ performance',\n",
       "  'Can Language Models Be Used in Multistep Commonsense 283\\nTable 2. The verb, element, action, and task accuracy (introduced in Sect. 3and\\nexpressed as a percentage) estimates of the ﬁne-tuned BERT model on the test set.\\nThe ﬁne-tuned BERT model is trained using all verb MCQA instances and elementMCQA instances from the training partition of the benchmark on a pre-trained BERT\\nmodel. The verb, element, and action accuracy estimates are ﬁrst calculated at the\\nlevel of each task variation and then averaged over all task variations of a given tasktype.\\nTopic Task Type Verb Element Action Task\\nAccuracy\\nMatter Changes of State (Boiling) 96 89 86 0\\nMatter Changes of State (Any) 95 86 82 0\\nMatter Changes of State (Freezing) 91 85 78 0\\nMatter Changes of State (Melting) 96 82 79 0\\nMeasurement Measuring Boiling Point (known) 92 68 61 0Measurement Use Thermometer 98 92 91 43\\nElectricity Create a circuit 97 95 92 60\\nElectricity Renewable vs Non-renewable Energy 86 89 76 0',\n",
       "  'Electricity Test Conductivity (known) 95 90 87 11\\nElectricity Test Conductivity (unknown) 96 97 94 33\\nClassiﬁcation Find an animal 100 91 91 48\\nClassiﬁcation Find a living thing 100 87 87 33\\nClassiﬁcation Find a non-living thing 99 82 81 47\\nClassiﬁcation Find a plant 100 83 83 23\\nBiology Grow a fruit 96 96 93 0\\nBiology Grow a plant 96 96 94 18\\nChemistry Mixing (generic) 91 80 73 0Chemistry Mixing paints (secondary colours) 96 89 85 22\\nChemistry Mixing paints (tertiary colours) 86 93 80 0\\nBiology Identify longest-lived animal 100 93 93 78\\nBiology Identify longest-then-shortest-lived animal 100 89 89 63\\nBiology Identify shortest-lived animal 100 99 99 97\\nBiology Identify life stages (animal) 100 91 98 60\\nBiology Identify life stages (plant) 100 100 100 100\\nForces Inclined Planes (determine angle) 98 99 97 69\\nForces Friction (known surfaces) 99 99 98 74\\nForces Friction (unknown surfaces) 97 99 97 67\\nBiology Mendelian Genetics (known plants) 95 97 93 0',\n",
       "  'Biology Mendelian Genetics (unknown plants) 95 96 92 0\\nAverage 96 91 88 33\\non the level of the task. As indicated by the results, the typical task accuracy\\nis below 50%. However, there is considerable variance observed at times. Forexample, it appears that the ‘identify’ task types within the biology topic might',\n",
       "  '284 Z. Tang and M. Kejriwal\\nbe an ‘easier’ task, as the ﬁne-tuned BERT model even achieves 100% accuracy\\nin one of the task types and achieves above 60% accuracy in other task typesof similar nature. The task type ‘changing the state of a matter’ seems to pose\\nthe biggest challenge for the model, as in all four such task types, the ﬁne-tuned\\nBERT model achieves 0% accuracy. Interestingly, the above two observationsare conﬁrmed by the experiments done in the original ScienceWorld work. This\\nfurther demonstrates that our benchmark can serve as an eﬀective complement,\\nor extension, to the original benchmark.\\n5 Conclusion and Future Work\\nWe designed and constructed an i.i.d. MCQA benchmark, QAScienceWorld, thatis derived from the original ScienceWorld environment using its gold-standard\\ntrajectories. The QAScienceWorld benchmark is posited to be more friendly to',\n",
       "  'evaluating transformer-based language models rather than reinforcement learn-ing agents, as the original benchmark is primarily positioned to do. Furthermore,\\nto better understand the performance of transformer-based language models at\\ndiﬀerent levels of granularity, we proposed four metrics that are diﬀerent fromthe usual ‘all or nothing’ approach and seek to measure the incremental progress\\nof agents in a systematic fashion. We demonstrated the usefulness of our bench-\\nmark by ﬁne-tuning the pre-trained BERT model and evaluating its performance\\nusing the four metrics. The results indicate that even though, in general, BERT\\ncan only ﬁnish about 30% of the tasks, it does achieve over 90% accuracy onsome of the other metrics. This suggests that while transformer-based language\\nmodels struggle with such multistep commonsense planning problems, they can',\n",
       "  'play a powerful role in dealing with intermediate problems. Hence, they couldplay a critical and integrative role in systems better suited for planning but\\nneeding to use commonsense language to ﬁll in the gaps.\\nWe end with the caveat that the QAScienceWorld benchmark is derived using\\ngold-standard solutions in the original ScienceWorld benchmark, and ignores\\nother potential trajectories to ﬁnish the tasks. This can also lead to a disad-\\nvantage when evaluating agents’ performance, because of the restricted solutionspace. Hence, we will consider including other solutions in our benchmark in\\nfuture work. Additionally, when constructing the benchmark, we only consider\\nrandomly selected wrong choices to include with the ground-truth choice to formthe complete set of candidate choices for the MCQA instances. We recognize that\\nthis can pose an unfair advantage to the agents evaluated using our benchmark\\nover the original one, as both the verb and element space is narrower in our',\n",
       "  'benchmark. Hence, one approach that we are considering in a future updated\\nversion of this benchmark is adversarial (rather than random) sampling of thewrong choices (or in an even more challenging version, including allpossible\\nchoices).',\n",
       "  'Can Language Models Be Used in Multistep Commonsense 285\\nReferences\\n1. Hugging face repository: bert-base-uncased. https://huggingface.co/bert-base-\\nuncased\\n2. Brown, T., et al.: Language models are few-shot learners. Adv. Neural Inf. Process.\\nSyst. 33, 1877–1901 (2020)\\n3. Chen, L., et al.: Decision transformer: reinforcement learning via sequence model-\\ning. Adv. Neural Inf. Process. Syst. 34, 15084–15097 (2021)\\n4. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: pre-training of deep bidirec-\\ntional transformers for language understanding. arXiv preprint arXiv:1810.04805\\n(2018)\\n5. Hausknecht, M., Ammanabrolu, P., Cˆ ot´e, M.A., Yuan, X.: Interactive ﬁction\\ngames: a colossal adventure. In: Proceedings of the AAAI Conference on Artiﬁ-\\ncial Intelligence, vol. 34, pp. 7903–7910 (2020)\\n6. Murugesan, K., et al.: Text-based rl agents with commonsense knowledge: new',\n",
       "  'challenges, environments and baselines. In: Proceedings of the AAAI Conferenceon Artiﬁcial Intelligence, vol. 35, pp. 9018–9027 (2021)\\n7. Murugesan, K., Atzeni, M., Shukla, P., Sachan, M., Kapanipathi, P., Tala-\\nmadupula, K.: Enhancing text-based reinforcement learning agents with common-sense knowledge. arXiv preprint arXiv:2005.00811 (2020)\\n8. Tang, Z., Kejriwal, M.: Can language representation models think in bets? R. Soc.\\nOpen Sci. 10(3), 221585 (2023)\\n9. Tay, Y., Dehghani, M., Bahri, D., Metzler, D.: Eﬃcient transformers: a survey.\\nACM Comput. Surv. 55(6), 1–28 (2022)\\n10. Wang, R., Jansen, P., Cˆ ot´e, M.A., Ammanabrolu, P.: Scienceworld: Is your agent\\nsmarter than a 5th grader? arXiv preprint arXiv:2203.07540 (2022)\\n11. Yao, S., Rao, R., Hausknecht, M., Narasimhan, K.: Keep CALM and explore:\\nlanguage models for action generation in text-based games. In: Proceedingsof the 2020 Conference on Empirical Methods in Natural Language Pro-',\n",
       "  'cessing (EMNLP), pp. 8736–8754. Association for Computational Linguis-\\ntics, Online, November 2020. https://doi.org/10.18653/v1/2020.emnlp-main.704 ,\\nhttps://aclanthology.org/2020.emnlp-main.704',\n",
       "  'Explicit Goal-Driven Autonomous\\nSelf-Explanation Generation\\nKristinn R. Thórisson1,2(B), Hjörleifur Rörbeck1,2, Jeﬀ Thompson2,\\nand Hugo Latapie3\\n1Center for Analysis and Design of Intelligent Agents, Reykjavik University,\\nMenntavegur 1, Reykjavík, Iceland\\nthorisson@ru.is\\n2Icelandic Institute for Intelligent Machines, Reykjavík, Iceland\\njeff@iiim.is\\n3Cisco Systems, Emerging Technologies and Incubation, San Jose, CA, USA\\nhlatapie@cisco.com\\nAbstract. Explanation can form the basis, in any lawfully behaving\\nenvironment, of plans, summaries, justiﬁcations, analysis and predic-tions, and serve as a method for probing their validity. For systems with\\ngeneral intelligence, an equally important reason to generate explana-\\ntions is for directing cumulative knowledge acquisition: Lest they be bornknowing everything, a general machine intelligence must be able to han-\\ndle novelty. This can only be accomplished through a systematic logical',\n",
       "  'analysis of how, in the face of novelty, eﬀective control is achieved andmaintained—in other words, through the systematic explanation of expe-\\nrience . Explanation generation is thus a requirement for more powerful\\nAI systems, not only for their owners (to verify proper knowledge andoperation) but for the AI itself—to leverage its existing knowledge when\\nlearning something new. In either case, assigning the automatic genera-\\ntion of explanation to the system itself seems sensible, and quite possibly\\nunavoidable. In this paper we argue that the quality of an agent’s expla-\\nnation generation mechanism is based on how well it fulﬁls three goals –or purposes – of explanation production: Uncovering unknown or hidden\\npatterns, highlighting or identifying relevant causal chains, and identify-\\ning incorrect background assumptions. We present the arguments behindthis conclusion and brieﬂy describe an implemented self-explaining sys-',\n",
       "  'tem, AERA (Autocatlytic Endogenous Reﬂective Architecture), capa-\\nble of goal-directed self-explanation: Autonomously explaining its own\\nbehavior as well as its acquired knowledge of tasks and environment.\\nKeywords: Artiﬁcial Intelligence\\n·Explanation Generation ·\\nAutonomy ·General Machine Intelligence ·Causal Reasoning ·\\nSelf-Explanation\\n1 Introduction\\nExplainability is an important feature of any artiﬁcial intelligence (AI) systems,\\nfor numerous reasons. Explanations can form the basis of valid plans, summaries,\\nc⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2023\\nP. Hammer et al. (Eds.): AGI 2023, LNAI 13921, pp. 286–295, 2023.https://doi.org/10.1007/978-3-031-33469-6\\n_29',\n",
       "  'Explicit Goal-Driven Autonomous Self-Explanation Generation 287\\njustiﬁcations, predictions, etc. and serve as a method for probing their validity,\\ncost, and potential dangers—which, in fact, is the role of explanations in generalin society. The more complex an AI system is, the more important it is that\\nits operation be transparent and understandable not only by its owners, but\\nalso by the system itself. Being explainable implies support for direct inquiriesfor why a system did what it did, what it plans to do, and why it chose some\\naction over another. The level of transparency oﬀered this way will impact a\\nsystem’s trustworthiness. For any general machine intelligence, trustworthiness isa necessity, since such systems will handle novelty by deﬁnition; how they behave\\nin light of novel situations and tasks must be veriﬁable, at some reasonable level',\n",
       "  'of abstraction, to ensure their safety. Automating explanation generation in AIsystems is therefore an important goal [ 17], and it might be argued that it is\\nnecessary for a system to be worthy of being considered general [ 23].\\nIt is the ability of explanations to be veriﬁed that brings them their fun-\\ndamental value. To be veriﬁable means that they must be based on knowl-\\nedge of veriﬁable causal relationships in the situation, task, or circumstancesin question—in other words, they must be falsiﬁable . To be falsiﬁable they must\\nreference some set of causal relations whose validity is undisputed in the relevant\\ncontexts, or easily veriﬁable.\\nAn important function of explanation that is less often discussed than most\\nothers is their use for guiding an autonomous agent’s learning; the ability to\\nﬁnd explanations for learning failure or success can help uncover how the worldworks. In this case, to be eﬀective and eﬃcient, explanation generation must be',\n",
       "  'autonomous [ 17]. Here we examine goal-directed self-explanation: the ability of a\\nsystem to autonomously generate explanations about its own behavior, as well asits acquired knowledge of tasks and environment, under articulated requirements,\\ni.e. explicit goals. A key focus of this work is the use of such explanations as a\\nmethod for learning (and meta-learning, that is, learning to learn).\\nThe work rests on the argument that explanation generation is a funda-\\nmental and necessary process for general self-supervised learning [ 23]. We look\\nat how explanation generation for this purpose is achieved in the AERA sys-\\ntem, and discuss its approach in light of other systems aimed at general intel-\\nligence. Thórisson [ 21] describe a theory of pragmatic understanding that we\\ntake as the foundation for our work here. We consider their deﬁnition of under-\\nstanding well-suited for building a theory of explanation generation because it',\n",
       "  'already presents a strong foundation for relating prediction, goal achievement,knowledge acquisition, and explanation to causal reasoning.\\nThe paper is structured as follows: We start with an overview of related work,\\nthen we provide some important deﬁnitions for the subsequent discussion, whichoutlines our theory of goal-driven self-explanation generation.\\n2 Related Work\\nFor reasons of opaqueness, studies on explainable AI have so far primarily focused\\non artiﬁcial neural networks (ANNs), being mostly based on (manually guided)\\nabductive methods that attempt to trace certain outputs to the identiﬁcation of',\n",
       "  '288 K. R. Thórisson et al.\\nrelevant inputs (cf. [ 14]). For immediate clariﬁcation, this is not what the present\\npaper is about. In the allocentric methodologies employed in the developmentof these systems [ 19], training data, implicit goals, and hand-coded heuristics,\\nare all determined and provided by the developers themselves, a-priori. In this\\nsense, ANN-based systems are no diﬀerent from standard software applications.\\nWe envision the aims of ‘explainable AI’ research diﬀerently. First and fore-\\nmost, we recognize that the primary practical application of AI is all sorts of\\nautomation, and therefore autonomous explanation generation should be a pri-\\nmary goal for explainable AI. In short, the human eﬀort needed to arrive at an\\nexplanation should be minimized as far as possible, delegating the explanation\\ngeneration to the machine. Equally importantly, we see explanation – and itsextension into argumentation in general – to be a foundation for any general',\n",
       "  'machine intelligence to grow its knowledge reliably, eﬃciently and eﬀectively.\\nWe are working exclusively on systems that can generate explanations\\nautonomously, about themselves and their task-environment—i.e. systems that\\nare self-explaining . Generally speaking, explanations can vary in their quality. A\\ngood explanation eliminates blind spots, clariﬁes, or highlights that which was\\nobscure before (see section below). Above all, a good explanation observes cer-\\ntain implicit (explicit) constraints and does not break any relevant rules. To doso, it is not enough that an explanation refer to correlational data, it must be\\nbased on actual and relevant causal relations. This is because a good explanation\\nmust highlight whysomething – whether it be a course of events, situation, or\\nother outcome – must be the way it is , rather than some other way [ 4,15].\\nMost sources agree that causal attribution, or identifying underlying causes of',\n",
       "  'a class of (or particular) events or state of aﬀairs, is a vital part of explanation [ 3,\\n9,10,18,24].\\n1In fact, this is often how explanation is deﬁned. Josephson equates\\nﬁnding possible explanations with ﬁnding possible causes [ 7], and Halper and\\nPearl claim that explaining a set of events necessitates the acknowledgment of thecause of those events [ 3]. Miller expands on this, arguing that explanation begins\\nwith the cognitive process of identifying causes, followed by a social process\\nof conveying the knowledge acquired by the cognitive process to the intended\\nrecipient. As he also points out, causal attribution is a twofold process of inferring\\nthe key causes and then selecting a subset of those causes as the most relevantfor an explanation [ 10]. Our approach is somewhat aligned with this view.\\nHalpern and Pearl [ 4] deﬁne causal explanations using structural equations,\\nfor the purpose of determining and conveying an actual cause of an explanandum.',\n",
       "  'To accomplish this they assume that all relevant facts are known to said model.\\nWhat is lacking is a treatment of tasks and goals rather than simply explaining.\\nThe assumption of a complete model is also unrealistic, particularly in complexreal-world situations. Their work thus leaves much to be desired when it comes\\nto AI, including how such models are autonomously built. This is addressed\\nin our AERA system by positioning explanation as the provisioning of missinginformation structures, making incomplete knowledge a feature, not a bug.\\n1Other types of explanation than causal have been proposed. Teleological explanations\\nare explanations focused on utility (to explain by deﬁning the purpose or intent ofthe thing to be explained [ 2]). But nowhere nearly all things in need of explaining\\nhave intent or utility behind them.',\n",
       "  'Explicit Goal-Driven Autonomous Self-Explanation Generation 289\\nHilton [ 5,6] researched explanations extensively from a psychological per-\\nspective. They point out the inherent fallacy in using covariational criteria forcausal attribution, as there are numerous examples of events occurring at the\\nsame time without one being the cause of the other. Their alternative model of\\nexplanation is based on ﬁndings in ordinary language where humans make useof contrastives and counterfactuals as criteria for causal attribution. This is also\\none of the major ﬁndings of Miller’s survey [ 10] on explanations: explanations\\nin human conversation most commonly are produced in response to contrastivequestions, for instance “Why did you do A and not B?” rather than simply “Why\\ndid you do A?” . Halpern and Pearl [ 3,4] also build on this idea, positioning\\ncounterfactuals as a way to highlight actual causes.\\nPalacio et al. went with a broader deﬁnition of explanation, arguing that',\n",
       "  'causation is not necessary for all explanation: “An explanation is the process of\\ndescribing one or more facts, such that it facilitates the understanding of aspects\\nrelated to said facts (by a human consumer)” [ 14, p. 5]. They further argue that\\nunderstanding is unique to humans, and therefore explanation from machine tomachine is merely veriﬁcation. We do not agree with either assertion—indeed,\\nwe consider it a central task artiﬁcial general intelligence research to endow\\nmachines with understanding [ 21], and we see causal relations as central in all\\nexplanations (if not explicit, then certainly implicit), because they are the fun-\\ndamental method for explanation veriﬁcation .\\nIn our view, all explanations of complex tasks with multiple steps and sub-\\ngoals must be based, in one way or another, on causal relations. We therefore\\ntreat causation and causal knowledge as a necessary element in this work.\\n3 Deﬁnitions',\n",
       "  'Here we give a compact description of key terms used in the following sections,in particular Sect. 4.\\nExplainer and Explanation. A process that produces explanations is an\\n‘explainer.’ This can be a human, a machine, or some other process which is\\npositioned to serve such a role. An ‘explanation’ is a compact description out-lining some subset of a modelset of the phenomenon that, for whatever reason,\\nis misunderstood, misrepresented, or missing from the phenomenon’s modelset.\\nAn explanation typically references existing parts of a modelset and presentseither a missing piece or highlights errors in it (see Sect. 4, page 7).\\nExplanandum and Explainee. ‘Explanandum’ is that which is to be\\nexplained. Given a particular outcome, situation, or turn of events, this canbe an anomaly, a missing but necessary relation, or other identiﬁed inconsis-\\ntency that calls for an explanation. ‘Explainee’ is the particular recipient of',\n",
       "  'an explanation—those to whom the explanation is addressed. This can be theExplaining process itself, a co-located interlocutor, or some future recipient of\\nthe information.\\nExplicit Goal. A ‘goal’ is a (constant state or steady) state to be achieved. An\\n‘explicit goal’ is one which can be described in some representational language',\n",
       "  '290 K. R. Thórisson et al.\\nthat references a knowledge base. An ‘active goal’ is one which can be thus rep-\\nresented and which may already be pursued—i.e. a goal that has been acceptedby an agent of change who is actively pursuing it.\\nExplainable vs. Interpretable. The terms ‘explainable’ and ‘interpretable’\\nare often used interchangeably in AI, but we see a deﬁnitive and important\\ndiﬀerence between the concepts behind them, based on who exactly is doingthe explaining and interpreting. For instance, in work involving artiﬁcial neural\\nnetworks, ‘interpretation’ is typically an explanation of the mechanisms of the\\nclassiﬁer, not of the task or environment for which the system is deployed [ 8]),\\nand it is the researchers who are doing the interpretation.\\n2In contrast, we deﬁne\\nself-explaining AI as ‘AI that is capable of generating valid explanation,’ and\\ninterpretable AI as ‘AI that can be interpreted (or explained) by a third party.’',\n",
       "  'Phenomenon. A phenomenon Φ(process, state of aﬀairs, occurrence) – where\\nWis the world, and Φ⊂W, – is made up of a set of elements, including\\nsub-structures, component processes, whole-part relations, causal relations, or\\nother sub-divisions of Φ{ϕ1...ϕ n∈Φ} of various kinds, including relations ℜΦ\\n(causal, mereological, positional, episodic, etc.) that couple elements of Φwith\\neach other, and with those of other phenomena.\\nComplex Task-Environment. We deﬁne a ‘task-environment’ as the tuple\\nof an assigned task and the environment in which the task is to be performed.A ‘complex’ task-environment is, for all practical purposes, a combination of\\nan assigned task in a particular environment that, for accomplishing the task,\\nrequires (a) detection and separation of patterns and sub-patterns with non-trivial causal and part-whole relations, that must be combined with (b) assump-\\ntions about high-level logical relations between these (e.g. objects cannot be in',\n",
       "  'to places at once), combined with (c) creation, execution, and monitoring ofpartial non-linear plans with nested contingency composition, and/or (d) direct\\napplication of ampliative\\n3reasoning and analogy generation.\\nValid Explanation. An explanation ε(x,y ),w h e r e xis the explanandum and\\nyis a network of known (causal) relations and patterns relevant to x,c a nb e\\nvalidated through a process that seeks to uncover inconsistencies in it through\\nthe generation of questions that probe y’s causal relations relevant to x.T od o\\nso the validating process must be able to (a) represent causality, and use this to(b) abduce arguments which “argue for” – or serve as veriﬁable evidence for –\\nthe validity of the explanation. The arguments could also be veriﬁed by direct\\nmeasurement (but is only necessary if the background assumptions on which theevidence rests are not well-veriﬁed).\\n2Providing adequate levels of transparency modern machine learning and AI sys-',\n",
       "  'tems such as reinforcement learners and deep neural networks, with adequate levels\\nof transparency, requires considerable post-hoc eﬀort and skill in interpreting algo-\\nrithms, and most of the time it is essentially prohibitive due to cost.\\n3Traditionally, ‘ampliative reasoning’ refers to any process that relies on abduction\\nand induction in any combination to achieve a particular result (cf. [ 16]); we include\\n(defeasible, non-axiomatic) deduction in that list.',\n",
       "  'Explicit Goal-Driven Autonomous Self-Explanation Generation 291\\n4 Goal-Driven Explanation Generation\\nWe base this work on a theory of pragmatic understanding proposed by Thóris-\\nson et al. [ 21] which uses the concept of a modelset (set of peewee models4)\\nfor describing a phenomenon, and that can be manipulated through a set ofprocesses for performing four types of tasks, one of which is explanation gener-\\nation. Given a phenomenon Φ,M\\nΦis the modelset intended to capture relevant\\naspects of the phenomenon; the models ({m1...m n}∈MΦ)are information\\nstructures intended for (a) explaining Φ,( b ) predicting Φ, (c) producing eﬀec-\\ntive plans for achieving goals with respect to Φ, and (d) (re)creating Φin any\\nmedium (see Sect. 4,p .6 ) .F o ra n ym o d e l s e t MΦand phenomenon Φ, the closer\\nthe information structures as a whole represent key elements (sub-parts) ϕi∈Φ\\nand their couplings ℜΦ, at any level of detail, the greater the accuracy ofM',\n",
       "  'with respect to Φ.T h em o r e completely such a modelset captures all relevant\\naspects of Φfor achieving any of the four tasks, for any chosen challenge related\\ntoΦ,t h em o r e comprehensive it is. Our theory of goal-driven self-explanation\\nconsiders explanation generation itself to be a task with a particular top-level\\ngoal—namely:\\nGtop— The goal of explanation is to improve (or prove) understanding .\\nThis statement would in itself be a rather shallow if what we mean by\\n‘understanding’ was left unexplained; our deﬁnition of understanding is exactly\\nthis: The more correct – i.e. comprehensive and accurate – an intelligent agent’s\\nmodelset MΦofΦis, the better will the agent be said to understand phenomenon\\nΦ[21]. An explanation in this view is a concrete action that is intended to verify,\\nevaluate, or increase either the completeness of an agent’s models and relations\\n(Qcompl (MΦ,ℜΦ)), its accuracy ( Qacc(MΦ,ℜΦ)), or both.',\n",
       "  'As mentioned above (p. 5), the models of a phenomenon’s Φrelations ℜΦ\\ndescribe how its elements relate to each other, and to other phenomena. If we\\npartition ℜΦinto two disjoint sets, inward facing relations ℜin\\nΦ=ℜΦ∩(2Φ×2Φ)\\nand outward facing relations ℜout\\nΦ =ℜΦ\\\\ℜin\\nΦ, an agent whose models are only\\naccurate and complete for ℜin\\nΦunderstands Φbut not Φ’s relation to other phe-\\nnomena (i.e. its context); an agent whose models are only accurate and complete\\nforℜout\\nΦunderstands Φ’s relation to other phenomena but will have limited or\\nno understanding of Φ’s internals.\\nAgood explanation is one that unequivocally demonstrates or veriﬁes under-\\nstanding of a phenomenon Φ[1], or improves understanding of Φby aﬀecting\\nthe modelset describing the phenomenon in a way that improves the possessor\\nof that modelset’s ability to achieve the four tasks related to a phenomenon.\\nThe explanation generation process involves the skills of identifying (i) the',\n",
       "  'role that the explanation should fulﬁl, (ii) the relevant patterns and relations\\nthat must be referenced for it to serve this role, and (iii) producing a description\\n4Small models that can be composed into larger modelsets; see e.g. [ 11,13].',\n",
       "  '292 K. R. Thórisson et al.\\nthat meets these requirements (for a particular set of explainees). This is com-\\npatible and in line with earlier work on explanation generation (cf. [ 4,15]). With\\nthe exception of the ﬁrst skill, to achieve any of these in a complex environment\\nrequires information about cause and eﬀect, the knowledge representation capa-\\nble of supporting the above must, by deﬁnition, contain information about thecausal structure of Φ.\\nGenerating an explanation calls thus for certain necessary information and\\nmust meet certain necessary requirements. More speciﬁcally, producing an expla-nation involves the generation of a compact description that references or impli-\\ncates one or more causal relations that – if not present, or structured diﬀerently\\n– would result in a diﬀerent outcome. The causal relation(s) relevant to thephenomenon that explanation targets limit(s) the possible state space by pro-',\n",
       "  'viding constraints, thus contributing to a particular outcome or situation. The\\nnecessary ingredients to produce explanations are, therefore:\\n– knowledge of causal (and other) relations,\\n– named entities (and appropriate grammar) for producing this description,\\n– a fulﬁllment of a (possibly hypothesized) goal that the explanation is intended\\nto meet.\\nWe hypothesize three classes of purposes – or subgoals – that a generated\\nexplanation may serve, namely, to highlight or identify the following aspects\\nrelevant to an explanandum:\\nG1— Unknown or hidden variables, patterns, or other aspects.\\nG2— Unknown or hidden causal factors and chains.\\nG3— Unknown or hidden errors in background assumptions.\\nThe task of an explainer (explanation-generating process) is to meet the top-level\\ngoal that explanation serves, that is, to prove/improve understanding, by meet-ing one or more of these three subgoals as closely as possible. The explainee can',\n",
       "  'be co-temporal and co-spatial, (as in human realtime dialog), a future receiver\\nof a recorded or written explanation (e.g. instruction manuals), a group of stu-dents (as in a classroom), or the explanation-generating process itself (like during\\nlearning, when explaining things to oneself for veriﬁcation of understanding).\\nSince an explanation serves a purpose, as deﬁned by its subgoal(s), G\\n1−3,\\nwe can assume that it may do so on a continuum, from well to badly. The\\ngradient from meeting this goal perfectly, R(ε)=1 , to not meeting it at all,\\nR(ε)=0 , describes how well an explanation “hits the spot”—let’s call it the\\nexplanation’s role fulﬁllment ,R(ε,ϖ ),w h e r e ϖis its designated role. And since\\nan explanation could in theory highlight the relevant patterns, causal chains, orbackground assumptions anywhere from perfectly to not at all, we can deﬁne a\\ngradient for this dimension as well, ε(P\\nrvt)=[ 0 ,1];w ec a l li tt h e validity of an',\n",
       "  'explanation, V=ε(Prvt).T h e value of a given explanation is then the product\\nofhow well it meets its goal and how validit is,vpur (ε)=R×V .\\nWe call this an explanation’s “ pur(pure) value” because there is a third factor\\nthat could be considered here, that is, how well the explanation ﬁts an explainee',\n",
       "  'Explicit Goal-Driven Autonomous Self-Explanation Generation 293\\nagent’s Aknowledge, K(A). A ‘perfect explanation’ is deﬁned as an explanation\\nwhose pure validity is at maximum, vpur (ε)=1.0, and whose compactness could\\nnot be greater. The maximum compactness of an explanation εis in part dictated\\nby this factor, because the more an explainee knows, the more compact can the\\nexplanation be made. If an explainer makes incorrect assumptions about theexplainee’s knowledge – that is, there is misalignment between the explainee’s\\nknowledge and the explainer’s model of that knowledge – the compactness of\\nthe explanation will suﬀer. We propose to represent this relationship as a match,or overlap, between the constructed explanation’s encoding and the explainee’s\\nability to unwrap that encoding (in other words, the eﬀort required to decode\\nthe information it is intended to carry), that is, {ε\\nΦ−(Φ\\\\K (A))},w h e r e Φis\\nthe explanandum, εΦis the (encoded) explanation of a particular part of Φthat',\n",
       "  'references both known and unknown information, and K(A)is the knowledge\\nof the explainee.5This, then, may be taken into account when quantifying the\\nvalue of an explanation.6\\nIn a reﬂective controller, i.e. one that can reﬂect on its own inner opera-\\ntions, any explanation can become the subject of the agent’s own explanation\\nmachinery, allowing for the generation of explanations of explanations (like we\\nare doing right here right now). Capacity for this kind of self-explanation canenhance not only an AI system’s understanding of its task and environment but\\nalso of itself. In each case the explanations coming from within the system can\\nbe processed by the system for the purpose of further knowledge acquisition[23]. Stated diﬀerently, given that the system is a self-explaining AI, the better\\nthe above explanation generation functions are fulﬁlled and implemented in the',\n",
       "  'same system, the more trustworthy the system will be, but not only that, it couldpossibly learn faster and better. Going one step further, a paper by Thórisson\\nargues that autonomous general learning is not possible without some form of\\nexplanation-generating mechanisms [ 23].\\n5 Explanation Generation in AERA\\nThis section gives a short introduction to how AERA (Autocatalytic Endoge-\\nnous Reﬂective Architecture) meets the above requirements for generating expla-\\nnations [ 11,12]. Knowledge in AERA is represented using two main types of\\ninformation structures, composite states and causal-relational models (CRMs)[11,13,22]. Composite states capture patterns that an AERA agent can perceive;\\nCRMs capture causal relations by representing causes on the left-hand side and\\nresults on the right-hand side. Pattern matching is used to match perceived\\n5For convenience we include, as part of the ‘encoding’ of an explanation, any refer-',\n",
       "  'ences to related but diﬀerent phenomena intended to better match an explainee’s\\nknowledge—that is, to explain something better to a particular explainee, due to\\ntheir particular knowledge at the time of the explanation generation.\\n6This certainly is a factor in all explanations produced by one human for another. It\\nmay not, however, be relevant for self-explanation generation since the meaning of a\\nlow-value (or zero-value, i.e. worthless) explanation produced for oneself is undeﬁned.',\n",
       "  '294 K. R. Thórisson et al.\\nor desired states to either side. Using these constructs, AERA learns in a self-\\nsupervised way by constructing programs on the ﬂy for achieving self-generatedgoals and sub-goals [ 20]. The resulting networks of information produce both\\nconcrete and hypothetical plans, predictions, and sequences of actions that ful-\\nﬁll set goals.\\nAERA’s capacity for self-explaining comes primarily from two key principles.\\nFirstly, all its knowledge is explicit and compositional in a scale-independent way.\\nThis means that both small and large details can be captured with compara-ble information structures, and that hierarchies of knowledge can also be con-\\nstructed into modelsets (through combinations of smaller elements). Secondly,\\nbecause cause-eﬀect relationships are represented directly (also in a relativelyscale-free manner), computing the implications of particular actions, and pro-\\nducing appropriate plans for achieving goals, is directly supported.',\n",
       "  'Finally, the special programming language used to implement these mech-\\nanisms in AERA, Replicode [ 11], makes key parts of the system’s operational\\nsemantics accessible to itself, allowing it to use explanation to argue to itself\\nabout which action to take, which options may be better than others, and what\\nparticular actions may lead to in comparison to others.\\n6 Conclusion\\nExplainability and traceability are key requirements of all mission-critical engi-\\nneering. With the increasing use of software-controlled systems, complexity rises,and with complexity comes the need for smarter software systems. To be trust-\\nworthy, AI must be explainable. With the goal of creating systems with general\\nintelligence, AGI-aspiring systems should not only be explainable, they shouldbe able to explain themselves to their users. But if general intelligence requires\\nthe ability to explain – if not for any other reason that the sheer amount of',\n",
       "  'possibilities that the physical world presents to anyone who is learning aboutit from scratch – then such systems, upon having achieved generality in the\\nnear or distant future, will already be able to generate good explanations about\\ntheir own operation and their task-environment. We hope the work in this papermoves us one step closer to this future.\\nAcknowledgments. This work was supported in part by Cisco Systems, the Icelandic\\nInstitute for Intelligent Machines and Reykjavik University.\\nReferences\\n1. Bieger, J., Thórisson, K.R.: Evaluating understanding. In: IJCAI Workshop on\\nEvaluating General-Purpose AI, Melbourne, Australia (2017)\\n2. Cohen, J.: Teleological explanation. Proc. Aristot. Soc. 51, 255–292 (1950)\\n3. Halpern, J.Y., Pearl, J.: Causes and explanations: a structural-model approach –\\npart I: causes. Br. J. Philos. Sci. 56, 889–911 (2005)\\n4. Halpern, J.Y., Pearl, J.: Causes and explanations: a structural-model approach –',\n",
       "  'Part II: Explanations. Br. J. Philos. Sci. 56, 843–847 (2005)',\n",
       "  'Explicit Goal-Driven Autonomous Self-Explanation Generation 295\\n5. Hilton, D.J.: Conversational processes and causal explanation. Psychol. Bull.\\n107(1), 65–81 (1990)\\n6. Hilton, D.J., Slugoski, B.R.: Knowledge-based causal attribution: the abnormal\\nconditions focus model. Psychol. Rev. 93(1), 75–88 (1986). https://doi.org/10.\\n1037/0033-295X.93.1.75\\n7. Josephson, J., Josephson, S.: Abductive Inference: Computation, Philosophy, Tech-\\nnology. Computation, Philosophy, Technology, Cambridge University Press (1996)\\n8. Lapuschkin, S., Wäldchen, S., Binder, A., Montavon, G., Samek, W., Müller, K.R.:\\nUnmasking clever hans predictors and assessing what machines really learn. Nat.Commun. 10(1) (2019). https://doi.org/10.1038/s41467-019-08987-4\\n9. Lombrozo, T.: The structure and function of explanations. Trends Cogn. Sci.\\n10(10), 464–470 (2006)\\n10. Miller, T.: Explanation in artiﬁcial intelligence: insights from the social sciences\\n(2017)',\n",
       "  '11. Nivel, E., Thórisson, K.R.: Replicode: a constructivist programming paradigm and\\nlanguage. Technical report RUTR-SCS13001, Reykjavik University School of Com-\\nputer Science (2013)\\n12. Nivel, E., Thórisson, K.R.: Towards a programming paradigm for control systems\\nwith high levels of existential autonomy. In: Kühnberger, K.-U., Rudolph, S., Wang,\\nP. (eds.) AGI 2013. LNCS (LNAI), vol. 7999, pp. 78–87. Springer, Heidelberg(2013). https://doi.org/10.1007/978-3-642-39521-5_9\\n13. Nivel, E., et al.: Bounded recursive self-improvement (2013)14. Palacio, S., Lucieri, A., Munir, M., Hees, J., Ahmed, S., Dengel, A.: Xai handbook:\\ntowards a uniﬁed framework for explainable AI (2021)\\n15. Pearl, J.: Causality: Models, Reasoning and Inference, 2nd edn. Cambridge Uni-\\nversity Press, New York, NY, USA (2009)\\n16. Psillos, S.: An explorer upon untrodden ground: peirce on abduction. In: Handbook\\nof the History of Logic, vol. 10, pp. 117–151. Elsevier (2011)',\n",
       "  '17. Rörbeck, H.: Self-Explaining Artiﬁcial Intelligence: On the Requirements for\\nAutonomous Explanation Generation. M.Sc. Thesis, Dept. Comp. Sci., ReykjavikUniversity (2022)\\n18. Strevens, M.: The causal and uniﬁcation approaches to explanation uniﬁed-\\ncausally. Noûs 38(1), 154–176 (2004)\\n19. Thórisson, K.R.: A new constructivist AI: from manual construction to self-\\nconstructive systems. In: Wang, P., Goertzel, B. (eds.) Theoretical Foundations\\nof Artiﬁcial General Intelligence, vol. 4, pp. 145–171 (2012)\\n20. Thórisson, K.R.: Seed-programmed autonomous general learning. Proc. Mach.\\nLearn. Res. 131, 32–70 (2020)\\n21. Thórisson, K.R., Kremelberg, D., Steunebrink, B.R., Nivel, E.: About understand-\\ning. In: Steunebrink, B., Wang, P., Goertzel, B. (eds.) AGI -2016. LNCS (LNAI),\\nvol. 9782, pp. 106–117. Springer, Cham (2016). https://doi.org/10.1007/978-3-319-\\n41649-6_11\\n22. Thórisson, K.R., Talbot, A.: Cumulative learning with causal-relational models.',\n",
       "  'In: Iklé, M., Franz, A., Rzepka, R., Goertzel, B. (eds.) AGI 2018. LNCS (LNAI),\\nvol. 10999, pp. 227–237. Springer, Cham (2018). https://doi.org/10.1007/978-3-\\n319-97676-1_22\\n23. Thórisson, K.R.: The explanation hypothesis in general self-supervised learning.\\nProc. Mach. Learn. Res. 159, 5–27 (2021)\\n24. Woodward, J.: Making things Happen: A Theory of Causal Explanation. Oxford\\nUniversity Press, Oxford (2005)',\n",
       "  'Addressing the Unsustainability of Deep\\nNeural Networks with Next-Gen AI\\nAmanda Vallentin1,2(B),K r i s t i n nR .T h ´ orisson1,3(B), and Hugo Latapie4(B)\\n1Center for Analysis and Design of Intelligent Agents, Reykjavik University,\\nMenntavegur 1, Reykjav´ ık, Iceland\\nthorisson@ru.is\\n2IT University of Copenhagen, Rued Langgaards Vej 7, Copenhagen, Denmark\\namgv@itu.dk\\n3Icelandic Institute for Intelligent Machines, Reykjav´ ık, Iceland\\n4Cisco Systems, Emerging Technologies and Incubation, San Jose, CA, USA\\nhlatapie@cisco.com\\nAbstract. Humanity is currently facing one of its biggest challenges to\\ndate: The climate crisis. As a result, most industry sectors are reassess-\\ning their ways of working to be better equipped to address their share\\nof the situation. The digital sector often gets set aside in such consider-ations in talk about the green transition because a signiﬁcant amount of\\nits work consists of optimizing processes that can save resources. Deep',\n",
       "  'neural networks (DNNs) have gained great attraction and have showngood results regarding process automation. We argue that there are well-\\nknown and lesser known negative side-eﬀects to automation frameworks\\nbased on DNNs (and related technologies) in terms of energy consump-tion, pollution, and social equality, that must be questioned. We analyze\\nthe operating principles and deployment methods of DNNs, the new era\\nof automation eﬀorts this has launched, and argue on this basis thattheir continued use is both unsustainable and indefensible. Using three\\nexamples of ongoing research, we explain how alternative approaches to\\ndevelop more general machine intelligence are well-poised to power thenext phase of AI-based automation.\\nKeywords: Artiﬁcial Intelligence\\n·Methodology ·Deep Neural\\nNetworks ·General Machine Intelligence ·Empirical Reasoning ·\\nAutomation ·Energy Consumption ·Pollution ·Social Equality ·\\nInnovation',\n",
       "  'c⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2023\\nP. Hammer et al. (Eds.): AGI 2023, LNAI 13921, pp. 296–306, 2023.https://doi.org/10.1007/978-3-031-33469-6\\n_30',\n",
       "  'Addressing the Unsustainability of Deep Neural Networks with Next-Gen AI 297\\n1 Introduction\\nThe IT-sector is one of the most innovative and fastest-growing industries world-\\nwide1. The bleeding edge lies arguably in automation technologies, in no small\\npart because of the obvious incentive that reduced cost and increased speed\\ntranslates directly to increased revenue. Within contemporary2applied automa-\\ntion technologies, DNNs are the latest arrivals with signiﬁcant potential for var-\\nious applications. Spurred by predictions of its usefulness for a wide range of\\ntasks3, unbridled optimism has often characterized its coverage in the media. For\\ninstance, a Forbes article presents the 13 skills AI already has today (including\\n“smell” and “reading your mind”) [ 19]; the Guardian explains how AI is changing\\nhow a number of diﬀerent industry sectors operate [ 6]. However, after a period\\nof experimentation it is increasingly clear that DNN deployment is unavoidably',\n",
       "  'hampered by inherent deep limitations [ 17] and hidden costs [ 4,33].\\nDNNs risk compromising the path towards sustainable development of soci-\\nety4. In particular, their runtime and updating methodologies make them unsus-\\ntainable [ 16,33]. Another limitation has to do with how they are developed.\\nEnergy consumption during DNN development is incredibly high (PaLM, a lan-guage model from Google, consumed about 3 .4 GWh in about 2 months [ 3]).\\nFor this reason, and others, very few companies will be able to aﬀord developing\\nthem because of their sheer size and compute requirements (the BLOOM model,with 175 billion parameters, cost US$7 million to develop [ 3]). So this approach\\nis inappropriate for parties with only small data and small funding. DNNs are\\nthus nowhere nearly as appropriate or powerful for being deployed in automation\\ntasks as past and present moves by tech giants might indicate [ 2,5].\\nThis paper has two main parts: In Sects. 2and3we detail what we consider',\n",
       "  'key limitations of DNNs, and in Sect. 4and5we discuss how these could be\\novercome through research on artiﬁcial general intelligence.\\n2 Deep Limitations of Deep Neural Networks\\nIn recent years both the size and the training data of DNNs have exploded due\\nto an incentive to upscale the models to reach better performance [ 3,33]. GPT-3,\\n1According to Statista, IT-related revenue has a predicted annual growth rate of\\n6.86% and a predicted market volume of US$1,570.00bn by 2027 ( https://www.\\nstatista.com/outlook/tmo/it-services/worldwide –accessed March 1st, 2023) .\\n2Our use of the term ‘contemporary AI’ refers to a set of methodologies that are\\ncurrently in active experimentation or use in industry , including but not limited to\\nreinforcement learning, ANNs of all kinds, and other well-known methods.\\n3For instance, the annual prediction that “full self-driving cars will be available nextyear” has been updated at a rate of one year per year by Tesla’s CEO ( “Watch',\n",
       "  'Elon Musk Promise Self-Driving Cars ‘Next Year’ Every Year Since 2014,” https://\\nfuturism.com/video-elon-musk-promising-self-driving-cars —accessed March 1st,\\n2023.\\n4The UN deﬁnes ‘sustainable development’ as harmony between economic\\ngrowth, social inclusion, and environmental protection. https://www.un.org/\\nsustainabledevelopment/development-agenda/ —accessed April 4th, 2023 .',\n",
       "  '298 A. Vallentin et al.\\none of the biggest language models to date, has shown great results in some text\\ngeneration tasks [ 5] leading many to think that DNNs can be used to solve any\\ntask. We argue that this is neither wise nor possible.\\n2.1 DNNs: Expensive to Develop and Use\\nA search with Google’s new chatbot Bard can cost the company 10 times more\\nthan a traditional key word search [ 21]. However, the total cost of the models\\nare already high before they leave the lab (cf. the BLOOM model required $7\\nmillion worth of computing time during its development). To provide necessary\\ncomputing power for the training phase, the developers of DNNs also need access\\nto expensive specialized hardware [ 33]. Attempts have been made to measure the\\nenvironmental footprint of large language models (LLM) [ 16,33]. Luccioni et al.\\n2022 uses a life cycle analysis approach to estimate a more realistic environ-',\n",
       "  'mental footprint for a LMM called BLOOM. When they add emissions fromall training activities and experiments (not just the ﬁnal training run) as well\\nas emissions from the infrastructure that maintains the hardware and emissions\\nfrom manufacturing the hardware, the total footprint of BLOOM is ∼124 tons\\nCO\\n2eq[16]. However, the carbon intensity of the grid used to train BLOOM was\\nonly 57 gCO 2eq/kW h (trained in France) compared to GPT-3 where the carbon\\nintensity of the grid was 429 gCO 2eq/kW h (trained in the US). Unfortunately, we\\nonly know the power consumption of the ﬁnal training phase of GPT-3. Com-\\nparing the estimated carbon emissions from the two models’ ﬁnal training phase,\\nBLOOM was ∼25 tons CO 2eqand GPT-3 was ∼502 tons CO 2eq, which is a sig-\\nniﬁcant diﬀerence since the models have about the same amount of parameters\\n[16]. The environmental footprints of DNNs are strongly inﬂuenced by carbon',\n",
       "  'intensity of the energy grid. Considering that US and China are the biggestplayers in the AI market [ 20], and they use about 81% and 83% fossil fuels [ 27],\\nthe estimated footprint of only a small part of GPT-3’s life cycle is worrying\\n(Table 1).\\nTable 1. Comparison of the CO 2emission of diﬀerent products. The Scale column\\nshows the emissions multiplier matching a laptop computer’s complete lifecycle.\\nProduct Description CO 2eq(kg)Scale\\nOne laptop [ 1] Entire life cycle incl. power use (avg.) 423 1\\nOne automobile [ 33]Entire life cycle incl. fuel use (avg.) 57,153 135\\nOne GPT-3 [ 16] Final training phase 502,000 1187\\nAnother aspect that can raise the economic and environmental price is when\\nthe model needs to be corrected after it is deployed. In this case, the model needsto be taken down and retrained since DNNs cannot be taught anything new once\\nthey have left the lab. What often happens is that once the models are released,',\n",
       "  'Addressing the Unsustainability of Deep Neural Networks with Next-Gen AI 299\\nthey act in unexpected ways and the developers need to spend more resources\\non making them behave. In 2016 when Microsoft created a twitter account forthe chatbot Tay, it went from tweeting innocent tweets like “I love feminism\\nnow” to “Hitler was right I hate jews” in a single day, despite being trained\\non safe data as Microsoft claims [ 9], resulting in the bot having to be taken\\ndown. Seven years later, Microsoft ran into a similar problem when a journalist\\nat the New York Times had a conversation with their new chatbot that ended\\nwith the bot confessing its love to him and telling him to leave his wife [ 28].\\nAfter the incident, attempts were made to prevent the chatbot from answering\\npersonal questions, but even with countless reboots and alterations, it could not',\n",
       "  'be guaranteed that it behaved according to plan. Some have even made this into asport (called ‘JailBreaking’) where they share and test ways of getting around the\\n“lobotomized” chatbots and make them say racist, misogynistic, etc. statements.\\nConsidering the enormous resources spent on controlling the DNNs’ behaviors\\nafter they leave the lab, it seems that proper kinds of control mechanisms are\\nmissing. This is, however, hardly surprising, since DNNs are primarily based onstatistical methods and have no obvious ways of being steered through explicit\\ngoals or hierarchical rules.\\n2.2 The Limited “learning” of Statistics-Based Systems\\nAll animals learn cumulatively because the world does not reveal itself to any-\\none all-at-once. The “learning” that contemporary AI systems practise is a veryspecial case of what is normally called ‘learning,’ and it greatly limits which\\nkind of tasks they can be “trained” to solve well. Research by Eberding et al.',\n",
       "  '[7] compared several diﬀerent types of DNN-based learners (they also tested the\\nAGI-aspiring NARS — we discuss this in a later section) on the well-known cart-\\npole balancing task, which consists of learning to balance a stick standing on a\\ncart by issuing right and left commands (‘R’ and ‘L’). Once the various AI learn-ers had achieved this task, the researchers reversed the directional commands.\\nThe performance of the various learning algorithms to adjust their prior train-\\ning to this new condition is recorded. In a ﬁnal scenario, the researchers switchback to the original control method. The performance of all tested DNN-based\\nlearners dropped signiﬁcantly in the reversed phase, and it takes them many\\nmore iterations to reach the same performance, once the controls are switched\\nback to the original settings. The change in the task had to be “unlearned”',\n",
       "  'through enough new interactions before the performance could return to whatit was before the controls were switched. None of them returned to their original\\nperformance.\\nThis research exposes how DNNs have a static and simplistic representation\\nof the world. They are not capable of inferring simple relations (in Eberding’s [ 7]\\nexperiment, that the controls were switched around) which makes them unusable\\nfor many tasks where reasoning is of importance, like math. The best DNN scoreon the MATH data set is 50% [ 3]. The developers managed to reach this score\\nby training it only on mathematics-related texts and up scaling the size of the\\nmodel to astonishing 540 billion parameters. With this strategy they were hoping',\n",
       "  '300 A. Vallentin et al.\\nthat the model would evolve to be able to perform reasoning through pattern-\\nrecognition alone [ 3]. There have been attempts of creating reasoning abilities in\\nDNNs, for example using chain-of-thought prompting. The technique improves\\nmodels’ scores on certain data sets, but in bigger models [ 40]. This method does\\nnot, in fact cannot, turn ANN-based systems into reliable reasoners.\\n2.3 DNN Autonomous Learning After It Leaves the Lab:\\n‘Undeﬁned’\\nAnother of DNNs limitations is that once they are trained, their knowledge\\nis ﬁxed and they cannot be easily applied to another task. When faced with\\nsomething that was not part of their training data, performance decreases orthey do something that is unpredictable. This is likely one of the reasons why\\nself-driving cars have not met their makers’ expectations; there are countless',\n",
       "  'scenarios an artiﬁcial driver must be able to navigate before it is safe to letit out on the roads. The upshot is, when it comes to complex tasks, DNNs\\ncannot be trusted, due to the countless road scenarios that may occur. There\\nare attempts to overcome this problem, for example a one-shot learning modelcan classify images it has not seen in its data set. However, the models are more\\ncomputationally heavy to run and they only work if the image is similar to the\\nones in the training set [ 15].\\n2.4 DNNs and Social Inequality\\nWhen looking at LLMs, the data size requirements have exploded in recent years.\\nBERT was trained on 16 GB data in 2019 and GPT-3 was trained on 570 GB\\nin 2020 [ 4]. Firstly, it is diﬃcult to get a hold of this much data and secondly, it\\nis nearly impossible to ensure that the data has the right quality. In LLMs thismanifests itself in a bias against minorities because most of their data has been',\n",
       "  'scraped of sources like Reddit, Twitter, and Wikipedia where the majority of\\nwriters are white males [ 4]. In medical AI, we also see discrimination of patients\\nbecause it is diﬃcult to acquire data sets that are representative for all genders,\\nages, and races [ 25]. Due to the data requirements and cost of DNNs, their\\nincreased use will risk worsening inequality, as not everyone has equal access [ 33]\\nor is equally represented. Healthcare models only work for groups represented\\nin the data sets.\\nAdditionally, the price of using the DNNs will limit which users have access\\nthem. For instance, ChatGPT has recently made headlines about being able to',\n",
       "  'Addressing the Unsustainability of Deep Neural Networks with Next-Gen AI 301\\npass several advanced exams at universities5. If not all students have equal access\\nto DNN aids, we risk increasing social inequality6.\\n2.5 DNNs’ Domination of the AI Narrative\\nDespite the known limitations of DNNs, development of alternative approaches\\nto making machines smarter suﬀers from their media dominance. The privatesector has great inﬂuence on AI research and they tend to favor data-hungry\\nand computationally heavy DNNs [ 11].\\nDue to inordinate emphasis on a single technology, young researchers may\\nbe lead to believe that deep learning methods (a) are the end-all, be-all, (b)\\nwill overcome all the challenges we accounted for, and (c) will continue to be\\na key technology in our society [ 11]. It is no surprise that we see this develop-\\nment because many of the key DNN researchers still seem to believe that the\\ntechnology will overcome all these challenges with more data and more eﬃcient',\n",
       "  'hardware. Kaplan & McCandlish [ 10] argue that there exists a scaling law for\\nneural language models, suggesting that there is more to gain if we continue with\\nenlarging the DNNs. Altman predicts an AI revolution because of the incredible\\nwealth that will be created as DNNs replace the majority of our workforce [ 2].\\nHowever, there is ample evidence that DNNs are not living up to such\\nexpectations—and it probably never will [ 37]. The optimism echoes claims made\\nof the Cyc project in the 80s and 90s [ 14]. Looking at DNNs’ abilities regard-\\ning common sense, Marcus and Davis [ 18] recently challenged ChatGPT-3’s pre-\\nsumed theory of mind, arguing that the results do not show an ability of commonsense but rather that, due to being trained on data about thought-experiments\\nand logic tests, it can predict the answers on purely linguistic principles. When',\n",
       "  'the phrasing of questions changes slightly or the questions are asked in anotherlanguage, GPT-3 shows no sign of having a theory of mind. In a study by Stojnic\\net al. [ 32], they compared DNNs common sense ability to infants and the study\\nrevealed that the DNNs failed and did not appear to have common sense.\\nAlong with over-promising in the ﬁeld of DNNs, there is a lack of innovation\\nthat misleads newcomers, governments, and institutions who continue to support\\nresearch on the topic. By ignoring other strategies, society is not only wastingprecious resources but also risking the ﬁeld of AI as a whole to lose trust.\\n3 Summary of Limitations\\nBased on the foregoing, there can be little doubt that contemporary AI method-ologies, in all their variations, come with signiﬁcant limitations. DNNs are mono-\\n5ChatGPT has passed the Wharton Exam, US medical licensing exam, law school\\nexam, and others. ( https://www.businessinsider.com/list-here-are-the-exams-',\n",
       "  'chatgpt-has-passed-so-far-2023-1?r=US&IR=T#wharton-mba-exam-1 —accessed\\nMarch 4th, 2023) .\\n6As of April 2023, the price is $20 a month for reliable and fast access to ChatGPT,\\nalthough a free version with slower response is still available. ( https://openai.com/\\nblog/chatgpt-plus —accessed April 4th, 2023) .',\n",
       "  '302 A. Vallentin et al.\\nlithic technologies with limited scope. They only work well when they are built\\nfor a well-deﬁned limited task with extensive amounts of data of a certain qual-ity. If any changes are necessary due to unwanted behavior or a slightly diﬀerent\\ntask, the models must be rebuilt, repeating their resource-demanding training\\ncycles. Combined with the potential decrease in social equality, we have a tech-nology that both compromises social inclusion and environmental protection.\\nThis is unsustainable. To summarize the limitations of DNNs discussed so far:\\n– are exceedingly expensive to develop and use\\n– have a large environmental footprint due to energy consumption\\n– are diﬃcult to control\\n– only work well for certain types of tasks– are diﬃcult and expensive to adapt to new tasks\\n– can increase inequality in the world\\n– take away focus and resources from other approaches in AI\\nIt is neither good for the ﬁeld of AI nor for society at large that the inordi-',\n",
       "  'nate amount of funding and eﬀort poured into DNNs and related technologies\\ncontinues. How can we move forward to more sustainable AI?\\n4 Breaking the Stalemate Through Innovation\\nExamples of similar situations can be found in recent history of innovation,where a single framework had become too entrenched too early. One example\\nis the global windmill industry in the 1970s.s. Due to the energy crisis at the\\ntime, there was a push towards ﬁnding cheaper energy sources and many coun-\\ntries tried to develop megawatt windmills [ 22]. In Denmark another approach\\nwas taken, where smaller companies developed smaller and more experimentalwindmills and met up at annual windmills conferences and shared their results\\n[22]. The companies had incentive to do so because many private individuals\\nwere interested in buying their own local windmill, since the government wouldpay 30 percent of such investment [ 22]. Due to this approach, the development',\n",
       "  'of a new type of windmill was undertaken, one in which risk was lowered due to\\nthe willingness of the Danish population to buy smaller windmills. As a resultthe windmill industry was born in Denmark, which produced the best windmills.\\nIn other countries a more conservative approach was chosen by attempting to\\nupscale the best existing windmills at the time, with the aim of turning them intomegawatt windmills. All of those approaches failed as they could not compete\\nwith the Danish models, which were cheaper yet more robust [ 22]. The current\\ndevelopment of contemporary AI where researchers and companies upscale theirframeworks (more data and bigger models), believing that “bigger is better,”\\nresembles what we saw in the windmill industry in the 70’s. In our view, a\\nwholly new methodological paradigm is called for to develop more autonomous\\nAI that is more capable and whose behavior is easier to manage and predict.',\n",
       "  'Addressing the Unsustainability of Deep Neural Networks with Next-Gen AI 303\\n5 Sustainable Automation via AGI\\nThe main limitations of DNNs can be grouped into three sets based on their\\nsource: (a) opaqueness, (b) learning style, and (c) representation. All of three\\nhave made a regular occurrence throughout much of AI research [ 37], or certainly\\nsince the start of the annual AGI conference series in 2008. Here we present an\\noverview of selected recent work focusing on these areas.\\nFrom Opaque to Transparent Knowledge. A powerful way to represent\\nknowledge7, that makes it directly inspectable by human or machine, is to make\\nits structure explicitly hierarchical. Representing knowledge explicitly was of\\ncourse common in the expert systems of the 1970s, and some research in AI has\\ncontinued this tradition. The approach comes with known limitations, whichcan be overcome by taking speciﬁc steps. For instance, the Non-Axiomatic Rea-',\n",
       "  'soning System (NARS) represents knowledge as defeasible [ 26] statements that\\nnevertheless support reasoning; indeed, NARS-based systems learn through rea-soning processes that mix (non-axiomatic) deduction, abduction, and induction\\n(cf. [8,13,39]). Other systems take a compatible approach but use a diﬀerent\\nknowledge representation scheme, e.g. the Autocatalytic Endogenous ReﬂectiveArchitecture (AERA [ 24]). The results demonstrated by prototypes developed\\nby Latapie et al. [ 13] show that systems relying on explicit knowledge represen-\\ntation have come a long way, yet their funding is in no way proportional to the\\nresults achieved. These systems work on vastly smaller data than DNN-based\\nsystems, and thus use much less energy.\\n8\\nBesides non-axiomatism, another way to overcome the limitations of\\napproaches based on logic statements is to step up to second-order represen-\\ntation, allowing the system to inspect and operate on its own knowledge [ 34].',\n",
       "  'Such reﬂective systems have unfortunately not been given suﬃcient attention in\\nthe AI literature. The results of Nivel et al.’s [ 23] research on teaching an AERA-\\nbased agent to learn by observation how to conduct TV-style interview on thetopic of recycling in under 21 h, including learning the syntax and semantics of\\na 100-word vocabulary, how to take turns in dialog, manipulation of objects,\\ndeictic gestures of various forms, and more – from scratch – should suﬃce toconvince anyone that this very iconoclastic approach to machine learning should\\nbe pursued more vigorously by the AI community.\\nFrom Once-and-for-All Learning to Cumulative Learning. Learning in\\nnature has no choice but to proceed incrementally, because the world does notreveal itself to learners all-at-once. This means that the knowledge representation\\nscheme must be updatable piece-wise [ 36]. Furthermore, any autonomous system\\n7By ‘knowledge’ we mean a form of ‘actionable information’—that is, information that',\n",
       "  'can be used for making plans and getting things done in a particular environment.\\n8The AERA system, for example, learned to do a TV-style interview after learningfor only 20 h on a 6-core oﬃce desktop machine [ 35].',\n",
       "  '304 A. Vallentin et al.\\ndeployed in the physical world will encounter situations that are not identical to\\nsomething experienced before. Automating the handling and learning from theseis imperative for advancing the state of industrial automation. Viable solutions\\nto cumulative learning have already been proposed [ 8,36,39].\\nCompositional Knowledge Representation. This topic is closely related\\nto the ﬁrst point, which is to say that compositional knowledge representation\\ngoes hand-in-hand with knowledge transparency and cumulative learning. Theability to construct a goal hierarchy autonomously is a foundational requirement\\nfor any AI that is to operate autonomously (or even semi-autonomously); the\\ndesigners cannot possibly foresee every and all situations that the system mayencounter. A goal hierarchy that the system can itself manipulate safely is a\\nnecessity. Th´ orisson [ 38] presents arguments that general autonomous learning',\n",
       "  'is not possible without the capacity for some form of explanation generation.\\nWhile fully-functional AGI systems are still in their early phase of develop-\\nment, some examples are leading the way (cf. [ 8,12,13,29–31]). All this points\\ntowards next-generation systems having the potential to become a green alter-\\nnative to DNNs, promising easier reuse, increased generality, signiﬁcantly less\\nenergy consumption, lower data requirements, less compute power, and a widerrange of applications.\\nAcknowledgment. This work was supported in part by Cisco Systems, the Icelandic\\nInstitute for Intelligent Machines and Reykjavik University.\\nReferences\\n1. What is the carbon footprint of a laptop? Circular Computing (2021). https://\\ncircularcomputing.com/news/carbon-footprint-laptop/\\n2. Altman, S.: Moore’s law for everything (2021). https://moores.samaltman.com/\\n3. Ananthaswamy, A.: In AI, is bigger always better? Nature 615, 202–205 (2023)',\n",
       "  '4. Bender, E.M., McMillan-Major, A., Gebru, T., Shmitchell, S.: On the dangers\\nof stochastic parrots: can language models be too big?. In: Proceedings of the\\n2021 ACM Conference on Fairness, Accountability, and Transparency, pp. 610–623(2021)\\n5. Brown, T.B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., et al.: Language\\nmodels are few-shot learners. Adv. Neural Inf. Process. Syst. 33, 1–25 (2020)\\n6. Butler, S.: From retail to transport: how ai is changing every corner of the econ-\\nomy. The Guardian (2023). https://www.theguardian.com/technology/2023/feb/\\n18/from-retail-to-transport-how-ai-is-changing-every-corner-of-the-economy\\n7. Eberding, L.M., Th´ orisson, K.R., Prabu, A., Jaroria, S., Sheikhlar, A.: Compar-\\nison of machine learners on an aba experiment format of the cart-pole task. In:\\nProceedings of Machine Learning Research, vol. 159, pp. 49–63 (2021)\\n8. Hammer, P.: Reasoning-learning systems based on non-axiomatic reasoning system\\ntheory, vol. 192, pp. 88–106 (2022)',\n",
       "  ...],\n",
       " 'uris': None,\n",
       " 'data': None}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_chroma.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfd6065-b606-4816-a8da-f7a008156ede",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
